import requests
from datetime import datetime, timezone, timedelta
import json
import os
from bs4 import BeautifulSoup
import time
import re

def fetch_recent_arxiv_articles(query="cat:cs.AI", max_results=100, start=0):
    """
    R√©cup√®re les articles ArXiv les plus r√©cents dans les cat√©gories sp√©cifi√©es.
    
    Args:
        query: La requ√™te de recherche
        max_results: Nombre maximum de r√©sultats √† retourner (max 100 selon API ArXiv)
        start: Index de d√©part pour la pagination
        
    Returns:
        Le contenu XML de la r√©ponse
    """
    base_url = "http://export.arxiv.org/api/query"
    params = {
        "search_query": query,
        "start": start,
        "max_results": max_results,
        "sortBy": "submittedDate",
        "sortOrder": "descending"
    }
    
    # Respecter les limites de l'API ArXiv (min 3 secondes entre requ√™tes)
    print(f"‚è≥ D√©lai de pause pour respecter les limites de l'API ArXiv...")
    time.sleep(3)
    
    print(f"üì° Requ√™te API ArXiv: start={start}, max_results={max_results}")
    try:
        response = requests.get(base_url, params=params)
        response.raise_for_status()
        return response.content
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erreur API ArXiv: {e}")
        if hasattr(response, 'status_code'):
            print(f"Code d'erreur: {response.status_code}")
        if hasattr(response, 'text'):
            print(f"R√©ponse: {response.text[:500]}...")
        raise

def extract_article_id(link):
    """
    Extrait l'identifiant unique de l'article √† partir de son lien.
    
    Args:
        link: URL ou identifiant d'ArXiv
        
    Returns:
        Identifiant unique de l'article
    """
    # Les ID d'ArXiv sont g√©n√©ralement de la forme: http://arxiv.org/abs/XXXX.XXXXX ou XXXX.XXXXX
    if not link:
        return None
    
    # Chercher l'identifiant dans l'URL
    match = re.search(r'abs/([^/]+)$', link)
    if match:
        return match.group(1)
    
    # Sinon utiliser le lien entier comme ID
    return link

def parse_arxiv_articles(xml_data, target_categories, strict_category=False):
    """
    Parse le contenu XML d'ArXiv pour extraire les informations des articles.
    
    Args:
        xml_data: Contenu XML retourn√© par l'API ArXiv
        target_categories: Liste des cat√©gories √† inclure
        strict_category: Si True, ne garde que les articles dont la cat√©gorie principale correspond
                         Si False, inclut les articles qui ont une des cat√©gories cibles parmi leurs cat√©gories
        
    Returns:
        Une liste d'articles sous forme de dictionnaires
    """
    try:
        soup = BeautifulSoup(xml_data, features="xml")
    except:
        print("‚ö†Ô∏è Parser XML non disponible, utilisation du parser HTML")
        soup = BeautifulSoup(xml_data, "html.parser")
    
    entries = soup.find_all("entry")
    print(f"üìÑ Nombre d'entr√©es trouv√©es dans le XML: {len(entries)}")
    
    articles = []
    filtered_out_count = 0

    for entry in entries:
        try:
            # Extraire la cat√©gorie principale
            primary_category = entry.find("arxiv:primary_category")
            if not primary_category:
                primary_category = entry.find("category")
            
            primary_cat = primary_category.get("term") if primary_category else "Unknown"
            
            # Extraire toutes les cat√©gories (principales et secondaires)
            all_categories = []
            if primary_cat != "Unknown":
                all_categories.append(primary_cat)
                
            # Ajouter les cat√©gories secondaires
            category_tags = entry.find_all("category")
            for cat_tag in category_tags:
                cat_term = cat_tag.get("term")
                if cat_term and cat_term not in all_categories:
                    all_categories.append(cat_term)
            
            # V√©rifier si l'article correspond aux crit√®res de cat√©gorie
            category_match = False
            
            if not target_categories:  # Si aucune cat√©gorie n'est sp√©cifi√©e, on accepte tout
                category_match = True
            elif strict_category:
                # Mode strict: la cat√©gorie principale doit correspondre exactement
                category_match = primary_cat in target_categories
            else:
                # Mode inclusif: n'importe quelle cat√©gorie de l'article peut correspondre
                category_match = any(cat in target_categories for cat in all_categories)
            
            # Passer √† l'entr√©e suivante si la cat√©gorie ne correspond pas
            if not category_match:
                filtered_out_count += 1
                continue
            
            # Extraire les informations de l'article
            title = ""
            if entry.title:
                title = entry.title.string.strip() if hasattr(entry.title, 'string') else str(entry.title).strip()
            
            summary = "R√©sum√© non disponible."
            if entry.summary:
                summary = entry.summary.string.strip() if hasattr(entry.summary, 'string') else str(entry.summary).strip()
            
            link = ""
            if entry.id:
                link = entry.id.string.strip() if hasattr(entry.id, 'string') else str(entry.id).strip()
            
            # Extraire l'identifiant unique
            article_id = extract_article_id(link)
            
            published_date_str = ""
            if entry.published:
                published_date_str = entry.published.string.strip() if hasattr(entry.published, 'string') else str(entry.published).strip()
            
            # Extraire les auteurs
            authors = []
            author_tags = entry.find_all("author")
            for author in author_tags:
                name_elem = author.find('name')
                if name_elem:
                    author_name = name_elem.string.strip() if hasattr(name_elem, 'string') else str(name_elem).strip()
                    authors.append(author_name)
            
            # Ajouter l'article avec toutes ses cat√©gories
            articles.append({
                "id": article_id,
                "title": title,
                "authors": authors,
                "published_date": published_date_str,
                "category": primary_cat,   # Cat√©gorie principale
                "all_categories": all_categories,  # Toutes les cat√©gories
                "summary": summary,
                "link": link
            })
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'analyse d'un article: {e}")
            filtered_out_count += 1
            continue

    # Ajouter des statistiques suppl√©mentaires
    categories_count = {}
    for article in articles:
        cat = article["category"]
        if cat in categories_count:
            categories_count[cat] += 1
        else:
            categories_count[cat] = 1
    
    # Afficher les statistiques par cat√©gorie
    for cat, count in categories_count.items():
        print(f"   - {cat}: {count} articles")
    
    print(f"‚úÖ Articles inclus: {len(articles)}, articles filtr√©s: {filtered_out_count}")
    return articles

def get_latest_arxiv_articles(categories=None, max_articles=500, strict_category=False, callback=None):
    """
    R√©cup√®re les articles les plus r√©cents d'ArXiv sans duplication.
    
    Args:
        categories: Liste des cat√©gories ArXiv √† inclure ou None pour toutes
        max_articles: Nombre maximum d'articles √† r√©cup√©rer
        strict_category: Si True, ne consid√®re que la cat√©gorie principale
                         Si False, consid√®re toutes les cat√©gories de l'article
        callback: Fonction appel√©e pour informer de la progression (pour l'interface utilisateur)
        
    Returns:
        Liste d'articles uniques sous forme de dictionnaires
    """
    # Construire la requ√™te
    if categories and len(categories) > 0:
        query = " OR ".join([f"cat:{category}" for category in categories])
        print(f"üîç Recherche d'articles pour les cat√©gories: {categories}")
        print(f"üîç Mode de filtrage: {'Strict (cat√©gorie principale uniquement)' if strict_category else 'Inclusif (toutes cat√©gories)'}")
        
        # Informer l'UI si le callback existe
        if callback:
            callback(f"Recherche dans {len(categories)} cat√©gories: {', '.join(categories)}")
    else:
        query = "all"  # Tous les articles
        print("üîç Recherche sur toutes les cat√©gories ArXiv")
        if callback:
            callback("Recherche sur toutes les cat√©gories")
    
    print(f"üéØ R√©cup√©ration des {max_articles} articles les plus r√©cents...")
    
    all_articles = []
    seen_ids = set()  # Pour suivre les articles d√©j√† vus
    start = 0
    batch_size = 100  # Taille maximale par requ√™te selon l'API ArXiv
    empty_response_count = 0
    max_empty_responses = 5  # Arr√™t apr√®s 5 r√©ponses vides cons√©cutives
    duplicates_count = 0
    
    while len(all_articles) < max_articles and empty_response_count < max_empty_responses:
        try:
            batch_num = len(all_articles) // batch_size + 1
            print(f"üîÑ R√©cup√©ration du lot {batch_num}: articles {start+1} √† {start+batch_size}")
            if callback:
                callback(f"R√©cup√©ration du lot {batch_num}: articles {start+1} √† {start+batch_size}")
            
            # R√©cup√©rer un lot d'articles
            xml_data = fetch_recent_arxiv_articles(query=query, max_results=batch_size, start=start)
            articles_batch = parse_arxiv_articles(
                xml_data, 
                target_categories=categories, 
                strict_category=strict_category
            )
            
            if not articles_batch:
                empty_response_count += 1
                print(f"‚ö†Ô∏è Lot vide ({empty_response_count}/{max_empty_responses})")
                if callback:
                    callback(f"Lot {batch_num} vide ({empty_response_count}/{max_empty_responses})", level="warning")
                
                if empty_response_count >= max_empty_responses:
                    print("üõë Trop de lots vides cons√©cutifs, arr√™t de la r√©cup√©ration")
                    if callback:
                        callback("Trop de lots vides cons√©cutifs, arr√™t de la r√©cup√©ration", level="warning")
                    break
                start += batch_size
                continue
            
            # R√©initialiser le compteur si on a des r√©sultats
            empty_response_count = 0
            
            # Ajouter uniquement les articles non vus
            added_count = 0
            new_duplicates = 0
            for article in articles_batch:
                article_id = article.get("id")
                if article_id and article_id not in seen_ids:
                    all_articles.append(article)
                    seen_ids.add(article_id)
                    added_count += 1
                else:
                    new_duplicates += 1
                    duplicates_count += 1
            
            print(f"üìä Articles uniques ajout√©s dans ce lot: {added_count}")
            print(f"üìä Total d'articles uniques: {len(all_articles)}/{max_articles}")
            
            if callback:
                # Envoyer des informations d√©taill√©es sur ce lot
                categories_in_batch = {}
                for article in articles_batch:
                    cat = article["category"]
                    if cat in categories_in_batch:
                        categories_in_batch[cat] += 1
                    else:
                        categories_in_batch[cat] = 1
                
                cat_details = ", ".join([f"{cat}: {count}" for cat, count in categories_in_batch.items()])
                callback(f"Lot {batch_num}: {added_count} nouveaux articles, {new_duplicates} doublons ({cat_details})", 
                        batch_info={"total": len(all_articles), "duplicates": duplicates_count})
            
            # Si on n'a pas ajout√© de nouveaux articles, possible que nous ayons atteint la fin
            if added_count == 0:
                print("‚ö†Ô∏è Aucun nouvel article unique dans ce lot, passage au suivant...")
                if callback:
                    callback("Aucun nouvel article unique dans ce lot", level="warning")
                    
                if len(articles_batch) < batch_size:
                    print("üõë Moins d'articles que demand√© et pas de nouveaux articles, arr√™t de la r√©cup√©ration")
                    if callback:
                        callback("Fin des r√©sultats atteinte", level="warning")
                    break
            
            # Passer au lot suivant
            start += batch_size
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la r√©cup√©ration du lot: {e}")
            if callback:
                callback(f"Erreur: {str(e)}", level="error")
            time.sleep(5)  # Attendre plus longtemps en cas d'erreur
    
    print(f"‚úÖ R√©cup√©ration termin√©e. {len(all_articles)} articles uniques r√©cup√©r√©s.")
    if callback:
        callback(f"R√©cup√©ration termin√©e avec {len(all_articles)} articles uniques", level="success")
    
    # Retourner uniquement le nombre demand√© d'articles (tri√©s par date)
    sorted_articles = sorted(
        all_articles,
        key=lambda x: x.get('published_date', ''), 
        reverse=True
    )
    return sorted_articles[:max_articles]

def filter_articles_by_keywords(articles, keywords, field="title"):
    """
    Filtre les articles par mots-cl√©s dans un champ sp√©cifi√©.
    """
    if not keywords:
        return articles
        
    filtered_articles = []
    for article in articles:
        text = article.get(field, "").lower()
        if any(keyword.lower() in text for keyword in keywords):
            filtered_articles.append(article)
            
    return filtered_articles

def filter_articles_by_keywords_multi(articles, keywords):
    """
    Filtre les articles par mots-cl√©s sur plusieurs champs simultan√©ment:
    titre, r√©sum√© et cat√©gories.
    
    Args:
        articles: Liste des articles √† filtrer
        keywords: Liste des mots-cl√©s recherch√©s
        
    Returns:
        Liste des articles correspondant √† au moins un mot-cl√©
    """
    if not keywords:
        return articles
    
    filtered_articles = []
    for article in articles:
        # Cr√©er un texte combin√© pour la recherche
        title = article.get("title", "").lower()
        summary = article.get("summary", "").lower()
        
        # Inclure toutes les cat√©gories pour la recherche
        all_categories = article.get("all_categories", [])
        categories_text = " ".join(all_categories).lower()
        
        # Rechercher dans tous les champs
        combined_text = f"{title} {summary} {categories_text}"
        
        # V√©rifier si au moins un mot-cl√© correspond
        if any(keyword.lower() in combined_text for keyword in keywords):
            filtered_articles.append(article)
    
    return filtered_articles

def export_to_json(articles, filename="arxiv_articles.json"):
    """
    Exporte les articles au format JSON.
    """
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"üìÅ Articles export√©s vers {filename}")

if __name__ == "__main__":
    # Liste des cat√©gories principales li√©es √† l'IA
    target_categories = ["cs.AI", "cs.LG", "cs.CV", "cs.CL", "cs.NE", "cs.RO", "stat.ML"]

    try:
        # R√©cup√©rer les articles les plus r√©cents sans duplication
        articles = get_latest_arxiv_articles(
            categories=target_categories,
            max_articles=200,  # Par d√©faut, r√©cup√©rer les 200 articles les plus r√©cents
            strict_category=False  # Mode de filtrage par d√©faut
        )
        
        # Filtrer optionnellement par mots-cl√©s
        keywords = ["transformer", "llm", "large language model", "gpt", "attention"]
        filtered_articles = filter_articles_by_keywords_multi(articles, keywords)
        
        # Afficher un r√©sum√©
        print(f"\nüìä R√©capitulatif:")
        print(f"   - Articles r√©cup√©r√©s: {len(articles)}")
        print(f"   - Articles filtr√©s par mots-cl√©s: {len(filtered_articles)}")
        
        # Afficher les 10 premiers articles
        print("\nüìë Aper√ßu des articles r√©cents:")
        for i, article in enumerate(articles[:10]):
            print(f"  {i+1}. {article['title']} ({article['category']})")
            print(f"     Date: {article['published_date']}")
            print(f"     Auteurs: {', '.join(article['authors'])}")
            print(f"     Lien: {article['link']}")
            print()
            
    except Exception as e:
        print(f"‚ùå Erreur globale: {e}")
