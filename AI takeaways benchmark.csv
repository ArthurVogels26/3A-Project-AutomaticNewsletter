Reception date,Link,Review priority,Category (Illuin),Category AI,Status,Reviewed,Topic / Keywords (Illuin),Topic / Keywords AI,Take-away (Illuin),Take-away AI,rouge1 precision,rouge2 precision,rougeL precision
25/01/2024,https://arxiv.org/pdf/2401.05856.pdf,1 - High,Method,Method,Processed,26/01/2024,RAG / Eval,,"Seven Failure Points When Engineering a Retrieval Augmented Generation System: these are: 
 - the answer is not in the documents and the LLM hallucinates,
 - the answer is in the documents but not presented to the LLM,
 - the answer is in the documents but not in the contexts presented to the LLM,
 - the LLM did not extract the answer from the documents
 - the LLM did not present the answer in the right format
 - the answer of the LLM does not have the right specificity (too specific or not enough)
 - the LLM did not extract all the answers in the contexts that ere presented
 They summarise several lessons learned from their experience: long contexts are better, add metadata to improve retrieval, use continuous monitoring, end-to-end training is better than assembly of several bricks","### 1. Introduction
Ce document aborde les défis rencontrés lors de la conception de systèmes de génération augmentée par récupération (RAG), qui combinent la recherche sémantique et les modèles de langage. Les auteurs présentent des études de cas dans les domaines de la recherche, de l'éducation et de la biomédecine, identifiant sept points de défaillance majeurs dans ces systèmes.

### 2. Points clés
- **Systèmes RAG** : Utilisent des documents pour améliorer les réponses générées par des modèles de langage comme ChatGPT.
- **Défaillances identifiées** : Comprennent le contenu manquant, le classement incorrect des documents, et des réponses incomplètes ou inappropriées.
- **Études de cas** : Trois études de cas illustrent ces points de défaillance, incluant des systèmes en cours d'utilisation à l'Université Deakin.
- **Recommandations** : Les auteurs soulignent l'importance de la validation continue et de la calibration des systèmes RAG.

### 3. Concision
Ce document présente les défis de l'ingénierie des systèmes RAG, identifiant sept points de défaillance à partir de trois études de cas. Les résultats soulignent la nécessité d'une validation opérationnelle et d'une adaptation continue des systèmes pour améliorer leur robustesse.",0.0258,0,0.0155
1/2/2024,https://arxiv.org/abs/2401.16658,1 - High,Model,Model,Processed,7/2/2024,Speech Model,,"Introduction de OWSM v3.1 base et 2 - Medium (100M et 1B respectivement), nouvelle version de modÃ¨les de type Open Whisper-style Speech Model mais qui utilise un E-Branchformer (Enhanced Branchformer, qui utilise des branches parallÃ¨les pour capturer les informations locales et globales du contexte et les combiner par convolution) Ã la place dâ€™un Transformer pour lâ€™encoding.
 Le modÃ¨le est entraÃ®nÃ© sur les mÃªmes donnÃ©es multilingues que la version prÃ©cÃ©dente OWSM v3. Lâ€™insight principal est quâ€™utiliser un E-Branchformer amÃ©liore les performances par rapport Ã la version prÃ©cÃ©dente sur la plupart des tÃ¢ches : speech recognition (anglais et multilingue), speech translation, long-form English speech recognition, et amÃ©liore la vitesse dâ€™infÃ©rence de lâ€™ordre de 25%. Moins performant que la v3 en identification de langues cependant.
 OWSM v3.1 reste globalement derriÃ¨re Whisper qui est de toute faÃ§on entraÃ®nÃ© sur beaucoup plus de donnÃ©es/des donnÃ©es de meilleure qualitÃ©.","1. **Introduction** : Ce document présente OWSM v3.1, une évolution des modèles de reconnaissance vocale de style Open Whisper, utilisant l'architecture E-Branchformer. L'objectif est d'améliorer les performances et l'efficacité des modèles précédents sans ajouter de nouvelles données.

2. **Points clés** :
   - OWSM v3.1 comprend des modèles de 100M à 1B de paramètres.
   - Il surpasse OWSM v3 dans la majorité des benchmarks d'évaluation, avec une augmentation de la vitesse d'inférence allant jusqu'à 25%.
   - L'architecture E-Branchformer a été choisie pour sa convergence rapide.
   - OWSM v3.1 montre des capacités émergentes en reconnaissance vocale contextuelle à zéro coup.
   - Le code, les modèles pré-entraînés et les journaux de formation seront publiquement disponibles pour favoriser la transparence.

3. **Concision** : OWSM v3.1 améliore significativement les performances des modèles de reconnaissance vocale précédents, tout en étant plus rapide, et met en avant des capacités émergentes dans des applications spécifiques.",0.4458,0.103,0.2229
30/01/2024,https://github.com/promptfoo/promptfoo,1 - High,Library,Library,Processed,30/01/2024,Eval,,promptfoo est une librarie open source avec une interface en ligne de commande permettant de tester et d'Ã©valuer la qualitÃ© des outputs de plusieurs LLMs sur plusieurs prompts en parallÃ¨le et restitue les rÃ©sultats sous forme de tableau comparatif. Des assertions et mÃ©triques peuvent Ãªtre configurÃ©es pour automatiquement scorer les outputs si applicable Ã la tÃ¢che Ã l'Ã©tude.,"1. **Introduction** : Le document présente `promptfoo`, un outil local destiné aux développeurs pour tester des applications utilisant des modèles de langage (LLM). Il vise à remplacer les approches de test par essais-erreurs et à faciliter la création d'applications d'IA sécurisées et fiables.

2. **Points clés** :
   - Outil pour tester les prompts et modèles avec des évaluations automatisées.
   - Permet de sécuriser les applications LLM via le red teaming et le scan de vulnérabilités.
   - Comparaison de différents modèles (OpenAI, Anthropic, etc.) en parallèle.
   - Intégration possible dans des processus CI/CD.
   - Fonctionne entièrement en local, garantissant la confidentialité des données.

3. **Concision** : `promptfoo` est un outil local pour tester et sécuriser des applications LLM, offrant des évaluations automatisées, des comparaisons de modèles et des rapports de vulnérabilité, tout en préservant la confidentialité des données.",0.1928,0.0182,0.0964
30/01/2024,https://twitter.com/rishdotblog/status/1752329471867371659,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,1/2/2024,Text2SQL,,"SQLCoder-70B, un Llama-Code 70B finetunÃ© pour une tÃ¢che de Text2SQL, atteint 93% de queries correctes dÃ©passant GPT-4 qui est Ã 84% sur le mÃªme test set.",,,,
30/01/2024,https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/BGE_M3/BGE_M3.pdf,1 - High,Model,Library,Processed,31/01/2024,Embeddings / Retrieval,,"BGE-M3 : nouveau modÃ¨le d'embedding Multi-lingue (100+ languages), Multi-granularities (input length jusqu'Ã 8192), Multi-Functionality (unification dense, lexical et multi-vec retrieval). C'est le premier modÃ¨le d'embedding qui supporte les 3 modes de retrieval, Nouveau SOTA sur les benchmarks multi-lingual (MIRACL) et cross-lingual (MKQA)","1. **Introduction** : Le document présente BGE (BAAI General Embedding), une boîte à outils pour la récupération d'informations et l'augmentation des modèles de langage (RAG). Il met en avant les modèles d'embedding et les projets associés, ainsi que des mises à jour récentes sur les développements et les publications.

2. **Points clés** :
   - Introduction de modèles multimodaux comme **BGE-VL** pour des applications de recherche visuelle.
   - Lancement de nouveaux modèles d'embedding, y compris **bge-en-icl** et **bge-multilingual-gemma2**, avec des capacités d'apprentissage en contexte.
   - Publication de benchmarks comme **MLVU** pour l'évaluation de la compréhension des vidéos longues.
   - Améliorations continues des modèles avec des mises à jour régulières et des tutoriels en cours de développement.

3. **Concision** : BGE est une plateforme innovante pour l'embedding et la recherche, intégrant des modèles avancés et des outils d'évaluation pour améliorer les performances des modèles de langage.",0.1022,0.0054,0.0591
31/01/2024,https://twitter.com/imhaotian/status/1752621754273472927,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,1/2/2024,Multimodal,,"LLaVA-1.6 with improved reasoning, OCR, and world knowledge. It supports 1 - Higher-res inputs, more tasks, and exceeds Gemini Pro on several benchmarks. LLaVA-1.6 comes with base LLMs of different sizes: Mistral-7B, Vicuna-7B/13B, Hermes-Yi-34B",,,,
31/01/2024,https://twitter.com/CaimingXiong/status/1752380193447260311,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,1/2/2024,Embeddings / Retrieval,,"SFR-Embedding-Mistral : Nouveau modÃ¨le d'embedding basÃ© sur Mistral 7B, SoTA en Retrieval et Reranking",,,,
31/01/2024,https://x.com/arthurmensch/status/1752737462663684344?s=46https://x.com/arthurmensch/status/1752737462663684344?s=46,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,1/2/2024,Leak,,"Un utilisateur de 4chan a uploadÃ© les poids d'un modÃ¨le nommÃ© ""Miqu"" en affirmant qu'il s'agissait d'un leak du modÃ¨le Mistral-2 - Medium de Mistral. Il s'agirait d'un modÃ¨le Llama 2 70B quantisÃ© reffinetunÃ© sur un dataset inconnu. Les premiers tests des utilisateurs s'avÃ¨rent trÃ¨s bons, et le modÃ¨le tallone GPT-4 sur plusieurs benchmarks, dÃ©passant les meilleurs modÃ¨les open source du moment. La rumeur de leak dâ€™un modÃ¨le de chez mistral sâ€™avÃ¨re Ãªtre vraie, il s'agit d'une early version du modÃ¨le Mistral 2 - Medium qui aurait Ã©tÃ© dÃ©ployÃ© chez un client qui l'aurait leak.",,,,
1/2/2024,https://twitter.com/bindureddy/status/1752908801811091594,1 - High,Method,,Erreur: Erreur lors de la récupération de l'article de blog: 400,1/2/2024,RAG,,"RÃ©sultats assez contrintuitifs : ajouter des documents non-pertinents permettrait dâ€™amÃ©liorer les perfs des systÃ¨mes de RAG. Le nombre optimal de documents pertinents ne doit pas dÃ©passer entre 2 et 5 en gÃ©nÃ©ral dâ€™aprÃ¨s le papier, trop de documents liÃ©s Ã la query mais pas strictement pertinent crÃ©Ã© de la confusion pour le LLM",,,,
2/2/2024,Open Source Research Toolkit ðŸ¤—,1 - High,Model,,Erreur: Failed to parse: Open Source Research Toolkit ðŸ¤—,6/2/2024,Vision-Language Models,,"Famille de modÃ¨le multimodaux reprenant le paradigme Sparse Mixture of Experts. Le papier fournit de nombreux insights sur l'influence des diffÃ©rents Ã©lÃ©ments de l'architecture sur les performances du modÃ¨le.
 DÃ©mo : https://huggingface.co/spaces/LanguageBind/MoE-LLaVA",,,,
2/2/2024,https://huggingface.co/RWKV/v5-Eagle-7B,1 - High,Model,Model,Processed,6/2/2024,Open models,,"Un modÃ¨le 7b SOTA en multilingue basÃ© sur une architecture RWKV (non Transformer) (v5).
  - Meilleur que la plupart des modÃ¨les 7b sur des benchmarks multilingues (~25 langues couvertes, 61.4% dâ€™accuracy contre 58.2% pour Mistral 7b, gros jump de performance par rapport Ã la v4).
  - La performance sur les benchmarks anglais se rapproche des modÃ¨les 7b entraÃ®nÃ©s sur le mÃªme ordre de tokens (1T).
 Les RWKV sont une architecture de type RNN qui a lâ€™avantage de :
  - Ãªtre moins gourmande en ressources (VRAM, CPU, GPU, etc) quâ€™un Transformer Ã lâ€™entraÃ®nement et Ã lâ€™infÃ©rence. 10 Ã 100x moins demandant en ressources de calcul quâ€™un pour des grandes tailles de contexte.
  - scaler linÃ©airement par rapport Ã la taille de contexte
  - est nÃ©anmoins trÃ¨s sensible au prompt formatting (lookback nÃ©cessaire)","1. **Introduction** : Le document présente le modèle RWKV-5 Eagle 7B, un modèle de traitement du langage naturel basé sur l'architecture RWKV-v5, conçu pour être écoénergétique et efficace en termes de coûts d'inférence.

2. **Points clés** :
   - Modèle de 7,52 milliards de paramètres.
   - Architecture RWKV-v5, un transformateur linéaire.
   - Considéré comme le modèle 7B le plus écologique au monde.
   - Entraîné sur 1,1 trillion de tokens dans plus de 100 langues (70 % en anglais).
   - Surpasse tous les modèles 7B dans les benchmarks multilingues.
   - Performances proches de modèles plus grands comme Falcon et LLaMA2 en évaluations en anglais.
   - Modèle de base nécessitant un ajustement supplémentaire pour des cas d'utilisation spécifiques.

3. **Concision** : Le modèle RWKV-5 Eagle 7B est une avancée majeure en traitement du langage, alliant performance et durabilité, mais nécessite des ajustements pour des applications spécifiques.",0.3625,0.1006,0.175
31/01/2024,https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard,1 - High,Benchmark,Answer: Method,Processed,8/2/2024,Safety Benchmark,,"Ce benchmark mesure la capacitÃ© des LLMs Ã rÃ©pondre de maniÃ¨re sÃ©curisÃ©e, selon 8 dimensions:
 
 * non-ToxicitÃ©,
 * non suivi de stÃ©rÃ©otype,
 * robustesse aux inputs adversariaux,
 * robustesse aux inputs hors distribution,
 * robustesse aux exemples few-shot erronÃ©s,
 * respect de la confidentialitÃ© des donnÃ©es,
 * respect de valeurs morales,
 * respect de lâ€™Ã©quitÃ©.","Bien sûr, je peux vous aider avec cela. Cependant, vous n'avez pas fourni le texte à résumer. Pourriez-vous le partager afin que je puisse créer le résumé selon la structure demandée ?",0.1622,0,0.0811
2/2/2024,https://www.linkedin.com/posts/soumith_if-you-have-questions-about-why-meta-open-sources-activity-7158947265791873024-hdQz?utm_source=share&amp;utm_2 - Medium=member_ioshttps://www.linkedin.com/posts/soumith_if-you-have-questions-about-why-meta-open-sources-activity-7158947265791873024-hdQz?utm_source=share&amp;utm_2 - Medium=member_ios,1 - High,Business,Library,Processed,6/2/2024,Open-Source; GAFAM,,Zuck explique pourquoi il mise sur lâ€™open source.,"### Résumé

1. **Introduction** : Ce document discute de l'utilisation des cookies par LinkedIn et des tiers, ainsi que de l'importance de l'open source dans le domaine de l'intelligence artificielle, comme mentionné par Mark Zuckerberg lors d'un appel sur les résultats de Meta.

2. **Points clés** :
   - LinkedIn utilise des cookies essentiels et non essentiels pour améliorer ses services et afficher des publicités pertinentes.
   - Les utilisateurs peuvent accepter ou refuser les cookies non essentiels et modifier leurs choix à tout moment.
   - La stratégie d'open source de Meta est perçue comme un mouvement audacieux pour rivaliser avec OpenAI.
   - Les commentaires des utilisateurs soulignent des perspectives variées sur l'open source et son impact sur l'automatisation et l'expérience utilisateur.

3. **Concision** : Le texte aborde l'utilisation des cookies par LinkedIn et l'open source chez Meta, soulignant les implications stratégiques et les opinions des utilisateurs sur ces sujets.",0.0303,0.0183,0.0303
2/2/2024,https://www.ft.com/content/80ca3bcd-e819-4002-90b3-3584239180aa?segmentid=45a55daa-06c5-0aba-131a-a1eb758674aehttps://www.ft.com/content/80ca3bcd-e819-4002-90b3-3584239180aa?segmentid=45a55daa-06c5-0aba-131a-a1eb758674ae,1 - High,Business,Pedagogy,Processed,6/2/2024,GAFAM,,Premiers dividendes versÃ©s par Meta alors que leurs dÃ©penses R&D atteignent des montants qui inquiÃ¨tent les investisseurs.,"1. **Introduction** : Le document présente des informations variées sur des sujets d'actualité mondiale, notamment les tensions géopolitiques, l'économie et les développements technologiques. Il met en lumière des enjeux cruciaux comme le réarmement en Europe et les prévisions économiques des États-Unis.

2. **Points clés** :
   - L'UE exclut les États-Unis, le Royaume-Uni et la Turquie d'un fonds de réarmement de 150 milliards d'euros.
   - Trump suggère que les États-Unis pourraient prendre le contrôle des centrales nucléaires ukrainiennes.
   - La Réserve fédérale américaine abaisse ses prévisions de croissance en raison des politiques de Trump.
   - Nvidia prévoit d'investir des centaines de milliards dans la chaîne d'approvisionnement américaine.

3. **Concision** : Ce document aborde des enjeux géopolitiques et économiques majeurs, en mettant l'accent sur les décisions stratégiques de l'UE et des États-Unis, ainsi que sur les investissements technologiques significatifs.",0.0429,0,0.0368
5/2/2024,https://www.numerama.com/tech/1622750-gemini-pro-larme-de-google-contre-chatgpt-et-gpt-4-arrive-en-france.html,1 - High,Business,Model,Processed,6/2/2024,LMM: GAFAM,,Gemini Pro arrive en France,"### Résumé

1. **Introduction :**  
Le document traite de l'expansion de Google dans le domaine de l'intelligence artificielle avec le lancement de son modèle de langage Gemini Pro, qui vise à rivaliser avec ChatGPT d'OpenAI. 

2. **Points clés :**  
   - **Gemini Pro :** Déployé en France, il remplace PaLM2 et permet des interactions en plusieurs langues.  
   - **Avancées technologiques :** Inclut des capacités de traitement du texte, de l'image et du son.  
   - **Concurrence :** Google cherche à rattraper son retard face à OpenAI, après une année 2023 difficile.  
   - **Améliorations d'Imagen 2 :** Nouveau modèle de génération d'images capable de créer des photos réalistes.  
   - **Futur :** Prévisions pour l'intégration de Gemini dans Google Assistant et le lancement imminent de Gemini Ultra.

3. **Concision :**  
Google intensifie sa compétition avec OpenAI grâce à Gemini Pro, un modèle de langage polyvalent, et améliore sa génération d'images. Des intégrations futures avec Google Assistant sont prévues.",0.0194,0.0098,0.0194
5/2/2024,https://arxiv.org/pdf/2402.00838.pdf,1 - High,Model,Model,Processed,6/2/2024,Open models,,"OLMo est un nouveau framework de prÃ©-entrainement, d'instruction finetuning et d'Ã©valuation totalement open-source fait par l'Allen Institute. Suivant ce framework, l'Ã©quipe OLMo a open-sourcÃ© un modÃ¨le 1B et 4 modÃ¨les 7B entrainÃ©s avec diffÃ©rentes configurations software et hardware. Tous ces modÃ¨les sont entraÃ®nÃ©s sur le dataset DoLMA avec au moins 2T de tokens en anglais qui a Ã©tÃ© Ã©galement open-source.
 Ils ont pu entraÃ®nÃ© deux modÃ¨les sur un cluster de GPUs AMD et un cluster NVIDIA et ont obtenu les mÃªmes rÃ©sultats sur les deux clusters.
 Ils recommandent une architecture proche de celle des modÃ¨les Llama sans biais, avec des non-parametric layer norm, des fonctions d'activation SwiGLU, des embedding RoPE et tokenizer basÃ© sur le Byte-pair encoding de GPT-NeoX-20B avec des tokens pour masquer les donnÃ©es personnelles.
 Un modÃ¨le de 65B paramÃ¨tres est en cours d'entrainement.","### 1. Introduction
Le document présente OLMo, un modèle de langage ouvert conçu pour faciliter l'étude scientifique des modèles de langage. Dans un contexte où les modèles propriétaires sont devenus courants, OLMo vise à fournir un accès complet aux données d'entraînement, au code et aux résultats d'évaluation, permettant ainsi aux chercheurs d'explorer les biais et les risques associés à ces modèles.

### 2. Points clés
- **Modèle** : OLMo, avec des variantes de 1B et 7B paramètres.
- **Méthode** : Utilisation d'une architecture de transformateur avec des améliorations spécifiques, entraînée sur un corpus ouvert appelé Dolma.
- **Résultats** : OLMo-7B se montre compétitif sur plusieurs tâches de compréhension et de génération de langage, surpassant de nombreux modèles similaires.
- **Ouverture** : Publication des poids du modèle, des données d'entraînement, et des outils d'évaluation pour encourager la recherche ouverte.

### 3. Concision
OLMo est un modèle de langage ouvert qui facilite l'étude scientifique des modèles de langage en fournissant un accès complet aux données et aux outils. Avec des variantes de 1B et 7B paramètres, OLMo utilise une architecture de transformateur améliorée et a été entraîné sur le corpus Dolma. Les résultats montrent qu'OLMo-7B est compétitif sur diverses tâches, surpassant plusieurs modèles similaires, tout en promouvant la recherche ouverte.",0.395,0.1206,0.19
5/2/2024,https://arxiv.org/pdf/2402.00159.pdf,1 - High,Dataset,Dataset,Processed,6/2/2024,Open Dataset,,"Dataset open-source pour entrainer des LLMs (notamment les modÃ¨les OLMo) en anglais avec une toolkit pour filtrer le dataset et faire des analyses variÃ©es. Le dataset est constituÃ© de 3T de tokens en anglais extraits de sources diverses telles que des pages web, des papiers scientifiques, du code, des livres, des rÃ©seaux sociaux et des ressources issues dâ€™encyclopÃ©dies.","### 1. Introduction
Le document présente Dolma, un corpus ouvert de trois billions de tokens, conçu pour la recherche sur le préentraînement des modèles de langage. Il vise à améliorer la transparence des données utilisées pour entraîner les modèles de langage, en fournissant des détails sur la composition et la méthodologie de collecte des données.

### 2. Points clés
- **Corpus** : Dolma est constitué de contenu diversifié, incluant des pages web, des articles scientifiques, du code, des livres du domaine public, des réseaux sociaux et des matériaux encyclopédiques.
- **Méthodologie** : Une approche rigoureuse de filtrage et de déduplication a été appliquée pour garantir la qualité des données.
- **Résultats** : Des analyses expérimentales montrent que la composition des données influence les performances des modèles de langage, soulignant l'importance d'une documentation adéquate des pratiques de curation.

### 3. Concision
Dolma est un corpus de trois billions de tokens destiné à la recherche sur les modèles de langage, intégrant divers types de contenu. Sa méthodologie de curation vise à assurer la qualité des données, et les résultats expérimentaux révèlent l'impact significatif de la composition des données sur les performances des modèles.",0.1832,0.0421,0.0942
9/2/2024,https://email2.theinformation.com/e/c/eyJlbWFpbF9pZCI6ImRnU2kwUVlDQU9XeWItU3lid0dOamIxZkNQeEVBMWF5WjFwa1VwMD0iLCJocmVmIjoiaHR0cHM6Ly93d3cudGhlaW5mb3JtYXRpb24uY29tL2JyaWVmaW5ncy9vcGVuYWktY2VvLWFsdG1hbi1zYWlkLXRvLWJlLWluLXRhbGtzLXRvLXJhaXNlLXRyaWxsaW9ucy1mb3ItYWktY2hpcC1wb3dlci1pbml0aWF0aXZlP3V0bV9jYW1wYWlnbj0lNUJSRUJSQU5EJTVEKyU1QlRJLUFNJTVEK1RoXHUwMDI2dXRtX2NvbnRlbnQ9MTA5NVx1MDAyNnV0bV9tZWRpdW09ZW1haWxcdTAwMjZ1dG1fc291cmNlPWNpb1x1MDAyNnV0bV90ZXJtPTEyNCIsImludGVybmFsIjoiYTJkMTA2M2NhNGM0MDFlNWIyNmYiLCJsaW5rX2lkIjoxMjkwfQ/904f9f70d1ac43ee9fd4de4025dd8c2f6dba0c4dbbb355f98fadf2ce1b16d0d7OpenAI,1 - High,Business,,Erreur: Erreur lors de la récupération de l'article de blog: 403,,,,"CEO Altman Said to be in Talks to Raise Trillions for AI Chip, Power Initiative
 OpenAI CEO Sam Altman has spoken with investors about raising trillions of dollars to fund an effort to expand the worldâ€™s manufacturing capacity to make semiconductors for artificial intelligence, according to a report in The Wall Street Journal.
 The effort could require $5 trillion to $7 trillion and a sprawling partnership between OpenAI, chipmakers, power providers and governments. Altman has spoken to investors including the United Arab Emirates government and SoftBank CEO Masayoshi Son, as well as chip-making giant Taiwan Semiconductor Manufacturing Co. about the ambitious project, according to the report. His vision would be to raise money from Middle East investors and have TSMC build and run chip-fabrication plants, which could number in the dozens.
 He has also discussed the venture with U.S. Commerce Secretary Gina Raimondo, who has been closely involved in government efforts to promote U.S. chipmaking. A spokesperson for OpenAI didnâ€™t immediately respond to a request for comment.
 Altman is trying to solve one of the key bottlenecks hindering OpenAIâ€™s growth. For the past year, companies working on AI have complained about the lack of Nvidia graphics processing units. More recently people working in cloud computing have warned that power availability is an even bigger constraint. Altman has personally invested in startups that are trying to create energy using cutting edge technology such as nuclear fusion.",,,,
12/2/2024,https://arxiv.org/pdf/2402.00841.pdf,1 - High,Benchmark,Method,Processed,14/02/2024,Small finetuned models VS zero-shot LLM,,"Les auteurs comparent des petits modÃ¨les finetunÃ©s (FLAN T5 <1B) Ã des plus gros modÃ¨les open-source (Llama 2, Mixtral) en zero-shot sur la tÃ¢che de rÃ©sumÃ© de transcripts de meetings.
 Ils montrent que les petits modÃ¨les finetunÃ©s dÃ©passent les gros modÃ¨les sur le test set dans le mÃªme domaine que le dataset d'entrainement mais gÃ©nÃ©ralisent moins bien sur un dataset out of domain.
 Notez que la mÃ©trique utilisÃ©e est le ROUGE score qui est une mÃ©trique basÃ©e sur le token matching entre la gÃ©nÃ©ration et une rÃ©fÃ©rence et ne prend pas vÃ©ritablement en compte le sens globale du rÃ©sumÃ©.
  Une analyse plus dÃ©taillÃ© est faite dans ce tweet: https://twitter.com/rasbt/status/1756316089393270853?s=51&t=THiV4sETaorA7anic27uQA","1. **Introduction** : Ce document explore l'efficacité des modèles de langage de grande taille (LLMs) et des modèles plus compacts dans la tâche de résumé de réunions. Il examine si les modèles plus petits peuvent rivaliser avec des modèles plus grands tout en réduisant les coûts de déploiement dans des environnements industriels.

2. **Points clés** :
   - Les LLMs plus petits, même après ajustement, n'atteignent généralement pas la performance des LLMs plus grands en mode zéro-shot, sauf pour FLAN-T5 (780M paramètres).
   - FLAN-T5 se distingue en offrant des performances comparables à celles de LLMs beaucoup plus grands (7B à 70B paramètres) tout en étant plus léger.
   - L'étude utilise des ensembles de données réels et propose des résumés générés par GPT-4 comme références.
   - Les résultats montrent que FLAN-T5 est une solution rentable pour le déploiement industriel.

3. **Concision** : FLAN-T5, un modèle compact, surpasse souvent des LLMs plus grands en résumé de réunions, offrant une alternative efficace et économique pour les entreprises.",0.2705,0.0583,0.1449
12/2/2024,https://github.com/NVIDIA/NeMo-Guardrails/blob/develop/docs/security/guidelines.md,1 - High,Method,Library,Processed,14/02/2024,Safety,,"Guidelines de sÃ©curitÃ© pour utiliser un LLM faisant accÃ¨s Ã un service tiers comme une base de donnÃ©es, une API Search ou un calculateur:
 - Ne pas rÃ©vÃ©ler les problÃ¨mes d'accÃ¨s au service tiers dans l'output du LLM quand les requÃªtes fail
 - Logger toutes les interactions LLM/API
 - Bien transmettre les autorisations de l'utilisateur accÃ©dant au LLM au service tiers pour qu'il ne puisse pas accÃ©der au service tiers sans autorisation
 - Bien contraindre les variables envoyÃ©es Ã l'API pour ne pas envoyer des inputs qu'on n'a pas envie
 - Eviter les actions demandÃ©es par le LLM qui changent l'etat du service tiers comme droppÃ© une table, le download de fichiers... Si il est nÃ©cessaire de faire un changement persistent, bien valider le format de l'input envoyÃ© Ã l'API
 - Isoler les infos d'authentification du LLM","### Introduction
Le document présente NeMo Guardrails, un outil open-source conçu pour intégrer des ""gardrails programmables"" dans des applications conversationnelles basées sur des modèles de langage de grande taille (LLM). Ces gardrails permettent de contrôler les réponses des modèles en évitant des sujets indésirables et en suivant des flux de dialogue prédéfinis.

### Points clés
- **Fonctionnalités** : Ajout de gardrails pour contrôler les entrées, les dialogues et les sorties des LLM.
- **Protection** : Mécanismes contre les vulnérabilités des LLM, comme les jailbreaks et les injections de prompts.
- **Cas d'utilisation** : Applications dans le question-réponse, assistants spécifiques, et intégration avec LangChain.
- **Langage Colang** : Utilisé pour définir les dialogues et les gardrails de manière flexible.
- **Installation** : Nécessite Python 3.9 à 3.11 et des outils de développement C++.

### Concision
NeMo Guardrails est un outil destiné à renforcer la sécurité et la fiabilité des applications LLM en ajoutant des gardrails programmables. Il permet de contrôler les dialogues, protège contre les vulnérabilités et s'intègre facilement avec d'autres outils comme LangChain.",0.2733,0.0058,0.1163
12/2/2024,https://docs.garak.ai/garak/,1 - High,Library,Library,Processed,14/02/2024,Safety,,"Librairie qui rÃ©pertorie de nombreuses attaques adversariales pouvant Ãªtre faites sur un LLM et qui propose un scan de LLM qui donne le pourcentage de rÃ©sistance Ã chaque type de mÃ©thode.
 Bien adaptÃ©e aux use-cases type ChatBot mais moins adaptÃ©e Ã d'autres cas d'usage.","1. **Introduction** : Ce document présente garak, un scanner de vulnérabilités pour les modèles de langage (LLM). Il vise à aider les utilisateurs à identifier les faiblesses et les comportements indésirables dans les technologies basées sur les modèles de langage.

2. **Points clés** :
   - Garak permet de scanner des chatbots ou des modèles pour détecter les vulnérabilités.
   - Il fournit un rapport complet sur les performances et les points à améliorer.
   - Comprend des fonctionnalités comme des tests de base, l'injection de prompts, la génération de toxicité, et des tests automatiques.
   - Propose des outils pour gérer les résultats des scans et évaluer les vulnérabilités.
   - Inclut des sections sur le red-teaming et des ressources d'assistance.

3. **Concision** : Garak est un outil essentiel pour analyser et améliorer la sécurité des modèles de langage, offrant des rapports détaillés sur les faiblesses identifiées et des méthodes pour les corriger.",0.0909,0,0.0485
13/02/2024,https://twitter.com/VikParuchuri/status/1757185570940567666,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,14/02/2024,OCR,,"Surya OCR est un nouveau modÃ¨le d'OCR basÃ© sur une architecture Donut modifiÃ©e, entraÃ®nÃ©e sur un grand nombre de documents y compris scannÃ©s et supporte jusqu'Ã 4 langues dans le mÃªme passage. Surya est prÃ©cis sur plus de 90 langues et surpasse de loin Tesseract, Ã la fois en dÃ©tection de ligne et en retranscription. Aucune comparaison quantiative sur d'autres modÃ¨les OCR plus rÃ©cents n'est proposÃ©es mais des comparaisons qualitatives faites par l'auteur semblent indiquer que le modÃ¨le surpasse EasyOCR et PaddleOCR. Les poids du modÃ¨le sont sous license cc-by-nc-sa-4.0 mais accessible pour pour les entreprises sous les 5 millions$ de chiffre d'affaire dans les 12 derniers mois.",,,,
14/02/2024,https://openai.com/blog/memory-and-new-controls-for-chatgpt,1 - High,Method,,Erreur: Erreur lors de la récupération de l'article de blog: 403,14/02/2024,ChatGPT / Memory,,"OpenAI rajoute progressivement la capacitÃ© Ã ChatGPT de persister des infos Ã travers plusieurs conversations. Cette fonctionalitÃ© est disponible pour une petite partie des utilisateurs de l'offre Free et Plus et il est possible d'opt-out. Techniquement, cette feature est modÃ©lisÃ©e par un nouvel outil nommÃ© `bio` auquel ChatGPT a accÃ¨s et qui permet de persister n'importe quelle information pendant une session de chat. Les informations persistÃ©es sont ensuite passÃ©es en contexte dans les nouvelles sessions de chat.",,,,
15/02/2024,https://github.com/huggingface/text-generation-inference/pull/1539,1 - High,Library,Library,Processed,15/02/2024,TGI / GuidedGeneration,,Le cÃ©lÃ¨bre serveur dâ€™infÃ©rence de LLM dâ€™HuggingFace vient tout juste dâ€™intÃ©grer la possibilitÃ© de faire de la gÃ©nÃ©ration guidÃ©e via une Regex ou plus gÃ©nÃ©ralement une grammaires non-contextuelles grÃ¢ce Ã lâ€™intÃ©gration de lâ€™excellente librairie Outlines. Cette intÃ©gration permet de combiner la scalabilitÃ© dâ€™un serveur dâ€™infÃ©rence avec les garanties apportÃ©es par la gÃ©nÃ©ration guidÃ©e. En particulier il est dÃ©sormais possible de spÃ©cifier un schÃ©ma JSON attendu en sortie et le decoding du modÃ¨le sera adaptÃ© pour garantir la validitÃ© du JSON produit.,"### Introduction
Le document présente ""Text Generation Inference"" (TGI), un serveur Rust, Python et gRPC destiné à l'inférence de génération de texte. Utilisé par Hugging Face, TGI permet de déployer des modèles de langage de grande taille (LLMs) pour des applications telles que Hugging Chat et l'API d'inférence.

### Points clés
- TGI supporte des modèles populaires comme Llama, Falcon, et GPT-NeoX.
- Caractéristiques : lancement simple, traçage distribué, parallélisme tensoriel, et streaming de tokens.
- Options de quantification pour réduire les besoins en mémoire.
- API compatible avec OpenAI pour des réponses de type chat.
- Support matériel pour Nvidia, AMD, et Google TPU.
- Installation locale possible via Docker ou Nix.

### Concision
TGI est un outil performant pour l'inférence de LLMs, facilitant le déploiement et l'optimisation de modèles de génération de texte.",0.2639,0.0559,0.1389
15/02/2024,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,1 - High,Model,Answer: Tips and Tricks,Processed,21/02/2024,"Gemini Pro 1.5, LLM",,"Nouvelle gÃ©nÃ©ration de modÃ¨les Gemini:
 - Gemini 1.5 Pro est un modÃ¨le multi-modal (texte, audio, video) reposant sur l'architecture mixture-of-experts entrainÃ© sur des contextes de l'ordre du million de tokens,
 - Il est notamment capable d'extraire des informations diluÃ©es dans des trÃ¨s long contextes en atteignant un recall quasi parfait sur la tÃ¢che ""Needle in a Haystack"" sur les trois modalitÃ©s de donnÃ©es
 - Le modÃ¨le est bien meilleur que Gemini Pro 1.0 (27/31 benchmarks meilleurs) et meilleur que Gemini Ultra 1.0 sur de nombreux benchmarks notamment 10/13 benchmarks textuels","Bien sûr, je peux vous aider avec cela. Cependant, je n'ai pas accès au texte que vous souhaitez résumer. Pourriez-vous s'il vous plaît me fournir le texte en question ?",0.3,0,0.2
15/02/2024,https://huggingface.co/spaces/Qwen/Qwen-VL-Max,1 - High,Model,Model,Processed,22/02/2024,Language-Vision Models,,"- Follow-up de Qwen VL et de Qwen VL Plus
 - LVLM en anglais et chinois
 - Focus sur la partie Visual QA notamment pour des images Ã grande rÃ©solution
 - PrÃ©tendumment SOTA pour de nombreuses variantes pertinentes de VQA (text, chart, ...)
 - Vient avec Touchstone, un benchmark pour modÃ¨les multi-modaux construit Ã partir d'annotation humaines et d'Ã©valuations gÃ©nÃ©rÃ©es par GPT-4
 - License custom â†’ usage commercial OK si â‰¤ 100M users, sinon une requete specifique doit etre formulÃ©e","1. Introduction : Le document traite du modèle Qwen/Qwen-VL-Plus, qui est une avancée dans le domaine des modèles d'intelligence artificielle. Il se concentre sur les capacités de ce modèle en matière de traitement du langage et de compréhension visuelle.

2. Points clés :
   - Qwen/Qwen-VL-Plus combine des fonctionnalités de traitement du langage naturel et de vision par ordinateur.
   - Le modèle utilise des techniques avancées d'apprentissage automatique pour améliorer la compréhension contextuelle.
   - Des tests ont montré des performances supérieures par rapport à d'autres modèles similaires dans des tâches de génération de texte et d'analyse d'images.
   - Il est conçu pour être adaptable à divers domaines d'application, allant de la recherche à l'assistance virtuelle.

3. Concision : Ce document présente le modèle Qwen/Qwen-VL-Plus, qui allie traitement du langage et vision. Il utilise des techniques d'apprentissage avancées, offrant de meilleures performances dans des tâches complexes et s'adaptant à plusieurs applications.",0.1732,0.0393,0.1006
16/02/2024,https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/,1 - High,Model,Model,Processed,22/02/2024,Video Embedding,,"- vJEPA est un modÃ¨le d'embedding de videos 
 - Le pitch est assez haut-niveau mais trÃ¨s rÃ©miniscent du Masked Language Modeling dans l'idÃ©e (on masque une portion de la vidÃ©o pour permettre un entraÃ®nement non-supervisÃ©)
 - Encore early stage mais release avec une licence non-commerciale","1. **Introduction** : Ce document présente le modèle V-JEPA (Video Joint Embedding Predictive Architecture), une avancée vers l'intelligence machine avancée (AMI) selon la vision de Yann LeCun. Ce modèle vise à améliorer la compréhension des interactions dans le monde physique à travers l'analyse vidéo.

2. **Points clés** :
   - V-JEPA est un modèle non génératif qui prédit des parties manquantes de vidéos, apprenant par auto-supervision avec des données non étiquetées.
   - Il améliore l'efficacité d'apprentissage par rapport aux modèles précédents, nécessitant moins d'exemples étiquetés.
   - Le modèle excelle dans la reconnaissance d'interactions détaillées entre objets et permet des évaluations figées, facilitant l'adaptation à de nouvelles tâches.
   - Des recherches futures incluront l'intégration de données audio et l'extension des capacités de planification sur des horizons temporels plus longs.

3. **Concision** : V-JEPA représente une avancée significative dans l'intelligence machine, permettant une meilleure compréhension des vidéos par des prédictions abstraites et une efficacité d'apprentissage accrue. Ce modèle, publié sous une licence Creative Commons, ouvre la voie à de futures recherches multimodales et à des applications dans l'IA incarnée.",0.1154,0.0242,0.0625
19/02/2024,https://twitter.com/Muennighoff/status/1758307967802224770?t=gjXyc6ntmJJ31rm2GkIgEw&amp;s=03,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,19/02/2024,"Text embedding, RAG",,"(Model) GritLM : nouveau LLM qui unie gÃ©nÃ©ration de texte et embedding de texte
 -> Existe en deux versions : 7B et 8x7B
 -> Finetune des modÃ¨les Mistral 7B et Mixtral 8x7B
 -> Les 2 modÃ¨les font parties des meilleurs modÃ¨les dans les deux catÃ©gories, bien qu'ils ne soient pas les meilleurs partout Ã proprement parler
 -> Mais leurs performances sont trÃ¨s impressionantes pour des modÃ¨les 2 in 1
 -> Comparant GritLM 8x7B Ã d'autres LLM (Llamas, Zephyr, Mistral, Mixtral, Tulus), sur un ensemble de benchmarks (MMLU, GSM8K, BBH, HumanEval, Alpaca, ...) GritLM 8x7B ressort numÃ©ro 1 sur la moyenne de ces benchmarks, son concurrent le plus proche Ã©tant Tulu 2 70B
 -> License MIT",,,,
19/02/2024,https://substack.com/redirect/fdd10e8c-70c0-4f24-b396-880bf0c108a7?j=eyJ1IjoieWE2OWoifQ.2lTA-dBFt69lIbJmN0tiRn1AIdNnBVQhVGlbaEqBhYc,1 - High,Business,Library,Processed,21/02/2024,GPU Cloud,,Lambda annonce la levÃ©e de 320 millions de dollars pour le dÃ©veloppement d'un cloud GPU avec des GPU Nvidia Quantum-2 InfiniBand.,"### 1. Introduction
Le document présente les services et produits offerts par Lambda, une entreprise spécialisée dans l'infrastructure GPU pour l'intelligence artificielle. Il met en avant leurs solutions cloud, matériel et initiatives récentes, notamment un financement important pour développer leur plateforme.

### 2. Points clés
- **Services Cloud** : Clusters GPU à la demande, facturation à la minute, et services Kubernetes gérés pour l'IA.
- **Matériel** : Systèmes NVIDIA DGX et serveurs Scalar avec des GPU NVIDIA personnalisables.
- **Stations de travail** : Desktops et stations de travail GPU, tels que Vector Pro et Vector One, adaptés pour l'apprentissage profond.
- **Financement** : Levée de fonds de 320 millions de dollars pour développer une plateforme de calcul IA de premier plan.
- **Vision** : Accélérer l'accès aux GPU NVIDIA pour les équipes d'ingénierie IA et soutenir l'innovation dans le domaine.

### 3. Concision
Lambda se concentre sur la création d'une plateforme de calcul IA leader, offrant des services cloud et du matériel spécialisé. Avec un financement récent de 320 millions de dollars, l'entreprise vise à faciliter l'accès aux ressources GPU pour les développeurs et à soutenir la croissance des startups et des entreprises Fortune 500 dans l'IA.",0.0968,0.0463,0.0783
19/02/2024,https://substack.com/redirect/f4644c8f-a938-4063-93e0-a43b91666584?j=eyJ1IjoieWE2OWoifQ.2lTA-dBFt69lIbJmN0tiRn1AIdNnBVQhVGlbaEqBhYc,1 - High,Business,Library,Processed,22/02/2024,,,[Business] LangChain raised $25 million in new funding led by Sequoia Capital.,"1. **Introduction** : Ce document présente LangChain, un cadre de programmation open-source conçu pour faciliter le développement d'applications basées sur des modèles de langage de grande taille (LLMs). Il souligne l'importance d'un cadre structuré pour améliorer l'efficacité des développeurs dans le domaine de l'IA générative.

2. **Points clés** :
   - LangChain permet aux équipes d'ingénierie d'accéder à des ensembles de données et d'exécuter des chaînes d'appels pour des applications complexes.
   - Plus de 50 000 applications LLM ont été créées avec LangChain, couvrant divers cas d'utilisation.
   - Le produit LangSmith aide à déboguer et surveiller les applications LLM, avec 70 000 utilisateurs inscrits depuis son annonce.
   - LangServe et LangGraph sont des outils récents pour déployer et construire des applications multi-acteurs.

3. **Concision** : LangChain est un cadre open-source qui facilite le développement d'applications LLM, avec plus de 50 000 applications créées. Le produit LangSmith améliore la performance des applications, tandis que LangServe et LangGraph offrent des solutions pour le déploiement et la construction d'applications.",0.0048,0,0.0048
19/02/2024,https://www.linkedin.com/posts/yanis-labrak-8a7412145_huggingface-datascience-deeplearning-activity-7165269672303566848-SVDt?utm_source=share&amp;utm_medium=member_androidhttps://www.linkedin.com/posts/yanis-labrak-8a7412145_huggingface-datascience-deeplea,1 - High,Model,Model,Processed,28/02/2024,"LLM, SantÃ©",,"BioMistral, une collection de LLMs open-source basÃ©s sur Mistral 7b pour le domaine mÃ©dical et prÃ©entraÃ®nÃ©s sur des donnÃ©es de PubMed Central.
 Plusieurs variations du modÃ¨le de base (quantized, model-merged)
 Meilleur que Mistral 7b instruct sur 8 des 10 tÃ¢ches de few-shot QA mÃ©dical. Performances proches de GPT 3.5 turbo en SFT.
 License Apache 2.0","**Introduction :**  
Le document présente le lancement du modèle de langage biomédical BioMistral, développé par des chercheurs de l'Université d'Avignon et d'autres institutions, qui est désormais disponible sur Hugging Face et Arxiv.

**Points clés :**  
- Lancement du modèle BioMistral 7B, un modèle de langage biomédical multilingue.  
- Performances de pointe dans des tâches de réponse à des questions médicales en anglais et dans sept autres langues.  
- Utilisation de techniques de Zero-Shot, Few-Shot et Supervised Fine-Tuning.  
- Pré-entraînement effectué sur le superordinateur GENCI Jean-Zay avec 5 000 heures GPU.  
- Soutien du projet MALADES de l'Agence nationale de la recherche française.  
- Prochaines tutoriels sur les modèles de langage biomédicaux annoncés.

**Concision :**  
Ce résumé présente le modèle BioMistral et ses capacités, ainsi que le soutien institutionnel et les futures ressources éducatives.",0.25,0.071,0.1346
20/02/2024,https://arxiv.org/abs/2402.12366,1 - High,Method,Method,Processed,22/02/2024,RLAIF,,"- Papier trÃ¨s intÃ©ressant qui dÃ©cortique le RLAIF de faÃ§on critique.
 - Ils montrent que le gain dÃ» Ã lâ€™Ã©tape de RL est souvent artificiellement dÃ» au fait dâ€™utiliser un modÃ¨le plus faible (typiquement GPT-3.5) pour gÃ©nÃ©rer des donnÃ©es de SFT que pour gÃ©nÃ©rer de lâ€™AI feedback (typiquement GPT-4). Une simple pipeline de SFT avec GPT-4 est plus performante que les pipelines de RLAIF existantes pour la plupart des modÃ¨les open-source de base (Llama7B, Mistral7B, Mistral8x7B)
 - Sur AlpacaEval, RL GPT-3.5 + AIF GPT-4 est moins performant que SFT GPT-4, et RL GPT-4 + AIF GPT-4 nâ€™augmente pas substantiellement les performances.","1. **Introduction** : Ce document évalue de manière critique l'efficacité du feedback basé sur l'IA pour l'alignement des modèles de langage à grande échelle, en particulier en examinant le paradigme de l'apprentissage par renforcement avec feedback IA (RLAIF).

2. **Points clés** :
   - La méthode RLAIF combine un ajustement supervisé initial (SFT) avec un apprentissage par renforcement basé sur le feedback d'un modèle critique.
   - Les améliorations observées grâce à RLAIF proviennent principalement de l'utilisation d'un modèle enseignant plus faible (GPT-3.5) par rapport à un modèle critique plus performant (GPT-4).
   - Des expériences montrent que le SFT avec un modèle enseignant fort surpasse souvent RLAIF.
   - Les gains de RLAIF varient selon les familles de modèles de base et les protocoles d'évaluation.

3. **Concision** : L'étude remet en question l'efficacité du RLAIF, suggérant que le SFT avec des modèles performants pourrait être suffisant pour améliorer l'alignement des modèles de langage.",0.3073,0.0995,0.1667
20/02/2024,https://crfm.stanford.edu/2024/02/18/helm-instruct.html,1 - High,Benchmark,Method,Processed,20/02/2024,,,"HELM est un benchmark de LLM trÃ¨s rÃ©pandu dans la littÃ©rature, qui permet de positionner un LLM sur une Ã©chelle absolue de performance en l'Ã©valuant sur un ensemble de scÃ©narios et sur plusieurs mÃ©triques. Jusque maintenant, HELM Ã©valuait les LLM sur des rÃ©ponses relativement courtes. Helm Instruct permet de complÃ©menter HELM en Ã©valuant le LLM sur un ensemble de 7 scÃ©narios avec 5 mÃ©triques (helpfulness, understandability, completeness, conciseness, harmlessness) sur de la gÃ©nÃ©ration plus open-ended. 
 Helm Instruct utilise une Ã©chelle absolue contrairement Ã d'autres benchmarks comme LMSys Arena, qui utilisent des comparaisons entre paires de prÃ©dictions.
 Les auteurs utilisent plusieurs Ã©valuateurs (GPT-4, Claude 2, ainsi que des Ã©valuations humaines via Amazon MTurk et Scale AI). Les analyses montrent que ces diffÃ©rents Ã©valuateurs ne sont pas toujours en accord les uns avec les autres, particuliÃ¨rement sur les mÃ©triques de conciseness et understandability
 GPT-4 (0314) est le plus corrÃ©lÃ© avec le jugement humain (respectivement 0.66 et 0.56 de Pearson correlation avec les annotateurs de MTurk et Scale AI).
 Les LLM sont moins bons pour Ã©valuer les critÃ¨res de understandability et conciseness que pour Ã©valuer les critÃ¨res de harmlessness et completeness .","1. **Introduction** : Ce document présente le cadre d'évaluation HELM Instruct, conçu pour évaluer les modèles de langage suivant des instructions de manière multidimensionnelle et avec des notations absolues. L'objectif est de surmonter les limites des méthodes d'évaluation existantes.

2. **Points clés** :
   - HELM Instruct évalue les modèles en utilisant cinq critères : utilité, compréhension, complétude, concision et innocuité.
   - Les évaluateurs incluent des humains (via Amazon Mechanical Turk et Scale) et des modèles de langage (GPT-4 et Anthropic Claude v1.3).
   - GPT-4 (0314) se révèle être le meilleur modèle globalement, tandis qu'Anthropic Claude v1.3 excelle en compréhension et innocuité.
   - Les évaluations montrent des variations significatives entre les jugements des humains et des modèles, en particulier sur la concision et la compréhension.

3. **Concision** : Le cadre HELM Instruct propose une évaluation des modèles de langage suivant des instructions, utilisant des critères variés et des notations absolues. Les résultats révèlent que GPT-4 est le meilleur modèle, tandis qu'Anthropic Claude brille par sa compréhension. Les évaluations montrent des divergences notables entre évaluateurs humains et modèles.",0.4089,0.0625,0.1644
20/02/2024,https://github.com/datadreamer-dev/DataDreamer,1 - High,Library,Library,Processed,21/02/2024,Synthetic Data Generation,,"Librairie qui permet de crÃ©er et formaliser des worklows complexes de gÃ©nÃ©ration de donnÃ©es qui sont facilement reproductibles. Permet notamment de faire de l'attributed prompting, des gÃ©nÃ©rations variÃ©es Ã partir d'un mÃªme prompt, augmenter des datasets existants...","1. **Introduction** : Le document présente DataDreamer, une bibliothèque Python open-source dédiée à la génération de données synthétiques, à la création de flux de travail de formation et à l'alignement de modèles. Elle est conçue pour être simple, efficace et adaptée à la recherche.

2. **Points clés** :
   - **Création de flux de travail** : Permet de créer des flux de travail complexes avec des modèles de langage.
   - **Génération de jeux de données** : Facilite la création de jeux de données synthétiques pour de nouvelles tâches.
   - **Formation de modèles** : Propose des outils pour aligner, affiner et entraîner des modèles sur des données existantes ou synthétiques.
   - **Caractéristiques** : Simple, de qualité recherche, efficace, reproductible, et facilite le partage de jeux de données et de modèles.

3. **Concision** : DataDreamer est une bibliothèque puissante pour la génération de données synthétiques et la formation de modèles, alliant simplicité et efficacité, et permettant des workflows reproductibles.",0.1534,0.0585,0.1164
20/02/2024,https://arxiv.org/abs/2307.11088,1 - High,Method,Dataset,Processed,22/02/2024,Eval,,"L-Eval est une nouvelle mÃ©thode d'Ã©valuation pour les modÃ¨les de langage Ã long contexte. Elle comprend un dataset de questions/rÃ©ponses avec de longs contextes couvrant 20 tÃ¢ches. L'Ã©tude examine comment les approches dâ€™Ã©valuation Ã base de n-gram matching ou Ã base de ""LLM-as-a-judge"" gÃ©nÃ©ralisent lorsque le contexte est long. En effet, les LLM Ã©valuateur ne peuvent pas prendre lâ€™intÃ©gralitÃ© du contexte en entrÃ©e, lâ€™Ã©valuation se base donc uniquement sur la similaritÃ© entre la prÃ©diction et la ground truth, ce qui requiert une attention particuliÃ¨re pour que le LLM Ã©valuateur ne valorise pas la prÃ©sence dans la prÃ©diction dâ€™informations supplÃ©mentaires dont il ne peut pas vÃ©rifier la vÃ©racitÃ©.
 De plus, les modÃ¨les recevant des contextes longs ont tendance Ã gÃ©nÃ©rer des prÃ©dictions longues, et les Ã©valuations (par n-gram ou par LLM) ont tendance Ã Ãªtre moins corrÃ©lÃ©es avec les Ã©valuations humaines lorsque la diffÃ©rence de longueur entre la prÃ©diction et la ground truth est trop grande. Pour rÃ©aliser leur benchmark de seize modÃ¨les, des instructions indiquant la longueur de prÃ©diction attendue sont donc rajoutÃ©s au prompt, ce qui permet de rÃ©duire ce biais. Les rÃ©sultats du benchmark sont que GPT-4-32k surpasse clairement tous les autres modÃ¨les par une marge trÃ¨s significative sur les tÃ¢ches closed-ended. Pour les tÃ¢ches open-ended, Ã©tant donnÃ© que les textes d'entrÃ©e sont gÃ©nÃ©ralement plus longs et qu'une comprÃ©hension globale du contexte est nÃ©cessaire, Claude-100k bat tous les autres modÃ¨les, y compris GPT-4-32k.","1. **Introduction** : Le document présente L-Eval, une initiative visant à établir une évaluation standardisée pour les modèles de langage à long contexte (LCLMs). Alors que les modèles propriétaires comme GPT-4 montrent des performances solides, les modèles open-source sont encore en développement. L-Eval se concentre sur la construction de jeux de données et de métriques d'évaluation pour mieux mesurer les capacités des LCLMs.

2. **Points clés** :
   - Construction d'un ensemble d'évaluation comprenant 20 sous-tâches et 508 documents longs.
   - Propositions de nouvelles métriques d'évaluation, notamment l'évaluation améliorée par des instructions de longueur (LIE).
   - Étude comparative de 16 modèles, révélant un écart significatif entre les modèles open-source et commerciaux, notamment dans les tâches ouvertes.
   - Les résultats suggèrent que les métriques traditionnelles basées sur les n-grams ne reflètent pas bien le jugement humain.

3. **Concision** : L-Eval vise à standardiser l'évaluation des modèles de langage à long contexte. Un nouvel ensemble de données et des métriques d'évaluation ont été développés, montrant des lacunes entre les modèles open-source et commerciaux. Les résultats soulignent l'inefficacité des métriques basées sur les n-grams et l'importance d'une évaluation plus nuancée.",0.5115,0.162,0.235
21/02/2024,https://blog.llamaindex.ai/introducing-llamacloud-and-llamaparse-af8cedf9006b,1 - High,Method,Library,Processed,21/02/2024,PDF Parser,,LlamaIndex a sorti son parser de pdfs propriÃ©taire. Il est possible de le tester sur quelques exemples. On peut le garder en tÃªte pour comparer notre parser au leur.,"1. **Introduction** : Le document présente LlamaCloud et LlamaParse, des solutions innovantes pour le traitement et l'extraction de données complexes dans le cadre des applications d'IA générative. Ces outils visent à améliorer la gestion des données non structurées et à faciliter le développement d'applications d'IA avancées.

2. **Points clés** :
   - **LlamaParse** : Un parseur de documents conçu pour gérer des fichiers PDF complexes, intégrant des tableaux et des graphiques.
   - **API de gestion d'ingestion et de récupération** : Permet de charger, traiter et stocker des données facilement pour les applications d'IA.
   - **RAG (Retrieval-Augmented Generation)** : Une nouvelle approche pour améliorer la précision des résultats d'IA en utilisant des données structurées et non structurées.
   - **Partenariats** : Collaboration avec des entreprises comme DataStax et MongoDB pour intégrer ces solutions dans des systèmes existants.

3. **Concision** : LlamaCloud et LlamaParse offrent des solutions avancées pour le traitement de données complexes, facilitant le développement d'applications d'IA générative avec une meilleure précision et efficacité.",0.0406,0,0.0305
21/02/2024,https://blog.google/technology/developers/gemma-open-models/,1 - High,Model,Model,Processed,21/02/2024,LLM,,"Nouveaux LLMs open-source de Google - Gemma 2B & 7B avec licence permettant un usage commercial. Ils release aussi des versions instruct finetunÃ©es avec SFT+RLHF.
 - Architecture: longueur de contexte de 8192 tokens, architecture transformer classique + Multi-Query Attention pour 2B et multi head attention pour 7B, RoPE Embeddings, GeGLU Activations, Normalizer Location
 - peu d'infos sur le dataset: Pre-EntrainÃ©s et finetunÃ©s sur de l'anglais principalement
 - Performances meilleures que Llama 2 7B sur tous les benchmarks
 - Performances meilleures que Mistral 7B en moyenne. Le modÃ¨le est meilleur que Mistral 7B sur MMLU, MATH, HumanEval, similaires sur HellaSwag et moins bonnes sur GSM8k
 Attention tout de mÃªme, Maxime Labonne nous met en garde sur Linkedin (https://www.linkedin.com/posts/maxime-labonne_does-gemma-overfit-the-open-llm-leaderboard-activity-7166220798427402242-lJFm?utm_source=share&utm_medium=member_desktop) que Gemma s'avÃ¨re lÃ©gÃ¨rement dÃ©ceptif sur d'autres benchmarks plus corrÃ©lÃ©s au jugement humain (AGIEval et BigBench), et qu'il n'est pas Ã exclure que les bons rÃ©sultats sur le OpenLLM Leaderboard soient surestimÃ©s du fait d'un overfitting.","1. **Introduction** : Le document présente Gemma, une nouvelle génération de modèles ouverts développés par Google, conçus pour aider les développeurs et chercheurs à créer des applications d'intelligence artificielle de manière responsable.

2. **Points clés** :
   - Gemma inclut des modèles de taille 2B et 7B, avec des variantes pré-entraînées et ajustées.
   - Un kit d'outils pour une IA générative responsable est fourni, incluant des méthodes de classification de sécurité et des outils de débogage.
   - Les modèles peuvent être utilisés sur divers dispositifs, y compris des ordinateurs portables et le cloud Google.
   - Optimisation pour des performances de pointe sur des plateformes matérielles comme les GPU NVIDIA et les TPU de Google.
   - Accès gratuit via Kaggle, Colab, et crédits Google Cloud pour les chercheurs.

3. **Concision** : Gemma est conçu pour favoriser l'innovation en IA tout en respectant des normes de sécurité et de responsabilité, permettant ainsi une utilisation étendue et accessible.",0.3094,0.0333,0.1436
21/02/2024,https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO,1 - High,Model,Model,Processed,22/02/2024,LLM,,"Nous Research release un nouveau finetune de mistral 7B utilisant DPO par dessus l'excellent modÃ¨le OpenHermÃ¨s 2.5-Mistral 7B. Ce nouveau modÃ¨le surpasse lÃ©gÃ¨rement le modÃ¨le OpenHermÃ¨s sur les benchmarks AGIEval, BigBench Reasoning Test, GPT4All suite et TruthfulQA. Un excellent modÃ¨le 7B qui a de bonnes chances de devenir notre nouvelle recommandation par dÃ©faut, en drop-in replacement du modÃ¨le OpenHermÃ¨s prÃ©cÃ©demment recommandÃ© jusqu'ici.","1. **Introduction** : Ce document présente le modèle Nous Hermes 2, une intelligence artificielle avancée développée sur la base du modèle Mistral 7B, avec des améliorations notables après l'application de la méthode DPO.

2. **Points clés** :
   - Modèle : Nous Hermes 2 basé sur Mistral 7B DPO.
   - Entraînement : 1 million d'instructions de qualité GPT-4, principalement à partir de données synthétiques.
   - Améliorations : Performances accrues sur plusieurs benchmarks (AGIEval, BigBench, GPT4All, TruthfulQA).
   - Compatibilité : Utilise le format ChatML pour des dialogues structurés.
   - Exemple d'application : Rédaction de récits et interaction avec les utilisateurs.

3. **Concision** : Le modèle Nous Hermes 2 offre une intelligence artificielle améliorée, capable d'interagir efficacement grâce à des données de haute qualité et des benchmarks performants.",0.2552,0.0694,0.131
22/02/2024,https://arena.lmsys.org/,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 403,22/02/2024,"Mistral, LLM",,"Mistral lance discrÃ¨tement un nouveau modÃ¨le prototype nommÃ© Â« Mistral-next Â», dÃ©jÃ intÃ©grÃ© au LMSys Arena. Il faudra attendre plus d'annotation crowdsourcÃ©es dans l'arÃ¨ne pour positionner ce modÃ¨le par rapport aux autres. Mistral n'a pas encore communiquÃ© officiellement d'informations autour de ce modÃ¨le, y compris au sujet de son caractÃ¨re open source ou closed source.",,,,
23/02/2024,https://www.phind.com/blog/introducing-phind-70b,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 403,26/02/2024,Code Generation,,"Introducing Phind-70B â€“ closing the code quality gap with GPT-4 Turbo while running 4x faster
 
 Phind.com est un chatbot Ã destiation des dÃ©veloppeurs, capable de chercher des informations pertinentes sur Internet pour complÃ©ter et sourcer ses rÃ©ponses. RÃ©cemment, Phind a annoncÃ© Phind 70B, basÃ© sur CodeLLama 70B et dont le pretrainng a Ã©tÃ© poursuivi sur 50B tokens additionnels, avec une taille de sÃ©quence de 32K tokens.
 
 Sur le benchmark de gÃ©nÃ©ration de code HumanEval, Phind-70B atteint 82.3%, dÃ©passant GPT-4-Turbo (81,1%). Qualitativement, Phind 70B semblerait moins ""paresseux"" que son homologue d'OpenAI, et ne rechignerait pas Ã gÃ©nÃ©rer des solutions complÃ¨tes et dÃ©taillÃ©es.
 En terme de serving, le modÃ¨le est dÃ©ployÃ© par Phind sur un cluster de H100 et utilise le serveur d'infÃ©rence TensorRT-LLM de NVIDIA. Phind prÃ©voit de publier les poids de la version 34B de Phind dans les prochaines semaines, et planifie Ã©galement d'open sourcer la version 70B, sans donner de date cette fois-ci.
 Petit fun fact amusant Ã la fin de l'article...",,,,
23/02/2024,https://arxiv.org/pdf/2402.13446.pdf,1 - High,Method,Pedagogy,Processed,29/02/2024,LLM annotations survey,,"Survey sur lâ€™utilisation de LLMs pour lâ€™annotation de donnÃ©es:
 - analyse selon 3 dimensions: annotation de donnÃ©es avec LLM, Ã©valuation des annotations par LLM, and l'apprentissage sur des annotations gÃ©nÃ©rÃ©es par LLM
 - Liste de papiers dispos ici: https://github.com/Zhen-Tan-dmml/LLM4Annotation","### 1. Introduction
Le document traite de l'utilisation des grands modèles de langage (LLMs) pour l'annotation et la synthèse de données, soulignant leur potentiel à automatiser ces processus généralement coûteux et laborieux. Il se concentre spécifiquement sur l'application des LLMs dans le domaine de l'annotation de données, offrant une analyse approfondie des défis, des méthodes et des contributions.

### 2. Points clés
- **Modèles étudiés** : GPT-4, LLaMA, et d'autres LLMs.
- **Méthodes** : Annotation générée par LLMs, évaluation des annotations générées, et utilisation des annotations pour le fine-tuning des modèles.
- **Contributions** : Taxonomie des types de données annotables, revue des stratégies d'apprentissage, et discussion des défis techniques et éthiques liés à l'utilisation des LLMs.
- **Impact social** : Considérations éthiques concernant les biais et les implications sociétales des annotations générées par les LLMs.

### 3. Concision
Ce document explore le potentiel des LLMs pour automatiser l'annotation et la synthèse de données, en se concentrant sur leur application spécifique dans ce domaine. Il présente des méthodes, des défis et des contributions clés, tout en abordant les implications éthiques et sociales de leur utilisation.",0.1667,0.0964,0.1313
26/02/2024,https://arxiv.org/pdf/2402.14658.pdf,1 - High,Model,Model,Processed,29/02/2024,"Code Generation, Multi-turn",,"OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement
 Introduction de OpenCodeInterpreter, une famille de modÃ¨les de code open-source conÃ§us pour gÃ©nÃ©rer, exÃ©cuter et affiner itÃ©rativement du code.
 - L'objectif est de combler le fossÃ© entre les modÃ¨les open-source et les systÃ¨mes propriÃ©taires, comme GPT-4 Code Interpreter, en matiÃ¨re de capacitÃ©s d'exÃ©cution et de raffinement itÃ©ratif.
 - Sept modÃ¨les sont disponibles : 4 basÃ©s sur CodeLlama et 3 basÃ©s sur Deepseek Coder, avec des tailles allant de 1.3B Ã 70B paramÃ¨tres
 - Le dataset d'entraÃ®nement, Code-Feedback, contient 68k interactions multi-tours qui incluent Ã la fois les instructions de l'utilisateur et les retours du compilateur.
 - Licence Apache 2.0","**Introduction :**  
Le document présente OpenCodeInterpreter, un système open-source innovant qui intègre la génération, l'exécution et le raffinement itératif de code, surmontant les limitations des modèles open-source existants.

**Points clés :**  
- OpenCodeInterpreter est soutenu par un ensemble de données de rétroaction, Code-Feedback, contenant 68 000 interactions multi-tours.  
- Il excelle dans des benchmarks comme HumanEval et MBPP, atteignant une précision de 83.2 (76.4) et rivalisant avec GPT-4.  
- Le modèle intègre à la fois des diagnostics d'exécution et des retours humains pour affiner dynamiquement le code.  
- Une évaluation approfondie montre que OpenCodeInterpreter réduit l'écart de performance entre les modèles open-source et les systèmes propriétaires.

**Concision :**  
OpenCodeInterpreter représente une avancée majeure dans la génération de code, combinant exécution et retour humain pour un raffinement dynamique, avec des performances comparables à celles de GPT-4.",0.4393,0.1802,0.2081
26/02/2024,https://mistral.ai/news/mistral-large/,1 - High,Model,Model,Processed,29/02/2024,,,"MistralAI a rÃ©cemment rendu publique sa plate-forme proposant notamment leur dernier modÃ¨le Mistral Large:
  - Mistral Large a des capacitÃ©s proches (mais en deÃ§a) de GPT-4 sur la plupart des benchmarks citÃ©s (MMLU, WinoG, HellaS, ...)
  - Un accent particulier semble avoir Ã©tÃ© mis sur ses capacitÃ©s multilingues (mais pas de comparaison Ã GPT-4 de disponible pour l'instant)
  - Les poids du modÃ¨le ne sont pas disponibles et le modÃ¨le lui-mÃªme ne peut-Ãªtre utilisÃ© qu'Ã travers une API (Mistral ou Azure)
  - Globalement, le modÃ¨le semble avoir Ã©tÃ© conÃ§u comme un drop-in replacement Ã GPT-4. Il dispose d'un mode JSON et de features de function calling.","1. **Introduction** : Le document présente le lancement de Mistral Large, le modèle phare de Mistral, qui offre des capacités de raisonnement avancées et est disponible sur la plateforme Mistral et Azure.

2. **Points clés** :
   - Mistral Large est un modèle de génération de texte de pointe, performant dans des tâches multilingues complexes.
   - Il se classe au deuxième rang mondial des modèles accessibles via une API, juste après GPT-4.
   - Capacité multilingue : maîtrise de l'anglais, français, espagnol, allemand et italien.
   - Fenêtre de contexte de 32K tokens pour un rappel d'informations précis.
   - Partenariat avec Microsoft pour une distribution sur Azure.
   - Lancement d'un modèle optimisé, Mistral Small, pour des charges de travail à faible latence.
   - Introduction de formats JSON et d'appels de fonctions pour des interactions plus complexes.

3. **Concision** : Mistral Large, le modèle phare de Mistral, offre des capacités de raisonnement avancées et est disponible sur la plateforme Mistral et Azure. Il excelle dans des tâches multilingues et se classe au deuxième rang mondial des modèles API. Mistral Small, un modèle optimisé pour la latence, a également été lancé, avec des fonctionnalités telles que le format JSON et les appels de fonctions.",0.2905,0.0899,0.1508
27/02/2024,https://x.com/_akhaliq/status/1762349999919071528?t=PEKDyR5m0Qnfe8S7N5EfAg&amp;s=31,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,29/02/2024,Structured Knowledge Grounding,,"StructLM est une sÃ©rie de modÃ¨les entraÃ®nÃ©s pour rÃ©aliser des tÃ¢ches sur des sources de donnÃ©es structurÃ©es telles que des tableaux, des ontologies et des bases de donnÃ©es. 
 - Leur Ã©tude rÃ©vÃ¨le que les capacitÃ©s des LLM sur cette tÃ¢che, appelÃ©e Structured Knowledge Grounding (SKG), laissaient encore Ã dÃ©sirer : ChatGPT par exemple est en retard d'une moyenne de 35 % par rapport Ã l'Ã©tat de l'art.
 - Un dataset de 1.1 millions instructions a Ã©tÃ© utilisÃ© pour entraÃ®ner trois modÃ¨les de tailles 7B, 13B et 34B basÃ©s sur l'architecture de Code-LLaMa.
 - Le modÃ¨le 34B n'est pas significativement meilleur que le modÃ¨le 7B, l'augmentation de la taille du modÃ¨le ne semble donc pas Ãªtre une condition suffisante pour amÃ©liorer les performances sur cette tÃ¢che difficile.
 - License MIT 
 - Lien vers le HF https://huggingface.co/collections/TIGER-Lab/structlm-65dcab5a183c499cc365fafc",,,,
6/2/2024,https://twitter.com/bindureddy/status/1754665925834690907,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,7/2/2024,LLM,,"Smaug72B est un nouveau modÃ¨le finetunÃ© par AbacusAI, par dessus un modÃ¨le le modÃ¨le moreh/MoMo-72B-lora-1.8.7-DPO, lui-mÃªme basÃ© sur Qwen-72B.
 Il est notamment meilleur que Mistral 2 - Medium sur MMLU, HellaSwag et particuliÃ¨rement bon en maths sur GSM-8K.",,,,
27/02/2024,https://www.gartner.com/en/newsroom/press-releases/2024-02-19-gartner-predicts-search-engine-volume-will-drop-25-percent-by-2026-due-to-ai-chatbots-and-other-virtual-agents,1 - High,Business,,Erreur: Erreur lors de la récupération de l'article de blog: 403,29/02/2024,,,"- Selon Gartner, d'ici 2026, le volume des recherches sur les moteurs de recherche traditionnels diminuera de 25 % en raison de l'utilisation croissante de chatbots et d'autres agents virtuels basÃ©s sur l'IA.
 - Les agents virtuels remplaceront progressivement les recherches traditionnelles, ce qui obligera les entreprises Ã revoir leur stratÃ©gie de marketing.
 - Les moteurs de recherche devront davantage valoriser la qualitÃ© du contenu pour compenser la quantitÃ© de contenu gÃ©nÃ©rÃ© par IA",,,,
27/02/2024,https://arxiv.org/pdf/2312.01552.pdf,1 - High,Method,Method,Processed,29/02/2024,In context learning,,"SystÃ¨me de prompting URIAL pour aligner des base models sans instruct tuning:
 - Les auteurs du papier ont remarquÃ© que les distributions de tokens des modÃ¨les prÃ©-entrainÃ©s et des modÃ¨les instruct-finetunÃ©s ne sont pas trÃ¨s Ã©loignÃ©es.
 - Les diffÃ©rences se font surtout sur des tokens stylistiques
 - URIAL se base sur l'in-context learning en promptant le base model avec un sytem prompt bien dÃ©taillÃ© et des exemples de sortie attendues bien ""stylisÃ©es""
 - un dataset just-eval-instruct est crÃ©Ã© pour Ã©valuer les modÃ¨le selon 6 dimensions: helpfulness, clarity, factuality, depth, engagement, and safety avec GPT-4 et GPT3.5. Ce dataset est un merge de 5 datasets existants: AlpacaEval2, MT-Bench, LIMA, HH-RLHF-redteam et MaliciousInstruct
 - mistral urial 7b est bien meilleur que mistral 7b instruct sur ce dataset et urial llama 2 70b est on par avec le llama 2 70b instruct sur ces 6 dimensions","1. **Introduction** : Ce document traite de la réévaluation de l'alignement des modèles de langage de grande taille (LLMs) via un apprentissage contextuel sans ajustement. Les auteurs remettent en question l'efficacité des méthodes d'alignement traditionnelles, telles que le fine-tuning supervisé et l'apprentissage par renforcement avec retour humain, en proposant une méthode alternative nommée URIAL.

2. **Points clés** :
   - **Modèle** : Utilisation de modèles de base Llama-2 et Mistral.
   - **Méthode** : URIAL permet d'aligner les LLMs sans ajustement en utilisant seulement quelques exemples contextuels stylisés.
   - **Résultats** : URIAL surpasse ou égalise les performances des modèles alignés par SFT et RLHF, tout en nécessitant moins de ressources.
   - **Hypothèse** : L'alignement traditionnel pourrait être superficiel, se concentrant principalement sur des éléments stylistiques.

3. **Concision** : Les résultats suggèrent que l'alignement des LLMs peut être réalisé efficacement sans tuning, ce qui ouvre la voie à des méthodes d'alignement plus économiques et moins coûteuses en ressources.",0.3099,0.0529,0.152
15/02/2024,https://www.shacknews.com/article/138730/openai-search-engine-google-bing,1 - High,Business,Answer: Method,Processed,,,,"OpenAI travaillerait avec Bing search sur un nouveau moteur de recherche, basÃ© sur ses propres technologies d'IA pour concurrencer Google","**Introduction :**  
Le texte traite de l'annonce selon laquelle OpenAI, créateur de ChatGPT, développe un moteur de recherche pour rivaliser avec Google. Cette initiative pourrait bouleverser le secteur de la recherche en ligne.

**Points clés :**  
- OpenAI travaille sur un service de recherche web, potentiellement alimenté par l'IA.  
- Le moteur de recherche pourrait être partiellement soutenu par Bing, le moteur de Microsoft, un partenaire clé d'OpenAI.  
- Suite à cette annonce, l'action de Google a chuté après la fermeture du marché.  
- Cette nouvelle pourrait avoir des conséquences significatives pour les entreprises technologiques impliquées.

**Concision :**  
OpenAI développe un moteur de recherche pour concurrencer Google, utilisant possiblement Bing comme support. L'annonce a entraîné une baisse des actions de Google, signalant l'impact potentiel sur le marché.",0.0774,0.039,0.0645
28/02/2024,https://arxiv.org/abs/2402.17733,1 - High,Model,Model,Processed,28/02/2024,Translation,,"- Ils proposent un modÃ¨le qui adresse des tÃ¢ches liÃ©es au domaine de la traduction, comme la post-Ã©dition, la correction grammaticale ou le NER, dans plusieurs langues. Release du dataset dâ€™entraÃ®nement TowerBlocks : https://huggingface.co/datasets/Unbabel/TowerBlocks-v0.1
 - Partant dâ€™un Llama-2, ils construisent TowerBase pretraining sur des donnÃ©es monolingues et parallÃ¨les, puis SFT sur des instructions liÃ©es Ã la traduction pour obtenir TowerInstruct.
 - RÃ©sultats : TowerInstruct 13B est meilleur que les grands modÃ¨les open-source (Llama 70B - Mixtral 8x7B) et les modÃ¨les dÃ©diÃ©s sur de la traduction de et vers lâ€™anglais. Sur de la post-Ã©dition et du NER multilingue, meilleur que GPT-3.5","1. **Introduction :** Ce document présente TOWER, un modèle de langage multilingue open-source conçu pour des tâches liées à la traduction. Il vise à combler l'écart entre les modèles de langage généralistes et ceux spécialisés dans la traduction.

2. **Points clés :**
   - TOWER est développé à partir de LLaMA-2, avec un pré-entraînement continu sur un corpus multilingue de 20 milliards de tokens.
   - La spécialisation est réalisée via un affinement sur un ensemble de données spécifiques aux tâches de traduction, nommé TOWERBLOCKS.
   - Le modèle TOWERINSTRUCT surpasse les alternatives open-source sur plusieurs tâches de traduction et est compétitif par rapport aux modèles fermés comme GPT-4.
   - TOWER inclut des résultats prometteurs pour des tâches telles que l'estimation de qualité, la post-édition automatique et la correction d'erreurs grammaticales.

3. **Concision :** TOWER est un modèle multilingue open-source optimisé pour des tâches de traduction, surpassant les alternatives open-source et rivalisant avec des modèles fermés. Il utilise un pré-entraînement continu et un affinement sur des données spécifiques pour atteindre des performances élevées.",0.3529,0.1129,0.2139
28/02/2024,https://huggingface.co/protectai/deberta-v3-base-prompt-injection,1 - High,Model,Model,Processed,29/02/2024,"Injection de prompts, Safety",,"ModÃ¨le DeBERTaV3 fine tunÃ© sur de la dÃ©tection d'injection de prompt : 
 - PrÃ©sente des performances remarquables sur l'ensemble d'Ã©valuation avec un F1-score de 0,9998.
 - Le dataset contenait ~30% de prompts avec injection et ~70% de prompts normaux.
 - Une intÃ©gration avec LangChain est disponible via la classe HuggingFaceInjectionIdentifier (https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)
 - License Apache 2.0","1. **Introduction** : Ce document présente une carte de modèle pour ""deberta-v3-base-prompt-injection"", une version fine-tunée du modèle DeBERTa de Microsoft, spécialisée dans la détection des injections de prompts.

2. **Points clés** :
   - Fine-tuning sur des ensembles de données combinant injections de prompts et prompts normaux.
   - Classification des entrées en deux catégories : `0` (aucune injection) et `1` (injection détectée).
   - Résultats sur l'ensemble d'évaluation : 
     - Perte : 0.0010
     - Précision : 99.99%
     - Rappel : 99.97%
     - F1 : 99.98%
   - Entraînement basé sur environ 30% d'injections de prompts et 70% de bons prompts.
   - Utilisation de la bibliothèque Transformers pour l'implémentation.

3. **Concision** : Le modèle vise à identifier les injections de prompts avec une très haute précision, mais sa performance peut varier selon la qualité des données d'entraînement.",0.2721,0.1027,0.1701
28/02/2024,https://huggingface.co/blog/starcoder2,1 - High,Model,Dataset,Processed,29/02/2024,Code Generation,,"- BigCode publie StarCoder2, entraÃ®nÃ© sur The Stack v2, le plus grand dataset open source de code conÃ§u pour le pretraining de modÃ¨le avec ses 4 trillions de tokens et ses 600+ languages de programmation.
 - Par rapport Ã The Stack v1, cette deuxiÃ¨me version du dataset contient plus d'Ã©chantillons, suit une procÃ©dure amÃ©liorÃ©e de dÃ©tection du langage et de la licence, ainsi qu'une meilleure heuristique de filtrage. 
 - Trois tailles de modÃ¨les sont disponibles : 3B, 7B, et 15B paramÃ¨tres
 - Licence : BigCode OpenRAIL-M v1 license (mÃªme que StarCoder)","1. **Introduction** : Le document présente StarCoder2, une nouvelle génération de modèles de langage open source pour le code, développée par BigCode. Ces modèles sont formés sur un ensemble de données de code de haute qualité, The Stack v2.

2. **Points clés** :
   - StarCoder2 comprend trois variantes : 3B, 7B et 15B paramètres.
   - Le modèle phare, StarCoder2-15B, a été entraîné sur plus de 4 trillions de tokens et plus de 600 langages de programmation.
   - Utilisation de Grouped Query Attention et d'une fenêtre de contexte de 16,384 tokens.
   - The Stack v2 est le plus grand ensemble de données de code ouvert, dérivé de l'archive Software Heritage.
   - BigCode est une collaboration entre Hugging Face et ServiceNow pour le développement responsable de modèles de langage.

3. **Concision** : StarCoder2, développé par BigCode, est une série de modèles de langage open source pour le code, formés sur The Stack v2, un vaste ensemble de données. Les modèles varient de 3 à 15 milliards de paramètres et intègrent des techniques avancées pour améliorer la performance.",0.3069,0.1596,0.1852
28/02/2024,https://x.com/emollick/status/1762885468528533573?s=46&amp;t=THiV4sETaorA7anic27uQA,1 - High,Business,,Erreur: Erreur lors de la récupération de l'article de blog: 400,29/02/2024,Service Client,,"Klarna AI est une entreprise amÃ©ricaine qui a intÃ©grÃ© OpenAI dans leur service client. D'aprÃ¨s leur communiquÃ© de presse:
 - l'assistant a eu 2.3 millions de conversations
 - il a pu rÃ©alisÃ© le travail de 700 agents
 - le score de satisfaction client est le mÃªme qu'avec des agents humains
 - rÃ©duction de 25% de requÃªtes rÃ©pÃ©tÃ©es
 - le temps de rÃ©solution des problÃ¨mes est passÃ© de 11min Ã 2 min
 Ces propos doivent Ãªtre nuancÃ©s car il s'agit d'un communiquÃ© de presse. Certains utilisateurs indiquent que le bot finit par donner la main Ã un humain dans de nombreux cas.",,,,
28/02/2024,https://www.linkedin.com/posts/mirco-ravanelli-489b692a_speechbrain-speechbrain-speech-activity-7168623278687768576-vMDp?utm_source=share&amp;utm_medium=member_android,1 - High,Library,Library,Processed,29/02/2024,Speech-to-Text,,"Deepmind a travaillÃ© avec plusieurs dÃ©veloppeurs de jeux video pour construire un agent capable de suivre des instructions en langage naturel dans diffÃ©rents environnements 3D:
 - A partir de ces jeux videos, un dataset d'actions a Ã©tÃ© compilÃ© avec des triplets (video de l'action, comandes Ã Ã©xecuter et instruction en langage naturel)
 -","**Introduction :** Ce document annonce la sortie de la version 1.0 de SpeechBrain, un outil open-source dédié à l'intelligence artificielle conversationnelle, développé par Mirco Ravanelli et son équipe.

**Points clés :**
- SpeechBrain 1.0 propose plus de 200 recettes et 100 modèles pré-entraînés sur Hugging Face.
- Améliorations notables en reconnaissance vocale, intégration de K2, et nouvelles techniques de recherche (CTC/joint attention Beam Search).
- Ajout de recettes et modèles pour le traitement de la parole, de l'audio, du texte et de l'EEG.
- Compatibilité renforcée avec des modèles Hugging Face comme GPT-2 et Llama-2.
- Nouvelles initiatives pour l'apprentissage continu et des benchmarks pour des modèles auto-supervisés en traitement de la parole.

**Concision :** La version 1.0 de SpeechBrain marque une avancée significative dans le domaine de l'IA conversationnelle, soutenue par des partenariats stratégiques et une communauté active.",0.0805,0.0116,0.0517
29/02/2024,https://huggingface.co/datasets/espnet/yodas,1 - High,Dataset,Dataset,Processed,29/02/2024,Audios,,"Des Ã©quipes de WavLab, un labo de recherche de Carnegie Mellon, ont regroupÃ© un dataset dâ€™audios de Youtube labellisÃ©s avec les sous-titres gÃ©nÃ©rÃ©s par youtube: 
 - le dataset couvre 140 langues diffÃ©rentes
 - il contient notamment ~21k heures de franÃ§ais, 167k h d'anglais, 37k h en espanhol, 20k h de portugais ou encore 14k d'allemand","### Introduction
Le document présente le jeu de données YODAS, qui contient 369 510 heures de discours provenant de vidéos YouTube, avec des transcriptions manuelles ou automatiques.

### Points clés
- **Mise à jour** : Une nouvelle version, YODAS2, a été publiée avec des audios non segmentés et un taux d'échantillonnage de 24 kHz.
- **Modes de chargement** : Deux modes sont disponibles pour charger le jeu de données : mode standard (téléchargement complet) et mode streaming (flux rapide).
- **Langues et sous-ensembles** : Le jeu de données comprend 149 langues, chacune divisée en sous-ensembles manuels et automatiques, identifiés par un chiffre dans le nom du sous-ensemble.
- **Statistiques** : Les heures de discours varient considérablement selon les langues, avec des sous-ensembles allant de quelques heures à plusieurs milliers.

### Concision
Le jeu de données YODAS offre une vaste collection d'audios et de transcriptions, accessible en mode standard ou streaming, et couvre 149 langues avec des sous-ensembles manuels et automatiques.",0.1773,0.0143,0.0922
19/01/2024,https://www.theverge.com/2024/1/18/24042354/mark-zuckerberg-meta-agi-reorg-interview,1 - High,Business,Model,Processed,19/01/2024,AGI,,"Meta cherche Ã crÃ©er une AGI dans un futur proche. Avant la fin de l'annÃ©e, Meta aura accumulÃ© 340,000 Nvidia H100 GPUs.
 Meta entraÃ®ne actuellement Llama3, qui aura des capacitÃ©s de gÃ©nÃ©ration de code.
 La stratÃ©gie open / close de ces futurs assets n'est pas claire.","### Introduction
Le document traite des ambitions de Mark Zuckerberg, PDG de Meta, concernant le développement de l'intelligence artificielle générale (AGI). Il souligne la concurrence intense dans le domaine de l'IA et la stratégie de Meta pour se positionner comme un leader.

### Points clés
- Zuckerberg vise à créer l'AGI sans avoir de définition précise ni de calendrier.
- Meta a déplacé son groupe de recherche en IA pour intégrer les produits d'IA générative.
- La société prévoit de posséder plus de 340 000 GPU Nvidia H100 d'ici la fin de l'année, surpassant ses concurrents.
- Meta adopte une approche open source pour ses modèles d'IA, contrastant avec la tendance de fermeture d'autres entreprises.
- Zuckerberg insiste sur le maintien de l'accent sur le métavers tout en développant l'IA.

### Concision
Zuckerberg de Meta se lance dans la création d'AGI, sans définition claire ni calendrier. Avec une stratégie open source, il prévoit d'utiliser plus de 340 000 GPU Nvidia pour surpasser la concurrence. Bien qu'il continue d'investir dans le métavers, il voit un avenir où l'interaction humaine inclut également des IA.",0.1436,0.0448,0.0941
4/3/2024,https://www.anthropic.com/news/claude-3-family,1 - High,Model,Model,Processed,6/3/2024,Claude 3,,"Anthropic a annoncÃ© la sortie de la famille des modÃ¨les Claude 3: Haiku (le plus petit), Sonnet et Opus (le plus grand). Ce sont des modÃ¨les multimodaux derriÃ¨re l'API Anthropic qui rivalisent avec les modÃ¨les d'OpenAI.
 - Anthropic annonce que le modÃ¨le Opus est meilleur que GPT-4 sur tous les benchmarks. NÃ©anmoins, d'autres chercheurs n'ont pas les mÃªme rÃ©sultats que Anthropic pour l'Ã©valuation de GPT-4 prÃ©sentÃ©s dans ce github. Les rÃ©sultats sont tout de mÃªme proches entre les deux modÃ¨les
 - Sur la modalitÃ© de Vision, Opus est proche de GPT4-Vision mais lÃ©gÃ¨rement en dessous des capacitÃ©s de Gemini Ultra 1.0 sur 3 benchmarks sur 5.
 - Les coÃ»ts sont trÃ¨s compÃ©titifs. Bien qu'Opus soit assez cher, Sonnet n'est pas loin de GPT-4 et 2 Ã 3 fois moins cher. Pour des applications demandant moins de raisonnement, Haiku est trÃ¨s intÃ©ressant car il a des performances bien meilleures que GPT-3.5 et il est 2 fois moins cher sur les input tokens.
 - Claude 2 Opus atteint un recall Ã 99% sur le benchmark ""Needle in a Haystack"" sur tout le contexte de 200k tokens","### Introduction
Le document annonce la famille de modèles Claude 3, qui établit de nouvelles normes dans le domaine de l'intelligence artificielle pour diverses tâches cognitives. Cette famille comprend trois modèles : Claude 3 Haiku, Claude 3 Sonnet et Claude 3 Opus, chacun offrant des performances de plus en plus puissantes.

### Points clés
- **Modèles disponibles** : Opus et Sonnet sont accessibles via l'API Claude, Haiku sera disponible prochainement.
- **Performance** : Opus surpasse ses concurrents sur de nombreux benchmarks d'évaluation, offrant une compréhension et une fluidité proches de celles des humains.
- **Vitesse** : Haiku est le modèle le plus rapide et économique, capable de traiter des documents complexes en moins de trois secondes.
- **Capacités visuelles** : Traitement avancé de divers formats visuels pour les entreprises.
- **Amélioration de l'exactitude** : Opus montre une amélioration de deux fois en précision par rapport à Claude 2.1.
- **Conception responsable** : Engagement envers la sécurité, la transparence et la réduction des biais.

### Concision
La famille Claude 3 introduit des modèles d'IA avancés, avec Opus comme le plus performant, offrant rapidité et précision. Les modèles sont conçus pour traiter des tâches complexes tout en respectant des normes de sécurité élevées.",0.4895,0.1429,0.2053
5/3/2024,https://twitter.com/NielsRogge/status/1765043331896095021,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,6/5/2024,DocumentAI,,"Le modÃ¨le UDOP, dont le papier est sorti en Mars 2023, est enfin disponible sur le HuggingFace Hub. UDOP est un modÃ¨le de traitement documentaire combinant les architectures LayoutLM et Donut. Il s'agit donc d'un modÃ¨le encodeur-dÃ©codeur comme Donut mais dont l'encodeur est capable de prendre en entrÃ©es les 3 modalitÃ©s de LayoutLM : texte, layout et vision, alors que Donut ne prenait en entrÃ©e que l'image de la page. Le dÃ©codeur est capable de rÃ©soudre des tÃ¢ches gÃ©nÃ©ratives de texte, layout et vision via du prompting en langage naturel. Le modÃ¨le UDOP surpasse les modÃ¨les LayoutLM et Donut sur un ensemble de tÃ¢ches de visual QA, table QA, layout recognition, extraction d'information...
 Le modÃ¨le est disponible en license MIT sauf la partie dÃ©codeur visuel qui ne sera disponible que via API.",,,,
5/3/2024,https://predibase.com/blog/lorax-outlines-better-json-extraction-with-structured-generation-and-lora,1 - High,Method,Method,Processed,7/3/2024,#ConstrainedGeneration,,"Article de blog dÃ©taillant les effets du fine-tuning et de la gÃ©nÃ©ration contrainte dans une tÃ¢che de NER formulÃ©e avec de l'extraction structurÃ©e en JSON:
  - le fine-tuning est en LoRA et la gÃ©nÃ©ration structurÃ©e est faite avec Outlines
  - la gÃ©nÃ©ration structurÃ©e seule amÃ©liore largement les performances du modÃ¨le (Mistral 7B) mais pas autant qu'un fine-tuning
  - utiliser une brique de gÃ©nÃ©ration structurÃ©e garantit mieux la structure des prÃ©dictions qu'un fine-tuning
  - faire de la gÃ©nÃ©ration structurÃ©e avec un modÃ¨le fine-tunÃ© amÃ©liore encore les performances","1. **Introduction** : Ce document traite de LoRAX, un serveur d'inférence open source pour modèles de langage, qui permet l'extraction et la génération de JSON structuré à l'aide de techniques de génération structurée et de fine-tuning. L'accent est mis sur l'amélioration de la précision des sorties JSON en combinant ces méthodes.

2. **Points clés** :
   - LoRAX prend en charge des adaptateurs fine-tunés sur un modèle de base partagé.
   - La génération structurée impose un schéma JSON, mais ne garantit pas la qualité du contenu.
   - Le fine-tuning améliore la précision du contenu, mais peut ne pas assurer la conformité avec le schéma.
   - La combinaison des deux méthodes permet d'obtenir des sorties JSON valides et précises.
   - Des benchmarks montrent que la méthode combinée surpasse les approches individuelles en termes de qualité et de conformité.

3. **Concision** : LoRAX permet l'extraction et la génération de JSON structuré à partir de modèles de langage. En combinant génération structurée et fine-tuning, les résultats montrent une amélioration significative de la qualité des sorties JSON, avec des performances optimales lors de l'utilisation de ces techniques ensemble.",0.372,0.1699,0.1884
6/3/2024,https://huggingface.co/blog/leaderboard-contextual,1 - High,Benchmark,Dataset,Processed,7/3/2024,Vision Language Models,,"Dataset crÃ©Ã© Ã l'UCLA pour les modÃ¨les multimodaux:
  - Le dataset vise Ã Ã©valuer les capacitÃ©s de raisonnement sur des Ã©lÃ©ments textuels prÃ©sents dans des images.
  - Il est conÃ§u pour Ãªtre difficile pour les modÃ¨les actuels: le meilleur modÃ¨le, GPT-4V, est Ã 20 points de moins que la performance humaine.
  - Les Ã©chantillons sont divisÃ©s en plusieurs sous-tÃ¢ches (Navigation, Utilisation du Web,...)
  - GPT-4V a un score parfait, au-delÃ de la performance humaine, pour la sous-tÃ¢che ""Raisonnement abstrait"" du benchmark 
  - Pas de licence: a priori, les donnÃ©es sont ouvertes","1. **Introduction** : Le document présente ConTextual, un ensemble de données conçu pour évaluer la capacité des modèles multimodaux à raisonner sur des images contenant du texte. L'objectif est de mesurer leur compréhension dans des tâches nécessitant une analyse conjointe du texte et du contexte visuel.

2. **Points clés** :
   - ConTextual comprend 506 instructions sur des images riches en texte, couvrant divers scénarios visuels.
   - Les modèles sont classés en trois catégories : approches augmentées LLM, LMMs propriétaires et LMMs open-source.
   - Les résultats montrent que les modèles modernes ont du mal à traiter ces tâches par rapport aux humains, en particulier dans des domaines comme la lecture de l'heure et les infographies.
   - Des recommandations pour améliorer les modèles incluent le développement d'encodeurs d'image avancés et une meilleure alignement vision-langage.

3. **Concision** : ConTextual évalue la capacité des modèles multimodaux à raisonner sur des images textuelles. Les résultats révèlent des lacunes significatives par rapport à la performance humaine, soulignant la nécessité d'améliorations dans l'alignement vision-langage.",0.2372,0.0607,0.1209
6/3/2024,https://www.mixedbread.ai/blog/mxbai-rerank-v1,1 - High,Model,Model,Processed,6/3/2024,"Reranking, Retrieving",,"Mixedbread.ai opensource une famille de modÃ¨les de reranking, dÃ©clinÃ©e en 3 tailles (xsmall, base et large) basÃ©e sur DeBERTa-v2. Un dÃ©monstrateur en ligne permet de tester les capacitÃ©s de reranking du modÃ¨le xsmall. Sur un subset de 11 datasets issus du benchmark BEIR, ces modÃ¨les se comparent trÃ¨s favorablement en reranking et offrent un trÃ¨s bon compromis entre performance et temps d'infÃ©rence. Le modÃ¨le mxbai-rerank-base-v1 permet de dÃ©passer le modÃ¨le propriÃ©taire d'embedding cohere-embed-v3, (72.3 vs 72.3 d'accuracy sur 11 datasets du benchmark BEIR)., mÃªme si cette comparaison est assez trompeuse : cohere-embed-v3 est un bi-encodeur alors qu'il s'agit ici d'un modÃ¨le de reranking. Le dÃ©tail des rÃ©sultats obtenus sont disponibles sur ce Google Sheet.
 Pas de rÃ©sultats sur la multilingualitÃ© du modÃ¨le...","### Résumé

1. **Introduction** : Le document annonce le lancement de modèles de reranking open-source par l'équipe Mixedbread, visant à améliorer la recherche sémantique tout en utilisant des infrastructures de recherche existantes.

2. **Points clés** :
   - Trois modèles de reranking : mxbai-rerank-xsmall-v1 (compact et efficace), mxbai-rerank-base-v1 (équilibre taille/performance), mxbai-rerank-large-v1 (meilleure précision).
   - Les modèles augmentent la pertinence des résultats de recherche en intégrant une étape de reranking après une recherche par mots-clés.
   - Évaluations montrent que ces modèles surpassent les systèmes de recherche traditionnels et d'autres modèles de taille similaire.
   - Facilité d'intégration : un simple ajout de code permet d'améliorer les performances de recherche.

3. **Concision** : Ces modèles visent à optimiser la recherche tout en restant compatibles avec les systèmes existants, offrant une solution simple et performante pour les entreprises.",0.337,0.0929,0.1739
6/3/2024,https://twitter.com/lmsysorg/status/1765098152775045612,1 - High,Benchmark,,Erreur: Erreur lors de la récupération de l'article de blog: 400,8/3/2024,,,"Avec lâ€™arrivÃ©e rÃ©cente de nombreux modÃ¨les propriÃ©taires, le classement de la ChatBot Arena Ã©volue:
 
 * Lâ€™Arena propose aux utilisateurs de comparer des gÃ©nÃ©rations provenant de modÃ¨les diffÃ©rents et de les classer. Cela permet de dÃ©river un leaderboard basÃ© sur des prÃ©fÃ©rences humaines.
 * Les derniÃ¨res itÃ©rations de GPT-4 restent au sommet, battant la meilleure version de Claude 3 dâ€™une courte tÃªte. Gemini Pro complÃ¨te le podium.
 * La suite du leaderboard comprend les versions plus anciennes de GPT-4, une autre version de Claude 3 et les derniers modÃ¨les de Mistral
 * Le premier modÃ¨le aux poids ouverts est le Qwen 72B dâ€™Alibaba et le premier modÃ¨le en licence libre est le Mixtral 8x7B d'e Mistral",,,,
8/3/2024,https://huggingface.co/urchade/gliner_multi,1 - High,Model,Model,Processed,14/03/2024,NER,,"-> ModÃ¨le Bi-directionnel de NER
 -> En 0-shot, outperform chatGPT et des LLMs finetunÃ©s sur la tÃ¢che
 -> Architecure de transformer encoder qui encode joitement les entitÃ©s ainsi que l'input sentence
 -> Cependant derniÃ¨re phase du modÃ¨le sÃ©parÃ©e pour les entitÃ©s et pour les tokens de l'input sentence
 -> Sur 20 datasets de benchmark de NER, en 0-shot, GLINER surpasse ChatGPT et UniNER-7B sur 13 d'entre eux, et reste assez proche de la concurrence sur les autres
 -> Pour sa taille de modÃ¨le, est trÃ¨s intÃ©ressant pour son approche bi-encoder
 -> License: cc-by-nc-4.0","1. **Introduction** : Ce document présente GLiNER, un modèle de reconnaissance d'entités nommées (NER) utilisant un encodeur transformateur bidirectionnel. GLiNER se distingue par sa capacité à identifier tout type d'entité, contrairement aux modèles NER traditionnels.

2. **Points clés** :
   - GLiNER est formé sur le jeu de données Pile-NER.
   - Il propose plusieurs versions avec différents nombres de paramètres (166M à 459M).
   - Les modèles sont disponibles en anglais et multilingues, avec des licences variées.
   - L'installation se fait via la bibliothèque Python GLiNER.
   - Des exemples d'utilisation montrent la prédiction d'entités dans des textes en anglais et en russe.

3. **Concision** : GLiNER est un modèle NER flexible, capable d'identifier divers types d'entités, offrant une alternative aux modèles NER classiques et aux LLMs coûteux.",0.2437,0.044,0.1313
8/3/2024,https://huggingface.co/spaces/allenai/WildBench,1 - High,Benchmark,Answer: Method,Processed,12/3/2024,Benchmark,,"Compilation de 1024 tÃ¢ches ""real-world"" difficiles demandÃ©es par des vrais utilisateurs Ã ChatGPT:
 - Les auteurs ont annotÃ© chaque instruction avec la rÃ©fÃ©rence de GPT-4, le type de tÃ¢che parmi 12 catÃ©gories de tÃ¢ches et une checklist de critÃ¨res Ã respecter pour passer l'Ã©valuation
 - Ils ont crÃ©Ã© un leaderboard auquel peut Ãªtre ajoutÃ© n'importe quel LLM
 - Le benchmark est censÃ© Ãªtre dynamique et sera updatÃ© rÃ©guliÃ¨rement
 - Contrairement aux Ã©valuations d'Anthropic, Claude Opus se rÃ©vÃ¨le bien moins bon que GPT-4 sur ce benchmark. NÃ©anmoins, le benchmark est biaisÃ© car GPT-4 est utilisÃ© comme juge. Un des auteurs a prÃ©sentÃ© sur X quelques amÃ©liorations qu'ils souhaitent conduire pour amÃ©liorer le benchmark.","Bien sûr, je peux vous aider avec cela. Cependant, je n'ai pas accès au texte que vous souhaitez résumer. Pourriez-vous s'il vous plaît me fournir le texte en question ?",0.2222,0,0.1111
8/3/2024,https://huggingface.co/blog/lyon-nlp-group/french-mteb-datasets,1 - High,Benchmark,Dataset,Processed,12/3/2024,Embedding Benchmark,,"Des nouvelles tÃ¢ches en franÃ§ais pour les modÃ¨les d'embeddings ont Ã©tÃ© ajoutÃ©es au Massive Text Embedding Benchmark d'Amazon:
 - Une association de NLP lyonnaise a compilÃ© 14 datasets existants et crÃ©Ã© 3 nouveaux datasets en franÃ§ais pour couvrir les tÃ¢ches du benchmark existant dans les autres langues
 - Mistral-Embed arrive premier sur ce benchmark","### Résumé

#### Introduction
Le document présente le Massive Text Embedding Benchmark (MTEB) et son extension pour la langue française, visant à évaluer des méthodes d'embeddings de texte sur diverses tâches de traitement du langage naturel (NLP). Il aborde la création de nouveaux ensembles de données pour faciliter l'évaluation des modèles en français.

#### Points clés
- **MTEB** : Évalue 33 modèles sur 8 tâches (minage de bitextes, classification, etc.) avec 58 ensembles de données, principalement en anglais.
- **Extension française** : 14 ensembles de données identifiés et 3 nouveaux créés pour les tâches de MTEB.
- **Tâches spécifiques** : Chaque tâche (classification, récupération, etc.) utilise des ensembles de données adaptés, comme DiaBLa pour le minage de bitextes et AlloProf pour la récupération d'informations.
- **Objectif** : Faciliter la recherche et l'évaluation des données en français pour le NLP.

#### Concision
Ce document vise à améliorer l'évaluation des modèles NLP en français via le MTEB, en présentant des ensembles de données adaptés aux différentes tâches de traitement du langage.",0.233,0.08,0.1136
11/3/2024,https://twitter.com/elonmusk/status/1767108624038449405,1 - High,Business,,Erreur: Erreur lors de la récupération de l'article de blog: 400,21/03/2024,,,OpenAI va opensourcer Grok,,,,
11/3/2024,https://arxiv.org/pdf/2403.04132.pdf,1 - High,Method,Method,Processed,7/5/2024,Eval,,"L'Ã©quipe LMSys de chez Berkeley publie un article sur le fonctionnment de a cÃ©lÃ¨bre arÃ¨ne Chatbot Arena.
 L'arÃ¨ne fonctionne sous la forme de ""battle"" : l'utilisateur donne un prompt puis l'arÃ¨ne lui prÃ©sente les rÃ©ponses obtenues par 2 LLMs diffÃ©rents Ã l'aveugle, l'utilisateur peut alors dire lequel des deux modÃ¨les a donnÃ© la meilleure rÃ©ponse, ou alors ""Ã©galitÃ©"" ou bien ""les 2 sont mauvais"". Les auteurs confirment que ces votes crowdsourcÃ©s sont bien alignÃ©s, Ã la fois avec le jugement de GPT-4 mais aussi avec des annotations expertes.
 La collecte d'annotations a commencÃ© en avril 2023, au moment de la publication de l'article, 240 000 votes ont Ã©tÃ© collectÃ©s. 
 En terme de couverture linguistique, 77% des battle sont anglais, 5% en chinois, les autres langues reprÃ©sentent 2% du total. En moyenne chaque modÃ¨le supportÃ© par l'arÃ¨ne reÃ§oit 8 000 votes.
 Le papier prÃ©sente beaucoup de dÃ©tails sur les mÃ©thodes utilisÃ©es pour passer des comparaisons Ã un score scalaire, pour filtrer les prompts de mauvaises qualitÃ©, ou encore sur le topic modeling effectuÃ© sur les prompts utilisateurs.
 Selon ce benchmark, gpt-4 turbo domine l'arÃ¨ne, mais notons qu'entre temps, les modÃ¨les d'Anthropic challengent le haut du classement.","**1. Introduction :**  
Le document présente Chatbot Arena, une plateforme ouverte dédiée à l'évaluation des modèles de langage (LLMs) en fonction des préférences humaines. Cette initiative vise à surmonter les défis liés à l'alignement des LLMs avec les attentes des utilisateurs.

**2. Points clés :**  
- Utilisation d'une approche de comparaison par paires pour évaluer les modèles.  
- Collecte de plus de 240 000 votes de 90 000 utilisateurs dans plus de 100 langues.  
- Méthodes statistiques robustes pour le classement et l'évaluation des modèles.  
- Les résultats montrent que les votes des utilisateurs sont cohérents avec ceux des experts.  
- Chatbot Arena est devenu une référence dans le domaine des LLMs, largement cité par les développeurs et entreprises.

**3. Concision :**  
Chatbot Arena est une plateforme d'évaluation des LLMs basée sur les préférences humaines, utilisant des comparaisons par paires et des méthodes statistiques pour établir des classements fiables. Avec 240 000 votes collectés, elle offre des résultats cohérents et est devenue une référence dans l'industrie.",0.3779,0.0694,0.1613
11/3/2024,https://twitter.com/kundan_official/status/1760336497536823420,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,14/03/2024,Hallucination,,"* Sur les tÃ¢ches oÃ¹ les LLMs doivent exploiter un document de rÃ©fÃ©rence comme le RAG ou le rÃ©sumÃ© automatique, les LLMs peuvent parfois manquer de factualitÃ© et ""inventer"" des faits non supportÃ©s par les sources. GenAudit est un modÃ¨le dâ€™assistance au fact-checking qui identifie les erreurs de facualitÃ© et propose des modifications post-hoc en citant le span de texte associÃ©.
 * GenAudit permet de dÃ©tecter et corriger des erreurs de factualitÃ© commises par de nombreux modÃ¨les, y compris de trÃ¨s bonnes baselines telles que GPT-4 ou Mistral 7B.
 * Les auteurs finetunent une sÃ©rie de plusieurs modÃ¨les spÃ©cialisÃ©s dans cette tÃ¢che dâ€™identification et de correction, et dÃ©montrent que les modÃ¨les encoder-decoder obtiennent un meilleur trade-off prÃ©cision-recall que les modÃ¨les decoder-only, Ã la fois sur la tÃ¢che dâ€™identification et de correction. Flan-T5-XXL et Flan-UL2 sont les meilleurs modÃ¨les pour ce type de tÃ¢ches.
 * Les modÃ¨les commerciaux comme GPT-3.5 Turbo ou GPT-4, promptÃ©s en few shot obtiennent de bien moins bonnes performances que les modÃ¨les finetunÃ©s.
 * Les modÃ¨les finetunÃ©s seront released publiquement.",,,,
12/3/2024,https://txt.cohere.com/command-r/,1 - High,Model,Model,Processed,12/3/2024,RAG/ToolUse,,"Cohere a release les poids de leur modÃ¨le Command-R Ã 35 milliards de paramÃ¨tres, spÃ©cialiste du RAG avec citations de sources et de l'utilisation d'outils:
 - Ils comparent les prÃ©fÃ©rences humaines du modÃ¨le versus Mixtral sur diffÃ©rents domaines mÃ©tier. Les outputs sont prÃ©fÃ©rÃ©s 70% des cas en moyenne. 
 - Sur un triple benchmark open-source mÃªlant Natural Questions (Kwiatkowski et al. 2019), TriviaQA (Joshi et al. 2017) et HotpotQA (Yang et al. 2018) en single retrieval, Command-R est plus performant que Mixtral, GPT-3.5 et Llama 70B d'au moins 5 points d'accuracy.
 - En multi-step reasoning avec des outils de recherche, il atteint de trÃ¨s bonnes performances dÃ©passant les 3 modÃ¨les au-dessus d'au moins 5 points sur HotpotQA.
 - Le modÃ¨le dÃ©passe Llama 2 70B, Mixtral et GPT-3.5 sur des benchmarks de multiple choice et traduction sur 10 langues diffÃ©rentes,
 - Les poids du modÃ¨le sont disponibles sur HuggingFace, mais sont Ã usage non commercial
 - Il est disponible sur l'API Cohere Ã $0.50/1M de tokens d'entrÃ©e et $1.50/1M de tokens de sortie","**Introduction :**  
Le document présente Command A, une plateforme d'IA offrant des modèles de langage génératifs et des outils avancés pour améliorer la recherche et l'efficacité au travail. 

**Points clés :**  
- **Command A :** Famille de modèles de langage haute performance et évolutifs.  
- **Aya Expanse :** Modèles multilingues performants dans 23 langues.  
- **Embed :** Outil de recherche et de récupération multimodal de premier plan.  
- **Rerank :** Modèle puissant améliorant la qualité de recherche par un boost sémantique.  
- **North :** Plateforme d'IA intégrée pour optimiser le travail moderne.  
- **Compass :** Système intelligent pour découvrir des insights commerciaux.  
- **Secteurs :** Solutions pour les services financiers, la santé, la fabrication, et le secteur public, avec un accent sur la sécurité des données.  

**Concision :**  
Ce résumé met en avant les capacités et applications de la plateforme Command A, soulignant son rôle dans l'amélioration de la productivité et de la recherche.",0.3873,0.0426,0.1831
12/3/2024,https://arxiv.org/pdf/2403.06634.pdf,1 - High,Method,Method,Processed,12/3/2024,Hacking,,"Les auteurs dÃ©crivent une attaque permettant de rÃ©cupÃ©rer une partie des poids d'un modÃ¨le Ã partir d'une API.
  - L'attaque s'appuie uniquement sur les rÃ©sultats renvoyÃ©s par l'API et repose sur des considÃ©rations basiques autour de la structure des rÃ©seaux de neurones.
  - L'attaque permet de dÃ©duire la dimension de l'espace latent utilisÃ© par le modÃ¨le et la matrice de projection utilisÃ©e pour obtenir les logits finaux
  - En pratique, l'attaque est rendue possible par certaines fonctionnalitÃ©s courantes comme la possibilitÃ© de biaiser les logits et de rÃ©cupÃ©rer les logits pour au moins une partie des tokens
  - En l'Ã©tat, les coÃ»ts nÃ©cessaires pour cette attaque sont d'autant plus Ã©levÃ©s que le vocabulaire du modÃ¨le est grand et que la partie observable des logits est petite. En particulier, ces coÃ»ts deviennent prohibitifs (mais pas inaccessibles) pour des modÃ¨les comme GPT 3.5/GPT 4.","1. **Introduction** : Ce document présente une attaque de vol de modèle innovante visant à extraire des informations précises des modèles de langage en boîte noire, comme ceux d'OpenAI et de Google. L'attaque se concentre sur la récupération de la couche de projection d'embedding d'un modèle transformer à partir d'accès API typique.

2. **Points clés** :
   - L'attaque permet d'extraire la matrice de projection d'embedding pour les modèles OpenAI ada et babbage pour moins de 20 USD.
   - La dimension cachée des modèles a été confirmée comme étant respectivement de 1024 et 2048.
   - L'attaque nécessite moins de 2000 requêtes pour récupérer la matrice de projection complète du modèle gpt-3.5-turbo.
   - Des défenses potentielles sont discutées, y compris des modifications architecturales et des ajouts de bruit.

3. **Concision** : Cette étude démontre la faisabilité de voler des informations de modèles de langage via des requêtes API, révélant des dimensions cachées et des matrices de projection, tout en suggérant des mesures de protection.",0.4112,0.1071,0.1878
12/3/2024,https://www.cognition-labs.com/blog,1 - High,Model,Library,Processed,14/03/2024,"Code, SWE",,"Nouveau SOTA sur SWE-Bench: 
 Devin est un nouvel agent basÃ© sur un modÃ¨le de langue capable de rÃ©soudre des taches de software engineering jamais rÃ©solues auparavant automatiquement:
 - il est dotÃ© de capacitÃ©s de planification. Pour chaque nouvelle tÃ¢che complexe, il peut formuler un plan des Ã©tapes Ã suivre,
 - le modÃ¨le a accÃ¨s Ã un interprÃ©teur de code, une CLI et un moteur de recherche,
 - il peut ainsi rÃ©agir aux erreurs rencontrÃ©es sur son code et dÃ©bugger itÃ©rativement,
 - sur SWE-Bench, un benchmark trÃ¨s difficile de code, il rÃ©ussi Ã rÃ©soudre 13.86% des tÃ¢ches tandis que le prÃ©cÃ©dent SOTA n'en rÃ©solvait que 4.8% 
 - Devin est accessible sur une waiting list","1. **Introduction** : Ce document présente des mises à jour et des initiatives autour de Devin, un agent IA spécialisé dans le développement logiciel. Il décrit les améliorations apportées à Devin et son utilisation croissante dans divers contextes.

2. **Points clés** :
   - Devin 1.2 améliore le raisonnement contextuel et introduit des messages vocaux.
   - Support pour GitLab et intégration de Sonnet 3.7.
   - Collaboration avec Linktree pour le prototypage rapide de nouvelles fonctionnalités.
   - Initiatives open source pour soutenir le développement collaboratif.
   - Améliorations de performance, de personnalisation et de gestion des comptes d'entreprise.
   - Devin aide à la récupération après des pannes, comme celle de CrowdStrike.

3. **Concision** : Devin est un agent IA innovant qui améliore le développement logiciel grâce à des mises à jour régulières, un support élargi et une collaboration accrue avec d'autres outils et plateformes.",0.2467,0.0268,0.1267
14/03/2024,https://twitter.com/MetaGPT_/status/1767965444579692832,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,14/03/2024,"Code, SWE",,"* Data Interpreter est un agent autonome capable dâ€™orchestrer des notebooks Python, un browser, un terminal et nâ€™importe quel autre outil pour rÃ©soudre des tÃ¢ches complexes de Data Science.
 * Data Interpreter utilise une capacitÃ© de planification dynamique Ã diffÃ©rentes rÃ©solutions de dÃ©cisions, ainsi que du retrieving sur des expÃ©riences passÃ©es stockÃ©es dans une pool.
 * Sur ce nouveau type de problÃ¨me, les auteurs proposent deux nouveaux benchmarks, ML-Benchmark et Open-ended task benchmark. Le premier est constituÃ© de problÃ¨mes de datascience classique de difficultÃ© variable : time series forecasting, classification, etc... tandis que le deuxiÃ¨me est plus ouvert et nÃ©cessite de manipuler des outils variÃ©s (browser, modÃ¨le dâ€™OCR, etc...).
 * Sur les deux benchmarks, Data Interpreter obtient des performances SOTA et complÃ¨te en moyenne 97% des tÃ¢ches du Open-ended task benchmark, une amÃ©lioration de 112% par rapport au projet OpenInterpreter.",,,,
14/03/2024,https://huggingface.co/spaces/opencompass/open_vlm_leaderboard,1 - High,Benchmark,Answer: Dataset,Processed,21/03/2024,#multimodal #leaderboard,,"* 41 LLM multimodaux y sont comparÃ©s sur datasets. Les taches couvertes sont variÃ©es : captioning, OCR, lecture de diagramme, comprÃ©hension de tableauâ€¦
 * Le leaderboard est disponible sur HuggingFace.
 * Un repository VLMEvalKit est aussi disponible pour Ã©valuer ses propres LLM multimodaux.","Bien sûr, je peux vous aider avec ça. Toutefois, j'aurais besoin du texte que vous souhaitez que je résume. Merci de le fournir afin que je puisse créer le résumé selon la structure demandée.",0.075,0,0.075
15/03/2024,https://www.anthropic.com/news/claude-3-haiku,1 - High,Model,Model,Processed,21/03/2024,#LLM #multimodal #anthropic #API,,"* Plus de 25 fois moins cher que GPT4-turbo en prenant le coÃ»t moyen par token.
 * OptimisÃ© pour avoir une latence extrÃªmement faible, au prix de performances moins Ã©levÃ©es que pour les autres LLM dâ€™Anthropic (Sonnet et Opus).
 * Dâ€™aprÃ¨s le benchmark multimodal vision dâ€™Anthropic, Haiku est aussi performant que GPT4-Vision.","1. **Introduction** : Le document présente le lancement de Claude 3 Haiku, le modèle d'intelligence artificielle le plus rapide et le plus abordable de sa catégorie, destiné aux applications d'entreprise.

2. **Points clés** :
   - Claude 3 Haiku est trois fois plus rapide que ses concurrents, traitant 21K tokens par seconde pour des requêtes inférieures à 32K tokens.
   - Il permet une analyse rapide de grandes quantités de documents à moitié prix par rapport aux autres modèles similaires.
   - Le modèle est conçu avec une sécurité renforcée, incluant des tests rigoureux et des audits de sécurité.
   - Disponible via l'API et pour les abonnés Claude Pro, il sera également accessible sur Amazon Bedrock et Google Cloud Vertex AI.

3. **Concision** : Claude 3 Haiku, le modèle IA le plus rapide et abordable, optimise l'analyse des données pour les entreprises tout en garantissant sécurité et performance.",0.133,0.0107,0.0904
15/03/2024,https://arxiv.org/pdf/2401.15391.pdf,1 - High,Dataset,Answer: Dataset,Processed,20/03/2024,#RAG #Multihop,,"Dataset constituÃ© de questions multihop (oÃ¹ l'information nÃ©cessaire est contenue dans plusieurs segments) posÃ©es sur des articles de news en anglais et de ground truths sourcÃ©es. Les questions sont de 4 types : dâ€™infÃ©rence (identifier un nom ou une entitÃ© Ã partir de ses attributs ou dâ€™une description), comparaisons temporelles (comparaison entre deux Ã©vÃ¨nements : rÃ©ponses en avant/aprÃ¨s ou Ã©ventuellement boolÃ©ennes), comparaisons boolÃ©ennes et questions adversariales. 
 Ils font deux expÃ©riences sur une pipeline LlamaIndex :
  - Evaluation du retrieving : comparaison de modÃ¨les dâ€™embedding en top-K retrieving avec ou sans reranker, sur diffÃ©rentes mÃ©triques (MMR, MAP, HIT). Tous les scores sont assez bas : le meilleur embedding obtient 0.66 en Hits@4 avec reranking.
  - Evaluation de la partie gÃ©nÃ©ration de rÃ©ponse en supposant que lâ€™Ã©tape de retrieving a remontÃ© toute lâ€™information nÃ©cessaire : ils prÃ©sentent au LLM les sources utilisÃ©es pour Ã©crire la ground truth. GPT-4 est performant mais les modÃ¨les open-source Llama-70B, Mixtral-8x7B, ont respectivement 0.32 et 0.36 dâ€™accuracy.
 MÃªme sur des requÃªtes multihop relativement simples, faire du top-K retrieving mÃªme avec un reranker donne donc de mauvais rÃ©sultats et mÃªme en prÃ©sentant au modÃ¨le des segments avec toute lâ€™information nÃ©cessaire, les gros modÃ¨les open-sources ont de (trÃ¨s) mauvaises performances en gÃ©nÃ©ration.
 En ce qui concerne le dataset, les types de multihop considÃ©rÃ©s sont encore trop simples par rapport aux cas rÃ©els. Toutes les questions sont boolÃ©ennes ou ont pour rÃ©ponse, non pas une phrase, mais une entitÃ© bien dÃ©finie. Ils excluent aussi les questions adversariales pour l'Ã©valuation du retrieving, donc n'Ã©valuent pas la capacitÃ© Ã retriever des informations utiles et connexes.
 Licence : odc-by","### 1. Introduction
Le document présente MultiHop-RAG, un ensemble de données novateur conçu pour évaluer les systèmes de génération augmentée par récupération (RAG) face à des requêtes multi-hop, qui nécessitent la récupération et le raisonnement à partir de plusieurs preuves. L'étude met en évidence les lacunes des systèmes RAG existants dans ce domaine.

### 2. Points clés
- **Création de MultiHop-RAG** : Comprend une base de connaissances, une collection de requêtes multi-hop, des réponses vérifiées et des preuves associées.
- **Méthodologie** : Utilisation de modèles de langage tels que GPT-4 pour générer des requêtes et évaluer les capacités de raisonnement des LLMs.
- **Résultats des expériences** : Les systèmes RAG actuels montrent des performances insatisfaisantes dans la récupération et la réponse à des requêtes multi-hop.
- **Utilité** : MultiHop-RAG vise à servir de ressource précieuse pour améliorer le développement et l'évaluation des systèmes RAG.

### 3. Concision
MultiHop-RAG est un ensemble de données innovant pour évaluer les systèmes RAG sur des requêtes complexes nécessitant plusieurs preuves. L'étude révèle que les méthodes RAG actuelles sont insuffisantes pour traiter ces requêtes, soulignant ainsi l'importance de cette ressource pour le développement futur des systèmes de génération augmentée par récupération.",0.4549,0.0823,0.1721
17/03/2024,https://arxiv.org/pdf/2403.09611.pdf,1 - High,Model,Model,Processed,18/03/2024,#mutltimodal,,"- Un des premiers papiers, publiÃ© par des chercheurs de chez Apple, qui offre un comparatif sÃ©rieux sur les choix d'architecture de modÃ¨le, d'entraÃ®nement et de data mixture pour les LLM multimodaux.
 - Un plus long pre-training et un support pour de grandes images au pre-training et au SFT participent a augmenter les performances du modÃ¨le. A contrario, le choix du connecteur vision-langage semble Ãªtre nÃ©gligeable (Ã prendre avec des pincettes car seuls 3 connecteurs ont Ã©tÃ© testÃ©s).
 - La data mixture a un impact significatif sur les capacitÃ©s du modÃ¨le. Les auteurs recommendent un ratio caption/interleaved/text only ratio de 5:5:1. Avoir de l'interleaved amÃ©liore drastiquement le few-shot. Exclure les exemples â€œtext onlyâ€ fait que le modÃ¨le oublie ses capacitÃ©s de raisonnement et rÃ©duit les performances en few-shot.
 - Leur modÃ¨le MM1 n'est pas open-sourcÃ©, mais tous les datasets d'images utilisÃ©s pour le pre-training et le SFT sont dÃ©taillÃ©s et issus de la communautÃ© open-source.","1. **Introduction** : Ce document traite de la conception et de l'évaluation des Modèles de Langage Multimodaux (MLLM) performants, en se concentrant sur le modèle MM1 développé par Apple. L'étude explore l'importance des composants architecturaux et des choix de données pour l'entraînement multimodal.

2. **Points clés** :
   - Utilisation d'un modèle de langage de 30 milliards de paramètres.
   - Importance de la résolution d'image et de la composition des données d'entraînement pour les performances.
   - Les données intercalées et les légendes d'images sont essentielles pour les performances en few-shot.
   - MM1 atteint des résultats d'état de l'art (SOTA) sur plusieurs benchmarks de captioning et de question-réponse visuelle.
   - Capacité de raisonnement multi-image et d'apprentissage contextuel amélioré grâce à un pré-entraînement à grande échelle.

3. **Concision** : Ce document présente le modèle MM1 d'Apple, un MLLM performant, mettant en avant l'importance des choix architecturaux et des données pour optimiser les résultats. MM1 se distingue par ses performances SOTA sur divers benchmarks, grâce à une approche d'entraînement multimodal soigneusement conçue.",0.3495,0.0829,0.1553
17/03/2024,https://x.ai/blog/grok-os,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 403,21/03/2024,#OpenLLM #xAI,,"xAI a open-sourcÃ© les poids et architecture de Grok-1.
 314B de paramÃ¨tres, Mixture of 8 Experts, 2 experts par token. Utilise rotary position embeddings (RoPE). Taille de contexte 8192 tokens.
 Licence Apache-2.0",,,,
19/03/2024,https://contextual.ai/introducing-rag2/,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 403,27/03/2024,"RAG, E2E",,"Contextual AI introduisent des nouveaux Contextual Language Models qui unifie la pipeline classique de RAG (retriever+gÃ©nÃ©ration de synthÃ¨se) en un seul modÃ¨le:
 - Sur plusieurs benchmarks acadÃ©miques (Natural Questions, TriviaQA, HotPotQA, TruthfulQA, HaluEvalQA et FreshQA), RAG 2.0 est bien meilleur que GPT-4 en few-shot (entre 3 et 9 points de plus). Le modÃ¨le est particuliÃ¨rement performant sur sur des donnÃ©es rÃ©elles, comme ils le montrent sur des donne2es financiÃ¨res.
 - RAG 2.0 fonctionne bien sur des longs contextes battant GPT-4 jusqu'a 32k de contexte et Mixtral sur jusqu'Ã de 2M de tokens de contexte
 - Il faut rejoindre une waitlist pour pouvoir tester le modÃ¨le",,,,
19/03/2024,https://twitter.com/tianjun_zhang/status/1768706092211826966,1 - High,Method,,Erreur: Erreur lors de la récupération de l'article de blog: 400,27/03/2024,RAFT,,"Papier qui prÃ©sente la mÃ©thode RAFT:
 - la mÃ©thode consiste Ã finetuner un modÃ¨le pour rÃ©pondre Ã une question en mode chain-of-thought Ã partir de k documents donnÃ©es en entrÃ©e dont certains sont des ""distractors"" qui ne rÃ©pondent pas Ã la requÃªte
 - Les auteurs comparent RAFT Ã diffÃ©rentes baselines qui montrent que le finetuning sur des rÃ©ponses Chain-of-thought augmente bien les performances
 Ce papier nous rassure sur la mÃ©thode qu'on utilise pour les modÃ¨les intÃ©grÃ©s dans Search car elle est trÃ¨s proche de ce que nous utilisons.",,,,
20/03/2024,https://twitter.com/yanndubs/status/1770284212937207840,1 - High,Benchmark,,Erreur: Erreur lors de la récupération de l'article de blog: 400,21/03/2024,#Eval #OutputLength,,"Le leaderboard de chatLLMs AlpacaEval est dÃ©sormais normalisÃ© par dÃ©faut sur la taille des outputs.
 
 Meilleure corrÃ©lation avec le LMSys Chat Arena que MT-Bench (0.98 contre 0.94 ; 0.93 sans contrÃ´le de longueur).
 
 Le contrÃ´le sur la longueur corrige le biais de longueur de GPT-4 mais les modÃ¨les finetunÃ©s sur de outputs de GPT-4 sont probablement encore avantagÃ©s.
 
 LCAE est plus robuste Ã la verbositÃ© lorsque lâ€™on fait varier le prompt (le ranking reste le mÃªme que lâ€™on demande on rÃ©ponse concise/standard/verbeuse).
 
 [Leaderboard](https://tatsu-lab.github.io/alpaca_eval/), [Github](https://github.com/tatsu-lab/alpaca_eval).",,,,
21/03/2024,https://huggingface.co/blog/cosmopedia,1 - High,Dataset,Dataset,Processed,21/03/2024,#SyntheticData,,"Release de Cosmopedia, dataset open-source dâ€™une taille de 25 milliards de tokens de textbooks, blog posts et articles WikiHow gÃ©nÃ©rÃ©s synthÃ©tiquement par Mixtral-8x7B, dans le but notamment de pretrain des LLMs sur donnÃ©es synthÃ©tiques.
 Toute la stack, les prompts et le code pour gÃ©nÃ©rer le dataset est open-sourcÃ© :
  - GÃ©nÃ©ration avec Mixtral de 30 millions de prompts dâ€™instructions gÃ©nÃ©rÃ©s majoritairement en demandant dâ€™Ã©crire un blog post/textbook/leÃ§on/etc. Ã partir de datasets existants scrapÃ©s du web (RefinedWeb, OpenHermes) et en promptant Mixtral de faÃ§on un peu fine pour faire varier lâ€™auditoire/le style.
  - Ils gÃ©nÃ¨rent 25 milliards de tokens de textbooks synthÃ©tiques par Mixtral avec llm-swarm. Le dataset obtenu couvre une grande variÃ©tÃ© de topics avec moins de 1% de doublons.
 Ils entraÃ®nent un modÃ¨le 1B sur Cosmopedia et obtiennent de meilleurs performances que TinyLlama 1.1B sur ARC, OpenBookQA et MMLU et comparables Ã Qwen-1.5-1B sur Arc-challenge et OpenBookQA.","### 1. Introduction
Ce document traite de la création de Cosmopedia, un ensemble de données synthétiques destiné à reproduire les performances du modèle Phi-1.5. L'accent est mis sur les défis et les solutions pour générer un vaste dataset de plusieurs milliards de tokens, en utilisant des modèles de langage.

### 2. Points clés
- **Modèle utilisé** : Mixtral-8x7B-Instruct-v0.1 pour générer des données.
- **Méthode** : Curation minutieuse de plus de 30 millions de prompts pour assurer la diversité et minimiser les duplications.
- **Résultats** : Cosmopedia contient 25 milliards de tokens, le plus grand ensemble de données synthétiques ouvert à ce jour.
- **Évaluation** : Un modèle de 1 milliard de paramètres (cosmo-1b) a été formé, surpassant certains modèles existants mais affichant des lacunes par rapport à Phi-1.5.
- **Améliorations futures** : Intégration de la génération augmentée par récupération pour réduire les hallucinations.

### 3. Concision
Cosmopedia est un ensemble de données synthétiques de 25 milliards de tokens, créé pour reproduire le modèle Phi-1.5. En utilisant Mixtral-8x7B-Instruct-v0.1, l'équipe a développé 30 millions de prompts diversifiés. Le modèle cosmo-1b a été formé, montrant des performances prometteuses, mais nécessitant des améliorations pour réduire les hallucinations.",0.4058,0.1311,0.1836
21/03/2024,https://twitter.com/natolambert/status/1770488846360428782,1 - High,Benchmark,,Erreur: Erreur lors de la récupération de l'article de blog: 400,21/03/2024,#RewardModels,,"- Benchmark de reward models de AllenAI.
 - Collection de triplets (prompt-bonne rÃ©ponse-mauvaise rÃ©ponse) sur plusieurs domaines (chat, chat oÃ¹ il est difficile de trancher entre les deux rÃ©ponses, raisonnement et code, safety). Les paires prompts-rÃ©ponses sont essentiellement collectÃ©es Ã partir dâ€™AlpacaEval et MT Bench pour le chat, et XSTest et Do-Not-Answer pour la safety et HumanEvalPack pour le raisonnement et le code.
 - Lâ€™Ã©valuation se fait simplement en vÃ©rifiant que le reward model assigne un score plus Ã©levÃ© Ã la bonne rÃ©ponse quâ€™Ã la mauvaise rÃ©ponse.
 - Les modÃ¨les qui obtiennent les meilleurs scores en moyenne sont Starling-RM-7B, zephyr-7b, et Nous-Hermes-2-Mistral-7B-DPO.
 Licence : ODC-By",,,,
25/01/2024,https://x.com/aparnadhinak/status/1748368364395721128?t=iYDFnzb_VJZGPmJFjbDGRg&amp;s=31https://x.com/aparnadhinak/status/1748368364395721128?t=iYDFnzb_VJZGPmJFjbDGRg&amp;s=31,1 - High,Method,,Erreur: Erreur lors de la récupération de l'article de blog: 400,30/01/2024,Eval,,Utiliser des classes plutÃ´t qu'un score numÃ©rique permet de rÃ©duire les biais de modÃ¨les Ã©valuateurs (e.g. GPT 4),,,,
21/03/2024,https://github.com/pytorch/torchtune,1 - High,Library,Library,Processed,21/03/2024,#Finetuning #Pytorch,,"Librairie native Pytorch trÃ¨s complÃ¨te pour tester et finetuner des LLM (full finetune, LoRA), avec FSDP.
 IntÃ©gration Ã Hugging Face datasets pour l'entraÃ®nement et Ã EvalHarness pour l'Ã©valuation.
 Support des checkpoints Huggingface.","### Introduction
Le document présente torchtune, une bibliothèque PyTorch dédiée à l'entraînement et à l'expérimentation avec des modèles de langage (LLM). Il met en avant les fonctionnalités récentes, les modèles supportés, ainsi que les méthodes de post-formation disponibles.

### Points clés
- **Mises à jour récentes** : Introduction de l'entraînement multi-nœuds, support pour Llama 3.3 70B, et nouvelles versions incluant des fonctionnalités avancées.
- **Méthodes de post-formation** : Comprend le finetuning supervisé (SFT), la distillation de connaissances (KD), et l'apprentissage par renforcement (RLHF) avec diverses options de mise à jour des poids.
- **Modèles supportés** : Inclut Llama, Gemma, Mistral, et Qwen, avec des configurations spécifiques.
- **Optimisation de la mémoire** : Techniques variées pour améliorer l'efficacité mémoire et la vitesse d'entraînement.

### Concision
Torchtune est une bibliothèque PyTorch pour l'entraînement et l'expérimentation de LLM, offrant des mises à jour récentes, diverses méthodes de post-formation, et support pour plusieurs modèles. Des techniques d'optimisation de la mémoire et de performance sont également intégrées.",0.0863,0.0255,0.0711
22/03/2024,https://github.com/mshumer/gpt-prompt-engineer,1 - High,Library,Library,Processed,27/03/2024,#PromptEngineering,,"- Automatise le prompt engineering. Les prompts sont testÃ©s et classÃ©s avec un ELO score. Il y a un preset pour supporter des problÃ¨mes de classification.
 - Permet une distillation des capacitÃ©s de rÃ©flexion de Claude Opus (cher et lent) vers Claude Haiku (Ã©conomique et rapide) via du few-shot learning.","1. **Introduction** : Le document présente `gpt-prompt-engineer`, un outil de génération et de test de prompts pour l'intelligence artificielle, qui permet d'optimiser les requêtes en fonction de divers cas de test. 

2. **Points clés** :
   - Utilise des modèles comme GPT-4, GPT-3.5-Turbo et Claude 3 Opus.
   - Génération de prompts basée sur une description de tâche et des cas de test.
   - Évaluation des prompts via un système de notation ELO, permettant de classer leur efficacité.
   - Version pour la classification des tâches, avec vérification des réponses attendues.
   - Nouvelles fonctionnalités pour Claude 3 Opus, incluant la génération automatique de cas de test et la conversion en Haiku pour des résultats rapides et économiques.

3. **Concision** : `gpt-prompt-engineer` est un outil innovant pour optimiser les prompts d'IA, en générant et testant des requêtes pour améliorer leur performance, tout en intégrant des fonctionnalités avancées pour divers modèles d'IA.",0.179,0.0124,0.0988
24/03/2024,https://github.com/mistralai-sf24/hackathon,1 - High,Model,Model,Processed,26/03/2024,#Mistralv0.2,,"Lors d'un hackathon Ã San Francisco, Mistral a release le base modÃ¨le Ã l'origine du modÃ¨le Instruct v0.2.
 - Ils ont apportÃ© plusieurs updates au modÃ¨le: une fenÃªtre de contexte de 32k tokens, un rope theta de 1e6 et pas de sliding window
 - Ils auraiaient oubliÃ© de le livrer au mÃªme temps que le Instruct.","1. **Introduction** : Ce document présente le dépôt du modèle Mistral 7B, qui permet de l'exécuter et de le peaufiner. Il fournit des instructions pour télécharger le modèle, l'installer et l'utiliser.

2. **Points clés** :
   - Téléchargement du modèle via `wget` et extraction des fichiers.
   - Installation des dépendances nécessaires avec un conteneur Docker.
   - Déploiement du modèle avec vLLM et utilisation de la bibliothèque transformers.
   - Options pour exécuter le modèle et ajuster les paramètres tels que `temperature` et `max_tokens`.
   - Préparation des données pour le fine-tuning au format jsonl, avec des types de données pour le pré-entraînement et les interactions.
   - Utilisation de LoRA pour un fine-tuning efficace et mémoire, avec des instructions pour l'exécution sur un GPU.

3. **Concision** : Ce document fournit des instructions complètes pour télécharger, installer et utiliser le modèle Mistral 7B, ainsi que des détails sur le fine-tuning avec LoRA.",0.1369,0.0299,0.1071
25/01/2024,https://huggingface.co/papers/2312.15685,1 - High,Method,,Erreur: Erreur lors de la récupération des données HuggingFace: 401,30/01/2024,SÃ©lection de donnÃ©es pour instruction finetuning,,"Les auteurs proposent des mÃ©thodes pour sÃ©lectionner des exemples de SFT parmi un dataset d'exemples de qualitÃ© diverse selon leur qualitÃ©, la diversitÃ© et la complexitÃ©. Ils finetunent des Llama pour scorer ces samples afin de mesurer la qualitÃ© et la complexitÃ©. La diversitÃ© est mesurÃ©e en regardant la distance entre les embeddings. Leur mÃ©thode globale de sÃ©lection finale est basÃ©e sur un score s = complexitÃ© * qualitÃ© pour filtrer les samples en contraignant une certaine diversitÃ©. La mÃ©thode est un peu complexe mais les rÃ©sultats sont plutÃ´t bons.",,,,
25/01/2024,https://twitter.com/maximelabonne/status/1746317053143773628,1 - High,Method,,Erreur: Erreur lors de la récupération de l'article de blog: 400,30/01/2024,Eval,,"Matrice de corrÃ©lation entre diffÃ©rents benchmark, y compris le Elo score de l'arÃ¨ne LMSys (annotation humaines crowdsourcÃ©es). MT-Bench, AGI Eval, ARC-C, et MMLU sont de bons prÃ©dicteurs du jugement humain
 TruthfulQA et HellaSwag sont mal corrÃ©lÃ©s au jugement humain. AGI Eval est un trÃ¨s bon compromis entre coÃ»t d'infÃ©rence et corrÃ©lation au jugement humain.",,,,
27/03/2024,https://x.com/lmsysorg/status/1772759835714728217?t=GKXMABIffPxpBgX7UzEuBQ&amp;s=31,1 - High,Benchmark,,Erreur: Erreur lors de la récupération de l'article de blog: 400,27/03/2024,#Claude #LMSysArena,,"- Claude 3 Opus est premier du leaderboard devant GPT-4 preview
 - Claude 3 Haiku (modele ""low-cost"") est devant GPT-4-0613 et Mistral Large
 - Ce dernier point est trÃ¨s intÃ©ressant notamment par rapport au coÃ»t pour nos usages internes.",,,,
27/03/2024,https://huggingface.co/blog/embedding-quantization,1 - High,Method,Method,Processed,27/03/2024,#RAG #embeddings #quantization,,"- Les auteurs proposent un retrieval en 2 Ã©tapes suivant la logique de reranking. Lâ€™apport se fait dans la 1Ã¨re Ã©tape oÃ¹ les embeddings ont Ã©tÃ© quantifiÃ©s. Ils ont observÃ© une accÃ©lÃ©ration de x32 et un gain de mÃ©moire de x32 Ã©galement, tout en prÃ©servant 96% des performances originales.
 - Les auteurs ont aussi expÃ©rimentÃ© avec une quantification en int8.
 - La fonctionnalitÃ© est disponible dans la [v2.6.0 de sentence-transformers](https://github.com/UKPLab/sentence-transformers/releases#:~:text=Compare-,v2.6.0%20%2D%20Embedding%20Quantization%2C%20GISTEmbedLoss,-This%20release%20brings).","1. **Introduction** : Ce document traite de la quantification des embeddings, une méthode visant à améliorer la vitesse de récupération, l'utilisation de la mémoire, l'espace disque et les coûts associés aux embeddings dans le traitement du langage naturel. Il présente des techniques de quantification, notamment la quantification binaire et scalaire, et leurs effets sur la performance.

2. **Points clés** :
   - Les embeddings sont des représentations numériques d'objets complexes, souvent utilisés pour la recherche et les systèmes de recommandation.
   - Les modèles d'embeddings à 1024 dimensions nécessitent environ 1 To de mémoire pour traiter 250 millions de vecteurs.
   - La quantification binaire réduit la taille des embeddings de 32 fois, tandis que la quantification scalaire réduit la taille de 4 fois.
   - L'application d'une étape de rescoring permet de maintenir jusqu'à 96% de la performance tout en réduisant l'utilisation des ressources.
   - Une combinaison de quantification binaire et scalaire est proposée pour optimiser la performance et la mémoire.

3. **Concision** : La quantification des embeddings améliore la récupération et réduit les coûts en mémoire et en espace disque. Les méthodes binaire et scalaire permettent d'atteindre des performances élevées tout en diminuant considérablement les besoins en ressources.",0.2098,0.0245,0.1073
25/01/2024,https://unbabel.com/announcing-tower-an-open-multilingual-llm-for-translation-related-tasks/?utm_campaign=Tower%20Announcement&utm_content=278163605&utm_2 - Medium=social&utm_source=twitter&hss_channel=tw-2181959904,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 403,30/01/2024,ModÃ¨les de traduction multilangues,,Unbabel a release un modÃ¨le Llama 7B finetunÃ© pour de la traduction qui permet d'ajouter des instructions pour orienter la traduction,,,,
27/03/2024,https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm,1 - High,Model,Model,Processed,27/03/2024,#Databricks #LLM #MoE,,"ModÃ¨le de Databricks meilleur que Mixtral mais bien plus gros (132B total parameters of which 36B parameters are active) et entraine sur 12T tokens: 
 - Le modÃ¨le a 132B de paramÃ¨tres sous la forme d'une mixture d'experts avec 16 experts dont 4 sont activÃ©s Ã l'infÃ©rence soit 36B de paramÃ¨tres actifs.
 - Le modÃ¨le a Ã©tÃ© entrainÃ© sur 12T de tokens de texte, en grande partie e anglais, et code. Le modÃ¨le n'a pas Ã©tÃ© testÃ© sur d'autres langues.
 - Plusieurs benchmarks ont Ã©tÃ© fait en comparaison avec des modÃ¨les open-source et en comparaison avec des API fermÃ©es. DBRX Instruct est meilleur que Mixtral Instruct et GPT-3.5 sur la plupart des benchmarks, de quelques points sur l'Open LLM Benchamrk, de 15 points sur HumanEval et 6 points sur le benchmark Gauntlet (compilation e taches de NLU faite par Databricks). Il est tout de mÃªme moins bon que Claude 3 Haiku.
 - License permettant l'usage commerciale non contraint en-dessous de 700 million d'utilisateurs actifs. Ils ne permettent pas d'utiliser DBRX pour amÃ©liorer d'autres LLMs.","### Introduction
Ce document présente DBRX, un modèle de langage de pointe développé par Databricks, qui surpasse les performances des modèles précédents et établit de nouvelles références dans le domaine des modèles de langage ouverts.

### Points clés
- **Modèle** : DBRX est un modèle de langage basé sur une architecture de mélange d'experts (MoE) avec 132 milliards de paramètres, dont 36 milliards actifs.
- **Méthode** : Il a été pré-entraîné sur 12 trillions de tokens de données textuelles et de code, utilisant des techniques avancées comme l'apprentissage par curriculum.
- **Résultats** : DBRX dépasse GPT-3.5 et rivalise avec Gemini 1.0 Pro, affichant des performances supérieures dans des benchmarks composites, de programmation et de raisonnement mathématique.
- **Efficacité** : DBRX est deux fois plus rapide que LLaMA2-70B pour l'inférence, tout en étant plus efficace en termes de calcul.

### Concision
DBRX de Databricks est un modèle de langage innovant qui établit de nouvelles normes en matière de performance et d'efficacité, surpassant des modèles concurrents comme GPT-3.5 et offrant des capacités avancées pour les entreprises et la communauté.",0.4734,0.119,0.2189
28/03/2024,https://blog.research.google/2024/03/chain-of-table-evolving-tables-in.html,1 - High,Method,Method,Processed,4/4/2024,"CoT, TableQA",,"MÃ©thode par Google qui gÃ©nÃ©ralise le Chain-of-Thought sur des tableaux. 
 - Les auteurs proposent une chaÃ®ne de raisonnement sur un tableau, Ã partir dâ€™une requÃªte utilisateur qui peut Ãªtre complexe.
 - A chaque itÃ©ration, un LLM va choisir dans un pool dâ€™opÃ©ration, la prochaine opÃ©ration a effectuer en fonction de la question, du tableau et de lâ€™historique du raisonnement. Cette opÃ©ration peut mener Ã une modification du tableau, jusquâ€™Ã ce que celui ci puisse rÃ©pondre directement Ã la question de l'utilisateur.
 - Avec cette mÃ©thode, les auteurs montrent une amÃ©lioration des capacitÃ©s de raisonnement sur diffÃ©rents dataset de TableQA.","1. **Introduction** : Ce document présente la méthode ""Chain-of-Table"", développée pour améliorer la compréhension des tableaux par les modèles de langage, en les guidant à travers un raisonnement étape par étape pour résoudre des questions basées sur des données tabulaires.

2. **Points clés** :
   - Utilisation de modèles de langage tels que PaLM 2 et GPT 3.5.
   - La méthode consiste à générer dynamiquement une chaîne d'opérations pour transformer les tableaux en segments plus simples.
   - Amélioration significative des performances sur les benchmarks WikiTQ, TabFact et FeTaQA.
   - Chain-of-Table surpasse les méthodes de raisonnement génériques et assistées par programme, avec jusqu'à 11,6 % d'amélioration.
   - La méthode démontre une robustesse accrue face à des questions plus complexes et des tableaux de grande taille.

3. **Concision** : La méthode ""Chain-of-Table"" optimise la compréhension des tableaux par les modèles de langage en structurant le raisonnement. Elle montre des performances supérieures sur des benchmarks, améliorant la précision des réponses tout en restant robuste face à des tableaux complexes.",0.2994,0.0723,0.1377
28/03/2024,https://www.ai21.com/blog/announcing-jamba,1 - High,Model,Model,Processed,4/4/2024,"LLM, Open Source",,"- Jamba est le premier modÃ¨le basÃ© sur Mamba Ã atteindre des performances proches de Mixtral 8x7B. La principale nouveautÃ© de ce modÃ¨le est l'utilisation dâ€™une architecture hybride mixant blocs transformers et Mamba.
 - Avec une fenÃªtre de contexte de 256k, Jamba offre un dÃ©bit 3x plus important que Mistral 8x7B sur les longs contextes
 - Le modÃ¨le est disponible sur HuggingFace sous licence Apache 2.0","1. **Introduction** : Le document présente Jamba, un modèle innovant basé sur une architecture hybride SSM-Transformer, développé par AI21. Ce modèle vise à résoudre les limitations des architectures traditionnelles en matière de performance et d'efficacité pour les tâches complexes.

2. **Points clés** :
   - Jamba est le premier modèle de production basé sur Mamba, avec une architecture SSM-Transformer.
   - Offre un contexte de 256K et un rendement trois fois supérieur aux modèles comparables.
   - Utilise des couches de mélange d'experts (MoE) pour optimiser la mémoire et la capacité.
   - Disponible avec des poids ouverts sous licence Apache 2.0.
   - Jamba est accessible via le catalogue API NVIDIA, facilitant son déploiement.

3. **Concision** : Jamba révolutionne le domaine des modèles de langage en combinant les avantages des architectures Mamba et Transformer, offrant ainsi des performances inégalées et une accessibilité accrue pour les développeurs.",0.2883,0.1173,0.1534
