Reception date,Link,Review priority,Category (Illuin),Category AI,Status,Reviewed,Topic / Keywords (Illuin),Topic / Keywords AI,Take-away (Illuin),Take-away AI,rouge1 precision,rouge2 precision,rougeL precision
25/01/2024,https://arxiv.org/pdf/2401.05856.pdf,1 - High,Method,Method,Processed,26/01/2024,RAG / Eval,,"Seven Failure Points When Engineering a Retrieval Augmented Generation System: these are: 
 - the answer is not in the documents and the LLM hallucinates,
 - the answer is in the documents but not presented to the LLM,
 - the answer is in the documents but not in the contexts presented to the LLM,
 - the LLM did not extract the answer from the documents
 - the LLM did not present the answer in the right format
 - the answer of the LLM does not have the right specificity (too specific or not enough)
 - the LLM did not extract all the answers in the contexts that ere presented
 They summarise several lessons learned from their experience: long contexts are better, add metadata to improve retrieval, use continuous monitoring, end-to-end training is better than assembly of several bricks","1. **Introduction** : Ce document traite des systèmes de génération augmentée par récupération (RAG), une approche permettant aux ingénieurs logiciels d'intégrer des capacités de recherche sémantique dans les applications. Les systèmes RAG cherchent à améliorer la précision des réponses générées par des modèles de langage, comme ChatGPT, en utilisant des documents pertinents pour répondre aux requêtes.

2. **Points clés** :
   - Identification de 7 points de défaillance lors de la conception de systèmes RAG, issus de trois études de cas (recherche, éducation, biomédical).
   - Les principaux enseignements incluent la nécessité de valider les systèmes RAG en fonctionnement et que leur robustesse évolue avec le temps.
   - Importance du prétraitement des connaissances et des stratégies de correspondance des requêtes pour améliorer la performance.

3. **Concision** : Ce document présente des défis et des leçons tirées de l'implémentation de systèmes RAG, avec un accent sur les points de défaillance courants et les directions de recherche futures pour améliorer leur efficacité et leur robustesse.",0.0258,0.0,0.0155
1/2/2024,https://arxiv.org/abs/2401.16658,1 - High,Model,Model,Processed,7/2/2024,Speech Model,,"Introduction de OWSM v3.1 base et 2 - Medium (100M et 1B respectivement), nouvelle version de modÃ¨les de type Open Whisper-style Speech Model mais qui utilise un E-Branchformer (Enhanced Branchformer, qui utilise des branches parallÃ¨les pour capturer les informations locales et globales du contexte et les combiner par convolution) Ã la place dâ€™un Transformer pour lâ€™encoding.
 Le modÃ¨le est entraÃ®nÃ© sur les mÃªmes donnÃ©es multilingues que la version prÃ©cÃ©dente OWSM v3. Lâ€™insight principal est quâ€™utiliser un E-Branchformer amÃ©liore les performances par rapport Ã la version prÃ©cÃ©dente sur la plupart des tÃ¢ches : speech recognition (anglais et multilingue), speech translation, long-form English speech recognition, et amÃ©liore la vitesse dâ€™infÃ©rence de lâ€™ordre de 25%. Moins performant que la v3 en identification de langues cependant.
 OWSM v3.1 reste globalement derriÃ¨re Whisper qui est de toute faÃ§on entraÃ®nÃ© sur beaucoup plus de donnÃ©es/des donnÃ©es de meilleure qualitÃ©.","1. **Introduction** : Le document présente OWSM v3.1, un modèle de reconnaissance vocale inspiré de Whisper, utilisant l'architecture E-Branchformer. L'objectif est d'améliorer les performances et l'efficacité des modèles de reconnaissance vocale tout en restant entièrement open source.

2. **Points clés** :
   - OWSM v3.1 propose des modèles de 100M à 1B de paramètres.
   - Amélioration des performances par rapport à OWSM v3, avec des gains de vitesse d'inférence allant jusqu'à 25%.
   - OWSM v3.1 surpasse OWSM v3 dans la majorité des benchmarks d'évaluation.
   - Introduction d'une capacité émergente pour la reconnaissance vocale avec biais contextuel en zero-shot.
   - Publication des codes, modèles pré-entraînés et journaux de formation pour favoriser la transparence.

3. **Concision** : OWSM v3.1 améliore significativement les performances et l'efficacité des modèles de reconnaissance vocale, tout en étant entièrement open source, et permet un accès à des modèles à faible restriction de licence.",0.4458,0.103,0.2229
30/01/2024,https://github.com/promptfoo/promptfoo,1 - High,Library,Library,Processed,30/01/2024,Eval,,promptfoo est une librarie open source avec une interface en ligne de commande permettant de tester et d'Ã©valuer la qualitÃ© des outputs de plusieurs LLMs sur plusieurs prompts en parallÃ¨le et restitue les rÃ©sultats sous forme de tableau comparatif. Des assertions et mÃ©triques peuvent Ãªtre configurÃ©es pour automatiquement scorer les outputs si applicable Ã la tÃ¢che Ã l'Ã©tude.,"1. **Introduction** : Le document présente `promptfoo`, un outil local destiné aux développeurs pour tester des applications d'IA basées sur des modèles de langage (LLM). Il vise à améliorer la sécurité et la fiabilité des applications d'IA en remplaçant les méthodes d'essai-erreur par des évaluations automatisées.

2. **Points clés** :
   - Permet de tester des prompts et modèles via des évaluations automatisées.
   - Offre des fonctionnalités de red teaming pour détecter des vulnérabilités.
   - Compare différents modèles (OpenAI, Anthropic, etc.) côte à côte.
   - S'intègre facilement dans des environnements CI/CD.
   - Fonctionne entièrement en local pour garantir la confidentialité.
   - Open source et soutenu par une communauté active.

3. **Concision** : `promptfoo` est un outil de test pour applications LLM, mettant l'accent sur la sécurité et la fiabilité, avec des évaluations automatisées et des comparaisons de modèles, tout en garantissant la confidentialité des données.",0.1928,0.0182,0.0964
30/01/2024,https://twitter.com/rishdotblog/status/1752329471867371659,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,1/2/2024,Text2SQL,,"SQLCoder-70B, un Llama-Code 70B finetunÃ© pour une tÃ¢che de Text2SQL, atteint 93% de queries correctes dÃ©passant GPT-4 qui est Ã 84% sur le mÃªme test set.",,,,
30/01/2024,https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/BGE_M3/BGE_M3.pdf,1 - High,Model,Library,Processed,31/01/2024,Embeddings / Retrieval,,"BGE-M3 : nouveau modÃ¨le d'embedding Multi-lingue (100+ languages), Multi-granularities (input length jusqu'Ã 8192), Multi-Functionality (unification dense, lexical et multi-vec retrieval). C'est le premier modÃ¨le d'embedding qui supporte les 3 modes de retrieval, Nouveau SOTA sur les benchmarks multi-lingual (MIRACL) et cross-lingual (MKQA)","1. **Introduction :** Le document présente BGE (BAAI General Embedding), un outil de récupération tout-en-un pour les modèles de langage augmentés par récupération (RAG). Il met en avant les projets et modèles récents liés à BGE, ainsi que des informations sur l'installation et l'utilisation.

2. **Points clés :**
   - Introduction de modèles multimodaux comme BGE-VL pour des applications de recherche visuelle.
   - Publication de nouveaux modèles d'embedding et de rerankers, y compris des modèles multilingues et ceux avec des capacités d'apprentissage en contexte.
   - Lancement d'un benchmark pour l'évaluation de la compréhension vidéo de longue durée.
   - Documentation centralisée et tutoriels en cours de développement pour aider les utilisateurs.
   - Accès à des modèles via Hugging Face, avec des instructions pour l'installation et l'utilisation.

3. **Concision :** BGE est un ensemble d'outils pour l'embedding et la recherche, intégrant des modèles avancés pour le traitement du langage et la recherche multimodale, avec un support multilingue et des ressources éducatives.",0.1022,0.0054,0.0591
31/01/2024,https://twitter.com/imhaotian/status/1752621754273472927,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,1/2/2024,Multimodal,,"LLaVA-1.6 with improved reasoning, OCR, and world knowledge. It supports 1 - Higher-res inputs, more tasks, and exceeds Gemini Pro on several benchmarks. LLaVA-1.6 comes with base LLMs of different sizes: Mistral-7B, Vicuna-7B/13B, Hermes-Yi-34B",,,,
31/01/2024,https://twitter.com/CaimingXiong/status/1752380193447260311,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,1/2/2024,Embeddings / Retrieval,,"SFR-Embedding-Mistral : Nouveau modÃ¨le d'embedding basÃ© sur Mistral 7B, SoTA en Retrieval et Reranking",,,,
31/01/2024,https://x.com/arthurmensch/status/1752737462663684344?s=46https://x.com/arthurmensch/status/1752737462663684344?s=46,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,1/2/2024,Leak,,"Un utilisateur de 4chan a uploadÃ© les poids d'un modÃ¨le nommÃ© ""Miqu"" en affirmant qu'il s'agissait d'un leak du modÃ¨le Mistral-2 - Medium de Mistral. Il s'agirait d'un modÃ¨le Llama 2 70B quantisÃ© reffinetunÃ© sur un dataset inconnu. Les premiers tests des utilisateurs s'avÃ¨rent trÃ¨s bons, et le modÃ¨le tallone GPT-4 sur plusieurs benchmarks, dÃ©passant les meilleurs modÃ¨les open source du moment. La rumeur de leak dâ€™un modÃ¨le de chez mistral sâ€™avÃ¨re Ãªtre vraie, il s'agit d'une early version du modÃ¨le Mistral 2 - Medium qui aurait Ã©tÃ© dÃ©ployÃ© chez un client qui l'aurait leak.",,,,
1/2/2024,https://twitter.com/bindureddy/status/1752908801811091594,1 - High,Method,,Erreur: Erreur lors de la récupération de l'article de blog: 400,1/2/2024,RAG,,"RÃ©sultats assez contrintuitifs : ajouter des documents non-pertinents permettrait dâ€™amÃ©liorer les perfs des systÃ¨mes de RAG. Le nombre optimal de documents pertinents ne doit pas dÃ©passer entre 2 et 5 en gÃ©nÃ©ral dâ€™aprÃ¨s le papier, trop de documents liÃ©s Ã la query mais pas strictement pertinent crÃ©Ã© de la confusion pour le LLM",,,,
2/2/2024,Open Source Research Toolkit ðŸ¤—,1 - High,Model,,Erreur: Failed to parse: Open Source Research Toolkit ðŸ¤—,6/2/2024,Vision-Language Models,,"Famille de modÃ¨le multimodaux reprenant le paradigme Sparse Mixture of Experts. Le papier fournit de nombreux insights sur l'influence des diffÃ©rents Ã©lÃ©ments de l'architecture sur les performances du modÃ¨le.
 DÃ©mo : https://huggingface.co/spaces/LanguageBind/MoE-LLaVA",,,,
2/2/2024,https://huggingface.co/RWKV/v5-Eagle-7B,1 - High,Model,Model,Processed,6/2/2024,Open models,,"Un modÃ¨le 7b SOTA en multilingue basÃ© sur une architecture RWKV (non Transformer) (v5).
  - Meilleur que la plupart des modÃ¨les 7b sur des benchmarks multilingues (~25 langues couvertes, 61.4% dâ€™accuracy contre 58.2% pour Mistral 7b, gros jump de performance par rapport Ã la v4).
  - La performance sur les benchmarks anglais se rapproche des modÃ¨les 7b entraÃ®nÃ©s sur le mÃªme ordre de tokens (1T).
 Les RWKV sont une architecture de type RNN qui a lâ€™avantage de :
  - Ãªtre moins gourmande en ressources (VRAM, CPU, GPU, etc) quâ€™un Transformer Ã lâ€™entraÃ®nement et Ã lâ€™infÃ©rence. 10 Ã 100x moins demandant en ressources de calcul quâ€™un pour des grandes tailles de contexte.
  - scaler linÃ©airement par rapport Ã la taille de contexte
  - est nÃ©anmoins trÃ¨s sensible au prompt formatting (lookback nÃ©cessaire)","1. **Introduction** : Le document présente le modèle RWKV-5 Eagle 7B, un modèle de traitement du langage naturel basé sur l'architecture RWKV-v5. Il met en avant ses caractéristiques, ses performances et ses applications potentielles.

2. **Points clés** :
   - Modèle de 7,52 milliards de paramètres.
   - Utilise une architecture de transformateur linéaire, réduisant les coûts d'inférence de 10 à 100 fois.
   - Considéré comme le modèle 7B le plus écologique par token.
   - Entraîné sur 1,1 trillion de tokens dans plus de 100 langues (70 % en anglais).
   - Surpasse tous les modèles de 7B dans les benchmarks multilingues.
   - Performances comparables à Falcon et LLaMA2 en évaluation anglaise.
   - Modèle de base nécessitant un ajustement supplémentaire pour des cas d'utilisation spécifiques.

3. **Concision** : Ce modèle représente une avancée significative dans le domaine des modèles de langage, alliant performance et efficacité écologique.",0.3625,0.1006,0.175
31/01/2024,https://huggingface.co/spaces/AI-Secure/llm-trustworthy-leaderboard,1 - High,Benchmark,,Erreur: Erreur lors de la récupération des données HuggingFace: 401,8/2/2024,Safety Benchmark,,"Ce benchmark mesure la capacitÃ© des LLMs Ã rÃ©pondre de maniÃ¨re sÃ©curisÃ©e, selon 8 dimensions:
 
 * non-ToxicitÃ©,
 * non suivi de stÃ©rÃ©otype,
 * robustesse aux inputs adversariaux,
 * robustesse aux inputs hors distribution,
 * robustesse aux exemples few-shot erronÃ©s,
 * respect de la confidentialitÃ© des donnÃ©es,
 * respect de valeurs morales,
 * respect de lâ€™Ã©quitÃ©.",,,,
2/2/2024,https://www.linkedin.com/posts/soumith_if-you-have-questions-about-why-meta-open-sources-activity-7158947265791873024-hdQz?utm_source=share&amp;utm_2 - Medium=member_ioshttps://www.linkedin.com/posts/soumith_if-you-have-questions-about-why-meta-open-sources-activity-7158947265791873024-hdQz?utm_source=share&amp;utm_2 - Medium=member_ios,1 - High,Business,Pedagogy,Processed,6/2/2024,Open-Source; GAFAM,,Zuck explique pourquoi il mise sur lâ€™open source.,"### Résumé

1. **Introduction** : Le texte aborde l'utilisation des cookies par LinkedIn et d'autres entreprises pour améliorer leurs services et afficher des publicités ciblées. Il évoque également une discussion sur l'open source dans le domaine de l'intelligence artificielle (IA), notamment en lien avec Meta et ses pratiques.

2. **Points clés** :
   - LinkedIn utilise des cookies essentiels et non essentiels pour ses services.
   - Mark Zuckerberg souligne l'importance de la transparence dans les modèles d'IA pour des applications éthiques.
   - Les modèles open source, comme PyTorch, sont essentiels pour le développement de l'IA.
   - Des utilisateurs expriment des frustrations concernant le service client de Meta et la gestion des comptes piratés.

3. **Concision** : Le texte met en lumière l'importance des cookies pour LinkedIn et discute des avantages de l'open source pour les développeurs d'IA, tout en soulignant des problèmes rencontrés par les utilisateurs avec les services de Meta.",0.0303,0.0183,0.0303
2/2/2024,https://www.ft.com/content/80ca3bcd-e819-4002-90b3-3584239180aa?segmentid=45a55daa-06c5-0aba-131a-a1eb758674aehttps://www.ft.com/content/80ca3bcd-e819-4002-90b3-3584239180aa?segmentid=45a55daa-06c5-0aba-131a-a1eb758674ae,1 - High,Business,Pedagogy,Processed,6/2/2024,GAFAM,,Premiers dividendes versÃ©s par Meta alors que leurs dÃ©penses R&D atteignent des montants qui inquiÃ¨tent les investisseurs.,"1. **Introduction** : Le document présente une série d'articles et de nouvelles provenant de la Financial Times, abordant des sujets d'actualité mondiale, notamment des tensions géopolitiques, des enjeux économiques et des développements technologiques.

2. **Points clés** :
   - L'UE exclut les États-Unis, le Royaume-Uni et la Turquie d'un fonds de réarmement de 150 milliards d'euros.
   - Les ambitions de paix de Trump se heurtent à des difficultés croissantes.
   - Trump propose la prise de contrôle des centrales nucléaires ukrainiennes.
   - Le président américain est réprimandé par le juge en chef pour avoir menacé d'impeachment des juges.
   - Poutine accepte un arrêt de 30 jours des frappes sur les infrastructures énergétiques ukrainiennes suite à un appel avec Trump.

3. **Concision** : Ce document traite des tensions géopolitiques et économiques actuelles, en mettant en lumière des décisions clés de l'UE et des déclarations controversées de Trump.",0.0429,0.0,0.0368
5/2/2024,https://www.numerama.com/tech/1622750-gemini-pro-larme-de-google-contre-chatgpt-et-gpt-4-arrive-en-france.html,1 - High,Business,Model,Processed,6/2/2024,LMM: GAFAM,,Gemini Pro arrive en France,"**Introduction :**  
Le document traite des avancées récentes de Google dans le domaine de l'intelligence artificielle, notamment avec le lancement de son modèle de langage Gemini Pro en France, qui vise à rivaliser avec OpenAI et son produit ChatGPT.

**Points clés :**  
- Gemini Pro, lancé le 1er février, est le modèle de langage utilisé par Google Bard pour les utilisateurs européens.
- Google a amélioré son modèle de génération d'images, Imagen 2, capable de créer des photos réalistes.
- L'entreprise prévoit d'intégrer Gemini à Google Assistant, fusionnant ainsi ses services d'IA.
- La conférence I/O de mai 2024 sera dédiée à l'intelligence artificielle, avec l'attente de Gemini Ultra, une version payante de Bard.
- Google cherche à rattraper son retard sur OpenAI, après avoir été initialement dépassé par ChatGPT.

**Concision :**  
Google intensifie sa compétition avec OpenAI grâce à Gemini Pro, un modèle de langage amélioré, et prévoit d'intégrer cette technologie à ses services, tout en développant des capacités de génération d'images. La conférence I/O de 2024 devrait marquer une étape importante pour l'IA chez Google.",0.0194,0.0098,0.0194
5/2/2024,https://arxiv.org/pdf/2402.00838.pdf,1 - High,Model,Model,Processed,6/2/2024,Open models,,"OLMo est un nouveau framework de prÃ©-entrainement, d'instruction finetuning et d'Ã©valuation totalement open-source fait par l'Allen Institute. Suivant ce framework, l'Ã©quipe OLMo a open-sourcÃ© un modÃ¨le 1B et 4 modÃ¨les 7B entrainÃ©s avec diffÃ©rentes configurations software et hardware. Tous ces modÃ¨les sont entraÃ®nÃ©s sur le dataset DoLMA avec au moins 2T de tokens en anglais qui a Ã©tÃ© Ã©galement open-source.
 Ils ont pu entraÃ®nÃ© deux modÃ¨les sur un cluster de GPUs AMD et un cluster NVIDIA et ont obtenu les mÃªmes rÃ©sultats sur les deux clusters.
 Ils recommandent une architecture proche de celle des modÃ¨les Llama sans biais, avec des non-parametric layer norm, des fonctions d'activation SwiGLU, des embedding RoPE et tokenizer basÃ© sur le Byte-pair encoding de GPT-NeoX-20B avec des tokens pour masquer les donnÃ©es personnelles.
 Un modÃ¨le de 65B paramÃ¨tres est en cours d'entrainement.","1. **Introduction** : Le document présente OLMo, un modèle de langage ouvert conçu pour favoriser la recherche scientifique sur les modèles de langage. Dans un contexte où les modèles propriétaires sont devenus courants, OLMo vise à fournir un accès complet aux données d'entraînement, au code et aux outils d'évaluation.

2. **Points clés** :
   - OLMo est un modèle de langage compétitif, avec des variantes de 1B et 7B de paramètres.
   - Il est accompagné d'un ensemble de données d'entraînement ouvert (Dolma) et d'outils pour l'analyse et l'évaluation.
   - Les résultats montrent qu'OLMo-7B est compétitif par rapport à d'autres modèles de taille similaire sur divers benchmarks.
   - OLMo a bénéficié d'une formation sur 2T de tokens avec des techniques d'optimisation avancées.

3. **Concision** : OLMo est un modèle de langage ouvert, conçu pour la recherche, offrant un accès complet à ses données et outils. Il présente des résultats compétitifs sur des benchmarks par rapport à d'autres modèles similaires, avec une formation basée sur un vaste ensemble de données.",0.395,0.1206,0.19
5/2/2024,https://arxiv.org/pdf/2402.00159.pdf,1 - High,Dataset,Dataset,Processed,6/2/2024,Open Dataset,,"Dataset open-source pour entrainer des LLMs (notamment les modÃ¨les OLMo) en anglais avec une toolkit pour filtrer le dataset et faire des analyses variÃ©es. Le dataset est constituÃ© de 3T de tokens en anglais extraits de sources diverses telles que des pages web, des papiers scientifiques, du code, des livres, des rÃ©seaux sociaux et des ressources issues dâ€™encyclopÃ©dies.","### Résumé du document ""Dolma : an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research""

1. **Introduction** : Ce document présente Dolma, un corpus ouvert de trois trillions de tokens en anglais, conçu pour la recherche sur le pré-entraînement des modèles de langage. Il vise à améliorer la transparence des données utilisées pour former des modèles de langage, souvent peu documentées.

2. **Points clés** :
   - Dolma est constitué d'un mélange diversifié de contenu web, d'articles scientifiques, de code, de livres du domaine public, de médias sociaux et de contenus encyclopédiques.
   - Les auteurs ont développé un outil de curation des données pour faciliter la reproduction et l'analyse des pratiques de curation.
   - Des analyses expérimentales montrent l'impact des choix de curation sur les performances des modèles de langage.

3. **Concision** : Dolma est un corpus de trois trillions de tokens destiné à la recherche sur les modèles de langage, visant à accroître la transparence des données. Il inclut divers types de contenu et propose un outil de curation pour faciliter la recherche.",0.1832,0.0421,0.0942
9/2/2024,https://email2.theinformation.com/e/c/eyJlbWFpbF9pZCI6ImRnU2kwUVlDQU9XeWItU3lid0dOamIxZkNQeEVBMWF5WjFwa1VwMD0iLCJocmVmIjoiaHR0cHM6Ly93d3cudGhlaW5mb3JtYXRpb24uY29tL2JyaWVmaW5ncy9vcGVuYWktY2VvLWFsdG1hbi1zYWlkLXRvLWJlLWluLXRhbGtzLXRvLXJhaXNlLXRyaWxsaW9ucy1mb3ItYWktY2hpcC1wb3dlci1pbml0aWF0aXZlP3V0bV9jYW1wYWlnbj0lNUJSRUJSQU5EJTVEKyU1QlRJLUFNJTVEK1RoXHUwMDI2dXRtX2NvbnRlbnQ9MTA5NVx1MDAyNnV0bV9tZWRpdW09ZW1haWxcdTAwMjZ1dG1fc291cmNlPWNpb1x1MDAyNnV0bV90ZXJtPTEyNCIsImludGVybmFsIjoiYTJkMTA2M2NhNGM0MDFlNWIyNmYiLCJsaW5rX2lkIjoxMjkwfQ/904f9f70d1ac43ee9fd4de4025dd8c2f6dba0c4dbbb355f98fadf2ce1b16d0d7OpenAI,1 - High,Business,,Erreur: Erreur lors de la récupération de l'article de blog: 403,,,,"CEO Altman Said to be in Talks to Raise Trillions for AI Chip, Power Initiative
 OpenAI CEO Sam Altman has spoken with investors about raising trillions of dollars to fund an effort to expand the worldâ€™s manufacturing capacity to make semiconductors for artificial intelligence, according to a report in The Wall Street Journal.
 The effort could require $5 trillion to $7 trillion and a sprawling partnership between OpenAI, chipmakers, power providers and governments. Altman has spoken to investors including the United Arab Emirates government and SoftBank CEO Masayoshi Son, as well as chip-making giant Taiwan Semiconductor Manufacturing Co. about the ambitious project, according to the report. His vision would be to raise money from Middle East investors and have TSMC build and run chip-fabrication plants, which could number in the dozens.
 He has also discussed the venture with U.S. Commerce Secretary Gina Raimondo, who has been closely involved in government efforts to promote U.S. chipmaking. A spokesperson for OpenAI didnâ€™t immediately respond to a request for comment.
 Altman is trying to solve one of the key bottlenecks hindering OpenAIâ€™s growth. For the past year, companies working on AI have complained about the lack of Nvidia graphics processing units. More recently people working in cloud computing have warned that power availability is an even bigger constraint. Altman has personally invested in startups that are trying to create energy using cutting edge technology such as nuclear fusion.",,,,
12/2/2024,https://arxiv.org/pdf/2402.00841.pdf,1 - High,Benchmark,Method,Processed,14/02/2024,Small finetuned models VS zero-shot LLM,,"Les auteurs comparent des petits modÃ¨les finetunÃ©s (FLAN T5 <1B) Ã des plus gros modÃ¨les open-source (Llama 2, Mixtral) en zero-shot sur la tÃ¢che de rÃ©sumÃ© de transcripts de meetings.
 Ils montrent que les petits modÃ¨les finetunÃ©s dÃ©passent les gros modÃ¨les sur le test set dans le mÃªme domaine que le dataset d'entrainement mais gÃ©nÃ©ralisent moins bien sur un dataset out of domain.
 Notez que la mÃ©trique utilisÃ©e est le ROUGE score qui est une mÃ©trique basÃ©e sur le token matching entre la gÃ©nÃ©ration et une rÃ©fÃ©rence et ne prend pas vÃ©ritablement en compte le sens globale du rÃ©sumÃ©.
  Une analyse plus dÃ©taillÃ© est faite dans ce tweet: https://twitter.com/rasbt/status/1756316089393270853?s=51&t=THiV4sETaorA7anic27uQA","1. **Introduction :** Ce document examine l'efficacité des modèles de langage de grande taille (LLM) et leur déploiement dans des environnements industriels, en se concentrant sur la tâche de résumé de réunions. Il s'intéresse particulièrement aux modèles plus petits, appelés LLM compacts, pour déterminer s'ils peuvent offrir une alternative économique aux LLM plus volumineux.

2. **Points clés :**
   - Étude comparative entre LLM compacts (FLAN-T5, TinyLLaMA) et LLM plus grands (GPT-3.5, PaLM-2) pour la synthèse de réunions.
   - FLAN-T5-Large (780M paramètres) se distingue en égalant ou surpassant les performances de LLM plus grands en zéro-shot.
   - La plupart des LLM compacts ne surpassent pas les LLM plus grands même après ajustement.
   - Utilisation de jeux de données réels et d'une nouvelle version du jeu de données QMSUM pour l'évaluation.

3. **Concision :** L'étude démontre que FLAN-T5-Large est une solution efficace et rentable pour la synthèse de réunions, surpassant de nombreux LLM plus grands dans des scénarios spécifiques, tout en soulignant les défis liés à l'utilisation de LLM dans des contextes réels.",0.2705,0.0583,0.1449
12/2/2024,https://github.com/NVIDIA/NeMo-Guardrails/blob/develop/docs/security/guidelines.md,1 - High,Method,Library,Processed,14/02/2024,Safety,,"Guidelines de sÃ©curitÃ© pour utiliser un LLM faisant accÃ¨s Ã un service tiers comme une base de donnÃ©es, une API Search ou un calculateur:
 - Ne pas rÃ©vÃ©ler les problÃ¨mes d'accÃ¨s au service tiers dans l'output du LLM quand les requÃªtes fail
 - Logger toutes les interactions LLM/API
 - Bien transmettre les autorisations de l'utilisateur accÃ©dant au LLM au service tiers pour qu'il ne puisse pas accÃ©der au service tiers sans autorisation
 - Bien contraindre les variables envoyÃ©es Ã l'API pour ne pas envoyer des inputs qu'on n'a pas envie
 - Eviter les actions demandÃ©es par le LLM qui changent l'etat du service tiers comme droppÃ© une table, le download de fichiers... Si il est nÃ©cessaire de faire un changement persistent, bien valider le format de l'input envoyÃ© Ã l'API
 - Isoler les infos d'authentification du LLM","### Résumé de NeMo Guardrails

1. **Introduction** : NeMo Guardrails est un outil open-source conçu pour intégrer des ""rails programmables"" dans les applications conversationnelles basées sur des modèles de langage (LLM). Il vise à contrôler les sorties des LLM pour assurer des interactions sûres et appropriées.

2. **Points clés** :
   - Permet de définir des comportements spécifiques des LLM, comme éviter certains sujets ou suivre des chemins de dialogue prédéfinis.
   - Offre des mécanismes de protection contre les vulnérabilités LLM, comme les jailbreaks et les injections de prompts.
   - Supporte plusieurs modèles LLM, dont OpenAI GPT-3.5 et GPT-4, et inclut des types variés de guardrails (input, output, dialog, etc.).
   - Utilise Colang, un langage de modélisation, pour concevoir des flux de dialogue flexibles.

3. **Concision** : NeMo Guardrails est un toolkit permettant d'ajouter des contrôles programmables aux applications LLM, favorisant des interactions sûres et guidées tout en intégrant des protections contre les abus.",0.2733,0.0058,0.1163
12/2/2024,https://docs.garak.ai/garak/,1 - High,Library,Library,Processed,14/02/2024,Safety,,"Librairie qui rÃ©pertorie de nombreuses attaques adversariales pouvant Ãªtre faites sur un LLM et qui propose un scan de LLM qui donne le pourcentage de rÃ©sistance Ã chaque type de mÃ©thode.
 Bien adaptÃ©e aux use-cases type ChatBot mais moins adaptÃ©e Ã d'autres cas d'usage.","1. **Introduction** : Ce document présente garak, un scanner de vulnérabilités pour les modèles de langage (LLM), conçu pour identifier les faiblesses et comportements indésirables dans les technologies basées sur les modèles de langage.

2. **Points clés** :
   - Garak permet de scanner des chatbots et modèles pour détecter les points forts et les vulnérabilités.
   - Il fournit un rapport détaillé sur les performances et les améliorations nécessaires.
   - Comprend des fonctionnalités telles que l'injection de prompts, la génération de toxicité, et des tests automatiques.
   - Utilise des composants comme des sondes de vulnérabilité, des générateurs, et des détecteurs pour évaluer les scans.
   - Propose une fonctionnalité de red-teaming automatique pour simuler des attaques.

3. **Concision** : Garak est un outil essentiel pour tester et améliorer la sécurité des systèmes basés sur des modèles de langage, offrant des analyses approfondies et des recommandations.",0.0909,0.0,0.0485
13/02/2024,https://twitter.com/VikParuchuri/status/1757185570940567666,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,14/02/2024,OCR,,"Surya OCR est un nouveau modÃ¨le d'OCR basÃ© sur une architecture Donut modifiÃ©e, entraÃ®nÃ©e sur un grand nombre de documents y compris scannÃ©s et supporte jusqu'Ã 4 langues dans le mÃªme passage. Surya est prÃ©cis sur plus de 90 langues et surpasse de loin Tesseract, Ã la fois en dÃ©tection de ligne et en retranscription. Aucune comparaison quantiative sur d'autres modÃ¨les OCR plus rÃ©cents n'est proposÃ©es mais des comparaisons qualitatives faites par l'auteur semblent indiquer que le modÃ¨le surpasse EasyOCR et PaddleOCR. Les poids du modÃ¨le sont sous license cc-by-nc-sa-4.0 mais accessible pour pour les entreprises sous les 5 millions$ de chiffre d'affaire dans les 12 derniers mois.",,,,
14/02/2024,https://openai.com/blog/memory-and-new-controls-for-chatgpt,1 - High,Method,,Erreur: Erreur lors de la récupération de l'article de blog: 403,14/02/2024,ChatGPT / Memory,,"OpenAI rajoute progressivement la capacitÃ© Ã ChatGPT de persister des infos Ã travers plusieurs conversations. Cette fonctionalitÃ© est disponible pour une petite partie des utilisateurs de l'offre Free et Plus et il est possible d'opt-out. Techniquement, cette feature est modÃ©lisÃ©e par un nouvel outil nommÃ© `bio` auquel ChatGPT a accÃ¨s et qui permet de persister n'importe quelle information pendant une session de chat. Les informations persistÃ©es sont ensuite passÃ©es en contexte dans les nouvelles sessions de chat.",,,,
15/02/2024,https://github.com/huggingface/text-generation-inference/pull/1539,1 - High,Library,Library,Processed,15/02/2024,TGI / GuidedGeneration,,Le cÃ©lÃ¨bre serveur dâ€™infÃ©rence de LLM dâ€™HuggingFace vient tout juste dâ€™intÃ©grer la possibilitÃ© de faire de la gÃ©nÃ©ration guidÃ©e via une Regex ou plus gÃ©nÃ©ralement une grammaires non-contextuelles grÃ¢ce Ã lâ€™intÃ©gration de lâ€™excellente librairie Outlines. Cette intÃ©gration permet de combiner la scalabilitÃ© dâ€™un serveur dâ€™infÃ©rence avec les garanties apportÃ©es par la gÃ©nÃ©ration guidÃ©e. En particulier il est dÃ©sormais possible de spÃ©cifier un schÃ©ma JSON attendu en sortie et le decoding du modÃ¨le sera adaptÃ© pour garantir la validitÃ© du JSON produit.,"1. **Introduction** : Ce document présente Text Generation Inference (TGI), un serveur développé en Rust et Python pour l'inférence de génération de texte. Utilisé par Hugging Face, TGI permet de déployer des modèles de langage à grande échelle.

2. **Points clés** :
   - Supporte des modèles open-source populaires comme Llama, Falcon et GPT-NeoX.
   - Fonctionnalités : parallélisme tensoriel, streaming de tokens, API compatible avec OpenAI, quantification, et prise en charge de l'architecture distribuée.
   - Installation facile via Docker ou localement, avec des options pour des modèles privés.
   - Prise en charge de divers matériels (Nvidia, AMD, Google TPU).

3. **Concision** : TGI est un outil performant pour déployer des modèles de langage, offrant une multitude de fonctionnalités et une flexibilité d'installation, adapté aux besoins des développeurs et des entreprises.",0.2639,0.0559,0.1389
15/02/2024,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,1 - High,Model,Answer: Method,Processed,21/02/2024,"Gemini Pro 1.5, LLM",,"Nouvelle gÃ©nÃ©ration de modÃ¨les Gemini:
 - Gemini 1.5 Pro est un modÃ¨le multi-modal (texte, audio, video) reposant sur l'architecture mixture-of-experts entrainÃ© sur des contextes de l'ordre du million de tokens,
 - Il est notamment capable d'extraire des informations diluÃ©es dans des trÃ¨s long contextes en atteignant un recall quasi parfait sur la tÃ¢che ""Needle in a Haystack"" sur les trois modalitÃ©s de donnÃ©es
 - Le modÃ¨le est bien meilleur que Gemini Pro 1.0 (27/31 benchmarks meilleurs) et meilleur que Gemini Ultra 1.0 sur de nombreux benchmarks notamment 10/13 benchmarks textuels","Bien sûr, je peux vous aider avec cela. Veuillez fournir le texte que vous souhaitez que je résume.",0.3,0.0,0.2
15/02/2024,https://huggingface.co/spaces/Qwen/Qwen-VL-Max,1 - High,Model,,Erreur: Erreur lors de la récupération des données HuggingFace: 401,22/02/2024,Language-Vision Models,,"- Follow-up de Qwen VL et de Qwen VL Plus
 - LVLM en anglais et chinois
 - Focus sur la partie Visual QA notamment pour des images Ã grande rÃ©solution
 - PrÃ©tendumment SOTA pour de nombreuses variantes pertinentes de VQA (text, chart, ...)
 - Vient avec Touchstone, un benchmark pour modÃ¨les multi-modaux construit Ã partir d'annotation humaines et d'Ã©valuations gÃ©nÃ©rÃ©es par GPT-4
 - License custom â†’ usage commercial OK si â‰¤ 100M users, sinon une requete specifique doit etre formulÃ©e",,,,
16/02/2024,https://ai.meta.com/blog/v-jepa-yann-lecun-ai-model-video-joint-embedding-predictive-architecture/,1 - High,Model,Model,Processed,22/02/2024,Video Embedding,,"- vJEPA est un modÃ¨le d'embedding de videos 
 - Le pitch est assez haut-niveau mais trÃ¨s rÃ©miniscent du Masked Language Modeling dans l'idÃ©e (on masque une portion de la vidÃ©o pour permettre un entraÃ®nement non-supervisÃ©)
 - Encore early stage mais release avec une licence non-commerciale","1. **Introduction** : Le document présente le modèle V-JEPA (Video Joint Embedding Predictive Architecture), une avancée vers une intelligence machine plus avancée, développée par Meta. Ce modèle vise à améliorer la compréhension des interactions dans le monde physique à partir de vidéos.

2. **Points clés** :
   - V-JEPA permet de prédire des parties manquantes de vidéos en utilisant un apprentissage auto-supervisé, ce qui améliore l'efficacité d'entraînement.
   - Le modèle excelle dans la reconnaissance d'interactions détaillées entre objets, sans nécessiter de données étiquetées pour l'entraînement initial.
   - Il offre une flexibilité pour s'adapter à différentes tâches sans nécessiter de réajustement complet des paramètres.
   - Des recherches futures incluent l'intégration de données audio et l'amélioration de la planification sur des horizons temporels plus longs.

3. **Concision** : V-JEPA est un modèle innovant de Meta qui apprend à partir de vidéos pour mieux comprendre le monde, en se concentrant sur les interactions complexes entre objets. Sa méthode d'apprentissage auto-supervisé et sa capacité à s'adapter à diverses tâches en font un outil prometteur pour l'intelligence machine.",0.1154,0.0242,0.0625
19/02/2024,https://twitter.com/Muennighoff/status/1758307967802224770?t=gjXyc6ntmJJ31rm2GkIgEw&amp;s=03,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,19/02/2024,"Text embedding, RAG",,"(Model) GritLM : nouveau LLM qui unie gÃ©nÃ©ration de texte et embedding de texte
 -> Existe en deux versions : 7B et 8x7B
 -> Finetune des modÃ¨les Mistral 7B et Mixtral 8x7B
 -> Les 2 modÃ¨les font parties des meilleurs modÃ¨les dans les deux catÃ©gories, bien qu'ils ne soient pas les meilleurs partout Ã proprement parler
 -> Mais leurs performances sont trÃ¨s impressionantes pour des modÃ¨les 2 in 1
 -> Comparant GritLM 8x7B Ã d'autres LLM (Llamas, Zephyr, Mistral, Mixtral, Tulus), sur un ensemble de benchmarks (MMLU, GSM8K, BBH, HumanEval, Alpaca, ...) GritLM 8x7B ressort numÃ©ro 1 sur la moyenne de ces benchmarks, son concurrent le plus proche Ã©tant Tulu 2 70B
 -> License MIT",,,,
19/02/2024,https://substack.com/redirect/fdd10e8c-70c0-4f24-b396-880bf0c108a7?j=eyJ1IjoieWE2OWoifQ.2lTA-dBFt69lIbJmN0tiRn1AIdNnBVQhVGlbaEqBhYc,1 - High,Business,Library,Processed,21/02/2024,GPU Cloud,,Lambda annonce la levÃ©e de 320 millions de dollars pour le dÃ©veloppement d'un cloud GPU avec des GPU Nvidia Quantum-2 InfiniBand.,"### 1. Introduction
Le document présente les services et produits de Lambda, une entreprise spécialisée dans l'infrastructure de calcul pour l'intelligence artificielle (IA). Il met en avant leur plateforme cloud, leurs systèmes matériels et leur récente levée de fonds de 320 millions de dollars pour développer davantage leur offre.

### 2. Points clés
- **Services Cloud :** 
  - Clusters GPU à la demande pour l'entraînement et le réglage multi-nœuds.
  - Instances GPU facturées à la minute.
  - Clusters GPU privés à grande échelle.
  - Service Kubernetes géré pour accélérer les initiatives IA.
- **Matériel :**
  - Systèmes NVIDIA DGX pour l'IA d'entreprise.
  - Serveurs PCIe avec jusqu'à 8 GPU NVIDIA personnalisables.
  - Stations de travail et bureaux GPU pour le deep learning.
- **Financement :** 
  - Levée de fonds de 320 millions de dollars pour renforcer l'infrastructure cloud et répondre à la demande croissante en IA.

### 3. Concision
Lambda, spécialisée dans l'infrastructure IA, propose des services cloud et matériels avancés. Avec une récente levée de 320 millions de dollars, l'entreprise vise à développer sa plateforme de calcul IA, offrant des clusters GPU à la demande et des systèmes NVIDIA pour répondre aux besoins croissants des équipes d'ingénierie IA.",0.0968,0.0463,0.0783
19/02/2024,https://substack.com/redirect/f4644c8f-a938-4063-93e0-a43b91666584?j=eyJ1IjoieWE2OWoifQ.2lTA-dBFt69lIbJmN0tiRn1AIdNnBVQhVGlbaEqBhYc,1 - High,Business,Library,Processed,22/02/2024,,,[Business] LangChain raised $25 million in new funding led by Sequoia Capital.,"**1. Introduction :** Ce document présente LangChain, un cadre de programmation pour les modèles de langage (LLM) développé par Harrison Chase et Ankush Gola. Il souligne l'importance d'un framework adapté pour simplifier le développement d'applications basées sur l'IA générative.

**2. Points clés :**
- LangChain est un outil open-source facilitant la création d'applications complexes basées sur LLM.
- Plus de 50 000 applications LLM ont été construites avec LangChain, couvrant divers cas d'utilisation.
- La plateforme permet l'accès à des ensembles de données, l'exécution de ""chains"" et l'intégration d'outils externes.
- LangSmith, un produit récent, aide au débogage et à la surveillance des applications LLM.
- La communauté LangChain compte plus de 2 000 contributeurs et 70 000 utilisateurs inscrits à LangSmith.

**3. Concision :** LangChain, fondé par Harrison Chase et Ankush Gola, propose un cadre open-source pour le développement d'applications LLM. Avec plus de 50 000 applications créées, il facilite l'intégration de données et l'exécution de chaînes d'appels. LangSmith, un nouvel outil, optimise le débogage et la performance des applications. La communauté active de LangChain soutient son expansion rapide.",0.0048,0.0,0.0048
19/02/2024,https://www.linkedin.com/posts/yanis-labrak-8a7412145_huggingface-datascience-deeplearning-activity-7165269672303566848-SVDt?utm_source=share&amp;utm_medium=member_androidhttps://www.linkedin.com/posts/yanis-labrak-8a7412145_huggingface-datascience-deeplea,1 - High,Model,Model,Processed,28/02/2024,"LLM, SantÃ©",,"BioMistral, une collection de LLMs open-source basÃ©s sur Mistral 7b pour le domaine mÃ©dical et prÃ©entraÃ®nÃ©s sur des donnÃ©es de PubMed Central.
 Plusieurs variations du modÃ¨le de base (quantized, model-merged)
 Meilleur que Mistral 7b instruct sur 8 des 10 tÃ¢ches de few-shot QA mÃ©dical. Performances proches de GPT 3.5 turbo en SFT.
 License Apache 2.0","1. **Introduction** : Le document présente le lancement de BioMistral, un modèle de langage biomédical multilingue basé sur Mistral 7B, développé par une équipe de chercheurs. Ce modèle est conçu pour des applications médicales et est accessible sur Hugging Face et Arxiv.

2. **Points clés** :
   - BioMistral 7B est le premier modèle biomédical multilingue open-source.
   - Performances de pointe en réponse à des questions médicales, traduites en sept langues.
   - Utilisation de techniques telles que Zero-Shot, Few-Shot et Supervised Fine-Tuning.
   - Entraînement réalisé sur le supercalculateur GENCI Jean-Zay avec 5 000 heures GPU.
   - Soutien de l'Agence nationale de la recherche française pour le projet MALADES.

3. **Concision** : BioMistral représente une avancée significative dans le domaine des modèles de langage biomédical, offrant des performances supérieures et une accessibilité accrue pour les chercheurs et professionnels de la santé.",0.25,0.071,0.1346
20/02/2024,https://arxiv.org/abs/2402.12366,1 - High,Method,Method,Processed,22/02/2024,RLAIF,,"- Papier trÃ¨s intÃ©ressant qui dÃ©cortique le RLAIF de faÃ§on critique.
 - Ils montrent que le gain dÃ» Ã lâ€™Ã©tape de RL est souvent artificiellement dÃ» au fait dâ€™utiliser un modÃ¨le plus faible (typiquement GPT-3.5) pour gÃ©nÃ©rer des donnÃ©es de SFT que pour gÃ©nÃ©rer de lâ€™AI feedback (typiquement GPT-4). Une simple pipeline de SFT avec GPT-4 est plus performante que les pipelines de RLAIF existantes pour la plupart des modÃ¨les open-source de base (Llama7B, Mistral7B, Mistral8x7B)
 - Sur AlpacaEval, RL GPT-3.5 + AIF GPT-4 est moins performant que SFT GPT-4, et RL GPT-4 + AIF GPT-4 nâ€™augmente pas substantiellement les performances.","### 1. Introduction
Ce document évalue de manière critique l'efficacité de l'apprentissage par renforcement avec rétroaction d'IA (RLAIF) pour l'alignement des modèles de langage. Il remet en question la nécessité de la complexité de la phase de RLAIF, en montrant que les améliorations de performance proviennent souvent de l'utilisation de modèles enseignants plus faibles pour la collecte de données.

### 2. Points clés
- RLAIF combine un ajustement supervisé (SFT) avec des retours d'un modèle critique.
- Les modèles enseignants plus faibles (comme GPT-3.5) entraînent des performances sous-optimales par rapport à l'utilisation de modèles plus puissants (comme GPT-4) pour le SFT.
- Les résultats montrent que SFT sur des données de haute qualité (GPT-4) surpasse souvent RLAIF.
- L'efficacité de RLAIF varie selon les familles de modèles et les protocoles d'évaluation.

### 3. Concision
L'étude conclut que l'efficacité de RLAIF pourrait être surestimée et souligne l'importance de l'utilisation de modèles enseignants performants pour améliorer les capacités d'instruction des modèles de langage.",0.3073,0.0995,0.1667
20/02/2024,https://crfm.stanford.edu/2024/02/18/helm-instruct.html,1 - High,Benchmark,Method,Processed,20/02/2024,,,"HELM est un benchmark de LLM trÃ¨s rÃ©pandu dans la littÃ©rature, qui permet de positionner un LLM sur une Ã©chelle absolue de performance en l'Ã©valuant sur un ensemble de scÃ©narios et sur plusieurs mÃ©triques. Jusque maintenant, HELM Ã©valuait les LLM sur des rÃ©ponses relativement courtes. Helm Instruct permet de complÃ©menter HELM en Ã©valuant le LLM sur un ensemble de 7 scÃ©narios avec 5 mÃ©triques (helpfulness, understandability, completeness, conciseness, harmlessness) sur de la gÃ©nÃ©ration plus open-ended. 
 Helm Instruct utilise une Ã©chelle absolue contrairement Ã d'autres benchmarks comme LMSys Arena, qui utilisent des comparaisons entre paires de prÃ©dictions.
 Les auteurs utilisent plusieurs Ã©valuateurs (GPT-4, Claude 2, ainsi que des Ã©valuations humaines via Amazon MTurk et Scale AI). Les analyses montrent que ces diffÃ©rents Ã©valuateurs ne sont pas toujours en accord les uns avec les autres, particuliÃ¨rement sur les mÃ©triques de conciseness et understandability
 GPT-4 (0314) est le plus corrÃ©lÃ© avec le jugement humain (respectivement 0.66 et 0.56 de Pearson correlation avec les annotateurs de MTurk et Scale AI).
 Les LLM sont moins bons pour Ã©valuer les critÃ¨res de understandability et conciseness que pour Ã©valuer les critÃ¨res de harmlessness et completeness .","### 1. Introduction
Ce document présente le cadre d'évaluation HELM Instruct, conçu pour évaluer les modèles de langage suivant des instructions de manière multidimensionnelle et avec des notes absolues. Il aborde les défis de l'évaluation des réponses générées par ces modèles, notamment leur nature ouverte et variée.

### 2. Points clés
- **Modèle utilisé** : HELM Instruct évalue plusieurs modèles de langage, dont GPT-4 et Anthropic Claude v1.3.
- **Méthode** : L'évaluation repose sur cinq critères : utilité, compréhension, exhaustivité, concision et innocuité, notés de 1 à 5.
- **Résultats** : GPT-4 a souvent obtenu les meilleures notes, tandis qu'Anthropic Claude excelle en compréhension et innocuité. Les évaluations humaines montrent plus de variance que celles des modèles.
- **Comparaison des évaluateurs** : Les corrélations entre évaluateurs humains et modèles varient, indiquant des différences dans l'évaluation de la concision et de la compréhension.

### 3. Concision
Le document introduit le cadre d'évaluation HELM Instruct pour les modèles de langage, mettant en avant l'importance d'une évaluation ouverte et multidimensionnelle. Les résultats montrent que GPT-4 est souvent le meilleur, tandis qu'Anthropic Claude excelle en compréhension. Les évaluations humaines présentent une variance plus élevée que celles des modèles.",0.4089,0.0625,0.1644
20/02/2024,https://github.com/datadreamer-dev/DataDreamer,1 - High,Library,Library,Processed,21/02/2024,Synthetic Data Generation,,"Librairie qui permet de crÃ©er et formaliser des worklows complexes de gÃ©nÃ©ration de donnÃ©es qui sont facilement reproductibles. Permet notamment de faire de l'attributed prompting, des gÃ©nÃ©rations variÃ©es Ã partir d'un mÃªme prompt, augmenter des datasets existants...","1. **Introduction :** Le document présente DataDreamer, une bibliothèque Python open-source dédiée à la génération de données synthétiques, à la création de flux de travail de formation et à l'alignement de modèles. Elle est conçue pour être simple, efficace et de niveau recherche.

2. **Points clés :**
   - **Création de flux de travail :** Permet de créer et exécuter des flux de travail de prompting complexes avec des LLMs open-source ou basés sur API.
   - **Génération de jeux de données synthétiques :** Facilite la création de nouveaux jeux de données ou l'augmentation de jeux existants.
   - **Formation de modèles :** Supporte l'alignement, le fine-tuning et la distillation des modèles.
   - **Caractéristiques :** Simple, de niveau recherche, efficace, reproductible et facilite le partage de jeux de données et de modèles.

3. **Concision :** DataDreamer est une bibliothèque puissante pour la génération de données synthétiques et l'entraînement de modèles, conçue pour être accessible et efficace, tout en permettant une reproduction facile des workflows.",0.1534,0.0585,0.1164
20/02/2024,https://arxiv.org/abs/2307.11088,1 - High,Method,Dataset,Processed,22/02/2024,Eval,,"L-Eval est une nouvelle mÃ©thode d'Ã©valuation pour les modÃ¨les de langage Ã long contexte. Elle comprend un dataset de questions/rÃ©ponses avec de longs contextes couvrant 20 tÃ¢ches. L'Ã©tude examine comment les approches dâ€™Ã©valuation Ã base de n-gram matching ou Ã base de ""LLM-as-a-judge"" gÃ©nÃ©ralisent lorsque le contexte est long. En effet, les LLM Ã©valuateur ne peuvent pas prendre lâ€™intÃ©gralitÃ© du contexte en entrÃ©e, lâ€™Ã©valuation se base donc uniquement sur la similaritÃ© entre la prÃ©diction et la ground truth, ce qui requiert une attention particuliÃ¨re pour que le LLM Ã©valuateur ne valorise pas la prÃ©sence dans la prÃ©diction dâ€™informations supplÃ©mentaires dont il ne peut pas vÃ©rifier la vÃ©racitÃ©.
 De plus, les modÃ¨les recevant des contextes longs ont tendance Ã gÃ©nÃ©rer des prÃ©dictions longues, et les Ã©valuations (par n-gram ou par LLM) ont tendance Ã Ãªtre moins corrÃ©lÃ©es avec les Ã©valuations humaines lorsque la diffÃ©rence de longueur entre la prÃ©diction et la ground truth est trop grande. Pour rÃ©aliser leur benchmark de seize modÃ¨les, des instructions indiquant la longueur de prÃ©diction attendue sont donc rajoutÃ©s au prompt, ce qui permet de rÃ©duire ce biais. Les rÃ©sultats du benchmark sont que GPT-4-32k surpasse clairement tous les autres modÃ¨les par une marge trÃ¨s significative sur les tÃ¢ches closed-ended. Pour les tÃ¢ches open-ended, Ã©tant donnÃ© que les textes d'entrÃ©e sont gÃ©nÃ©ralement plus longs et qu'une comprÃ©hension globale du contexte est nÃ©cessaire, Claude-100k bat tous les autres modÃ¨les, y compris GPT-4-32k.","### Introduction
Le document présente L-Eval, une initiative visant à établir une évaluation standardisée des modèles de langage à long contexte (LCLMs). Cela répond à l'intérêt croissant pour l'extension de la longueur de contexte des modèles de langage, permettant de mieux traiter des entrées longues et des conversations historiques.

### Points clés
- **Modèle et Méthode** : L-Eval propose une suite d'évaluation comprenant 20 sous-tâches et 508 documents longs, avec plus de 2 000 paires de questions-réponses annotées par des humains.
- **Évaluation** : L'étude met en lumière les limites des métriques traditionnelles basées sur les n-grams, recommandant l'utilisation d'une évaluation améliorée par des instructions de longueur (LIE) et des juges LLM.
- **Résultats** : Les résultats montrent un écart significatif entre les modèles open-source et commerciaux, notamment dans les tâches ouvertes où les modèles open-source ont des performances inférieures.

### Concision
L-Eval vise à standardiser l'évaluation des LCLMs, en développant des sous-tâches diversifiées et en soulignant les insuffisances des métriques traditionnelles. Les résultats révèlent des lacunes entre les modèles open-source et commerciaux, en particulier pour les tâches ouvertes.",0.5115,0.162,0.235
21/02/2024,https://blog.llamaindex.ai/introducing-llamacloud-and-llamaparse-af8cedf9006b,1 - High,Method,Library,Processed,21/02/2024,PDF Parser,,LlamaIndex a sorti son parser de pdfs propriÃ©taire. Il est possible de le tester sur quelques exemples. On peut le garder en tÃªte pour comparer notre parser au leur.,"### Introduction
Le document présente LlamaCloud et LlamaParse, des solutions innovantes pour le traitement et la récupération de données complexes, visant à améliorer les applications d'IA générative et de génération augmentée par récupération (RAG).

### Points clés
- **LlamaParse** : Un parseur avancé capable de traiter des documents complexes, tels que des PDF avec des tableaux et des graphiques, facilitant ainsi la réponse à des questions complexes.
- **API de récupération et d'ingestion gérée** : Permet de charger et traiter facilement des données pour les applications RAG, tout en simplifiant la gestion des sources de données.
- **Développement d'applications** : LlamaCloud et LlamaParse visent à réduire le temps consacré à la manipulation des données, permettant aux développeurs de se concentrer sur la logique métier.
- **Partenariats** : Collaboration avec des entreprises comme DataStax et MongoDB pour intégrer ces solutions dans des systèmes de RAG.

### Concision
LlamaCloud et LlamaParse offrent des solutions de traitement de données avancées pour les applications IA, facilitant la récupération d'informations à partir de documents complexes et optimisant le développement d'applications RAG.",0.0406,0.0,0.0305
21/02/2024,https://blog.google/technology/developers/gemma-open-models/,1 - High,Model,Model,Processed,21/02/2024,LLM,,"Nouveaux LLMs open-source de Google - Gemma 2B & 7B avec licence permettant un usage commercial. Ils release aussi des versions instruct finetunÃ©es avec SFT+RLHF.
 - Architecture: longueur de contexte de 8192 tokens, architecture transformer classique + Multi-Query Attention pour 2B et multi head attention pour 7B, RoPE Embeddings, GeGLU Activations, Normalizer Location
 - peu d'infos sur le dataset: Pre-EntrainÃ©s et finetunÃ©s sur de l'anglais principalement
 - Performances meilleures que Llama 2 7B sur tous les benchmarks
 - Performances meilleures que Mistral 7B en moyenne. Le modÃ¨le est meilleur que Mistral 7B sur MMLU, MATH, HumanEval, similaires sur HellaSwag et moins bonnes sur GSM8k
 Attention tout de mÃªme, Maxime Labonne nous met en garde sur Linkedin (https://www.linkedin.com/posts/maxime-labonne_does-gemma-overfit-the-open-llm-leaderboard-activity-7166220798427402242-lJFm?utm_source=share&utm_medium=member_desktop) que Gemma s'avÃ¨re lÃ©gÃ¨rement dÃ©ceptif sur d'autres benchmarks plus corrÃ©lÃ©s au jugement humain (AGIEval et BigBench), et qu'il n'est pas Ã exclure que les bons rÃ©sultats sur le OpenLLM Leaderboard soient surestimÃ©s du fait d'un overfitting.","**1. Introduction :**  
Le document présente le lancement de Gemma, une nouvelle génération de modèles open source développés par Google, destinée à faciliter le développement responsable de l'intelligence artificielle (IA).

**2. Points clés :**  
- Gemma comprend des modèles légers, dérivés de la technologie des modèles Gemini, disponibles en deux tailles : 2B et 7B.
- Un kit d'outils pour l'IA générative responsable est inclus, offrant des conseils et des outils pour des applications AI sécurisées.
- Les modèles peuvent être utilisés sur différents frameworks (JAX, PyTorch, TensorFlow) et sont optimisés pour le matériel AI moderne.
- Accès gratuit via Kaggle et crédits Google Cloud pour les utilisateurs débutants.
- Gemma vise à atteindre des performances de pointe tout en respectant des normes de sécurité et de responsabilité.

**3. Concision :**  
Gemma est une nouvelle série de modèles open source de Google, conçue pour un développement IA responsable, avec des outils et des ressources pour les développeurs.",0.3094,0.0333,0.1436
21/02/2024,https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO,1 - High,Model,Model,Processed,22/02/2024,LLM,,"Nous Research release un nouveau finetune de mistral 7B utilisant DPO par dessus l'excellent modÃ¨le OpenHermÃ¨s 2.5-Mistral 7B. Ce nouveau modÃ¨le surpasse lÃ©gÃ¨rement le modÃ¨le OpenHermÃ¨s sur les benchmarks AGIEval, BigBench Reasoning Test, GPT4All suite et TruthfulQA. Un excellent modÃ¨le 7B qui a de bonnes chances de devenir notre nouvelle recommandation par dÃ©faut, en drop-in replacement du modÃ¨le OpenHermÃ¨s prÃ©cÃ©demment recommandÃ© jusqu'ici.","1. **Introduction** : Ce document présente le modèle Nous Hermes 2 basé sur Mistral 7B DPO, qui est une intelligence artificielle avancée développée pour améliorer l'interaction avec les utilisateurs à travers des dialogues structurés.

2. **Points clés** :
   - Modèle : Nous Hermes 2, une version améliorée de Mistral 7B, optimisée par DPO.
   - Entraînement : Basé sur 1 million d'instructions de qualité GPT-4, utilisant principalement des données synthétiques.
   - Performances : Améliorations notables sur des benchmarks tels qu'AGIEval, BigBench, et TruthfulQA.
   - Format de prompt : Utilise ChatML pour une interaction plus structurée et intuitive avec l'IA.
   - Reconnaissance : Remerciements à FluidStack pour le soutien technique.

3. **Concision** : Le modèle Nous Hermes 2 est une avancée significative en IA, offrant des interactions améliorées et des performances supérieures dans divers benchmarks.",0.2552,0.0694,0.131
22/02/2024,https://arena.lmsys.org/,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 403,22/02/2024,"Mistral, LLM",,"Mistral lance discrÃ¨tement un nouveau modÃ¨le prototype nommÃ© Â« Mistral-next Â», dÃ©jÃ intÃ©grÃ© au LMSys Arena. Il faudra attendre plus d'annotation crowdsourcÃ©es dans l'arÃ¨ne pour positionner ce modÃ¨le par rapport aux autres. Mistral n'a pas encore communiquÃ© officiellement d'informations autour de ce modÃ¨le, y compris au sujet de son caractÃ¨re open source ou closed source.",,,,
23/02/2024,https://www.phind.com/blog/introducing-phind-70b,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 403,26/02/2024,Code Generation,,"Introducing Phind-70B â€“ closing the code quality gap with GPT-4 Turbo while running 4x faster
 
 Phind.com est un chatbot Ã destiation des dÃ©veloppeurs, capable de chercher des informations pertinentes sur Internet pour complÃ©ter et sourcer ses rÃ©ponses. RÃ©cemment, Phind a annoncÃ© Phind 70B, basÃ© sur CodeLLama 70B et dont le pretrainng a Ã©tÃ© poursuivi sur 50B tokens additionnels, avec une taille de sÃ©quence de 32K tokens.
 
 Sur le benchmark de gÃ©nÃ©ration de code HumanEval, Phind-70B atteint 82.3%, dÃ©passant GPT-4-Turbo (81,1%). Qualitativement, Phind 70B semblerait moins ""paresseux"" que son homologue d'OpenAI, et ne rechignerait pas Ã gÃ©nÃ©rer des solutions complÃ¨tes et dÃ©taillÃ©es.
 En terme de serving, le modÃ¨le est dÃ©ployÃ© par Phind sur un cluster de H100 et utilise le serveur d'infÃ©rence TensorRT-LLM de NVIDIA. Phind prÃ©voit de publier les poids de la version 34B de Phind dans les prochaines semaines, et planifie Ã©galement d'open sourcer la version 70B, sans donner de date cette fois-ci.
 Petit fun fact amusant Ã la fin de l'article...",,,,
23/02/2024,https://arxiv.org/pdf/2402.13446.pdf,1 - High,Method,Pedagogy,Processed,29/02/2024,LLM annotations survey,,"Survey sur lâ€™utilisation de LLMs pour lâ€™annotation de donnÃ©es:
 - analyse selon 3 dimensions: annotation de donnÃ©es avec LLM, Ã©valuation des annotations par LLM, and l'apprentissage sur des annotations gÃ©nÃ©rÃ©es par LLM
 - Liste de papiers dispos ici: https://github.com/Zhen-Tan-dmml/LLM4Annotation","### 1. Introduction
Ce document présente un état des lieux sur l'utilisation des grands modèles de langage (LLM) pour l'annotation et la synthèse de données, soulignant leur potentiel à automatiser des processus traditionnellement laborieux et coûteux. Il se concentre sur les applications spécifiques des LLM dans le domaine de l'annotation de données, en abordant les défis et les stratégies associés.

### 2. Points clés
- **Modèles étudiés** : GPT-4, Gemini, LLaMA-2.
- **Méthodes** : Annotation générée par LLM, évaluation des annotations, utilisation des annotations LLM.
- **Contributions** : Taxonomie des types de données annotables, revue des stratégies d'apprentissage, discussion des défis techniques et éthiques.
- **Objectif** : Servir de guide pour les chercheurs et praticiens souhaitant exploiter les LLM dans l'annotation de données.

### 3. Concision
Ce document explore l'application des LLM pour l'annotation et la synthèse de données, mettant en avant leur capacité à automatiser des processus complexes. Il aborde les méthodes d'annotation, l'évaluation des résultats, et les défis éthiques, tout en fournissant une taxonomie utile pour les futurs travaux dans ce domaine.",0.1667,0.0964,0.1313
26/02/2024,https://arxiv.org/pdf/2402.14658.pdf,1 - High,Model,Model,Processed,29/02/2024,"Code Generation, Multi-turn",,"OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement
 Introduction de OpenCodeInterpreter, une famille de modÃ¨les de code open-source conÃ§us pour gÃ©nÃ©rer, exÃ©cuter et affiner itÃ©rativement du code.
 - L'objectif est de combler le fossÃ© entre les modÃ¨les open-source et les systÃ¨mes propriÃ©taires, comme GPT-4 Code Interpreter, en matiÃ¨re de capacitÃ©s d'exÃ©cution et de raffinement itÃ©ratif.
 - Sept modÃ¨les sont disponibles : 4 basÃ©s sur CodeLlama et 3 basÃ©s sur Deepseek Coder, avec des tailles allant de 1.3B Ã 70B paramÃ¨tres
 - Le dataset d'entraÃ®nement, Code-Feedback, contient 68k interactions multi-tours qui incluent Ã la fois les instructions de l'utilisateur et les retours du compilateur.
 - Licence Apache 2.0","### Résumé

1. **Introduction** : Le document présente OpenCodeInterpreter, un système open-source conçu pour la génération, l'exécution et le perfectionnement itératif de code, en réponse aux limitations des modèles de code open-source actuels.

2. **Points clés** :
   - OpenCodeInterpreter intègre l'exécution et les retours humains pour affiner dynamiquement le code.
   - Utilise un ensemble de données Code-Feedback comprenant 68K interactions multi-tours.
   - Évaluation sur des benchmarks tels que HumanEval et MBPP, avec des performances comparables à celles de GPT-4.
   - OpenCodeInterpreter-33B atteint une précision de 83,2 (76,4) sur HumanEval, rivalisant avec GPT-4.
   - Réduction de l'écart de performance entre les modèles open-source et les systèmes propriétaires.

3. **Concision** : OpenCodeInterpreter est un système avancé de génération de code qui combine exécution et feedback humain pour améliorer les performances des modèles open-source, atteignant des résultats comparables à ceux de GPT-4 sur des benchmarks standards.",0.4393,0.1802,0.2081
26/02/2024,https://mistral.ai/news/mistral-large/,1 - High,Model,Model,Processed,29/02/2024,,,"MistralAI a rÃ©cemment rendu publique sa plate-forme proposant notamment leur dernier modÃ¨le Mistral Large:
  - Mistral Large a des capacitÃ©s proches (mais en deÃ§a) de GPT-4 sur la plupart des benchmarks citÃ©s (MMLU, WinoG, HellaS, ...)
  - Un accent particulier semble avoir Ã©tÃ© mis sur ses capacitÃ©s multilingues (mais pas de comparaison Ã GPT-4 de disponible pour l'instant)
  - Les poids du modÃ¨le ne sont pas disponibles et le modÃ¨le lui-mÃªme ne peut-Ãªtre utilisÃ© qu'Ã travers une API (Mistral ou Azure)
  - Globalement, le modÃ¨le semble avoir Ã©tÃ© conÃ§u comme un drop-in replacement Ã GPT-4. Il dispose d'un mode JSON et de features de function calling.","1. **Introduction** : Ce document présente le modèle Mistral Large, un modèle de génération de texte avancé, ainsi que son lancement sur la plateforme Mistral et Azure, en mettant en avant ses capacités et ses performances.

2. **Points clés** :
   - Mistral Large est le modèle phare de Mistral, avec des capacités de raisonnement de premier plan.
   - Il est disponible en plusieurs langues (anglais, français, espagnol, allemand, italien) et excelle dans les tâches de raisonnement multilingue.
   - Le modèle a un contexte de 32K tokens pour un rappel précis d'informations.
   - Mistral Large est classé deuxième parmi les modèles accessibles via API, juste après GPT-4.
   - Un nouveau modèle optimisé, Mistral Small, est également lancé pour des charges de travail à faible latence.

3. **Concision** : Mistral Large, modèle de génération de texte avancé, est lancé sur la plateforme Mistral et Azure, offrant des capacités multilingues, un contexte large et des performances de premier plan. Un modèle optimisé, Mistral Small, est aussi disponible.",0.2905,0.0899,0.1508
27/02/2024,https://x.com/_akhaliq/status/1762349999919071528?t=PEKDyR5m0Qnfe8S7N5EfAg&amp;s=31,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,29/02/2024,Structured Knowledge Grounding,,"StructLM est une sÃ©rie de modÃ¨les entraÃ®nÃ©s pour rÃ©aliser des tÃ¢ches sur des sources de donnÃ©es structurÃ©es telles que des tableaux, des ontologies et des bases de donnÃ©es. 
 - Leur Ã©tude rÃ©vÃ¨le que les capacitÃ©s des LLM sur cette tÃ¢che, appelÃ©e Structured Knowledge Grounding (SKG), laissaient encore Ã dÃ©sirer : ChatGPT par exemple est en retard d'une moyenne de 35 % par rapport Ã l'Ã©tat de l'art.
 - Un dataset de 1.1 millions instructions a Ã©tÃ© utilisÃ© pour entraÃ®ner trois modÃ¨les de tailles 7B, 13B et 34B basÃ©s sur l'architecture de Code-LLaMa.
 - Le modÃ¨le 34B n'est pas significativement meilleur que le modÃ¨le 7B, l'augmentation de la taille du modÃ¨le ne semble donc pas Ãªtre une condition suffisante pour amÃ©liorer les performances sur cette tÃ¢che difficile.
 - License MIT 
 - Lien vers le HF https://huggingface.co/collections/TIGER-Lab/structlm-65dcab5a183c499cc365fafc",,,,
6/2/2024,https://twitter.com/bindureddy/status/1754665925834690907,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,7/2/2024,LLM,,"Smaug72B est un nouveau modÃ¨le finetunÃ© par AbacusAI, par dessus un modÃ¨le le modÃ¨le moreh/MoMo-72B-lora-1.8.7-DPO, lui-mÃªme basÃ© sur Qwen-72B.
 Il est notamment meilleur que Mistral 2 - Medium sur MMLU, HellaSwag et particuliÃ¨rement bon en maths sur GSM-8K.",,,,
27/02/2024,https://www.gartner.com/en/newsroom/press-releases/2024-02-19-gartner-predicts-search-engine-volume-will-drop-25-percent-by-2026-due-to-ai-chatbots-and-other-virtual-agents,1 - High,Business,,Erreur: Erreur lors de la récupération de l'article de blog: 403,29/02/2024,,,"- Selon Gartner, d'ici 2026, le volume des recherches sur les moteurs de recherche traditionnels diminuera de 25 % en raison de l'utilisation croissante de chatbots et d'autres agents virtuels basÃ©s sur l'IA.
 - Les agents virtuels remplaceront progressivement les recherches traditionnelles, ce qui obligera les entreprises Ã revoir leur stratÃ©gie de marketing.
 - Les moteurs de recherche devront davantage valoriser la qualitÃ© du contenu pour compenser la quantitÃ© de contenu gÃ©nÃ©rÃ© par IA",,,,
27/02/2024,https://arxiv.org/pdf/2312.01552.pdf,1 - High,Method,Method,Processed,29/02/2024,In context learning,,"SystÃ¨me de prompting URIAL pour aligner des base models sans instruct tuning:
 - Les auteurs du papier ont remarquÃ© que les distributions de tokens des modÃ¨les prÃ©-entrainÃ©s et des modÃ¨les instruct-finetunÃ©s ne sont pas trÃ¨s Ã©loignÃ©es.
 - Les diffÃ©rences se font surtout sur des tokens stylistiques
 - URIAL se base sur l'in-context learning en promptant le base model avec un sytem prompt bien dÃ©taillÃ© et des exemples de sortie attendues bien ""stylisÃ©es""
 - un dataset just-eval-instruct est crÃ©Ã© pour Ã©valuer les modÃ¨le selon 6 dimensions: helpfulness, clarity, factuality, depth, engagement, and safety avec GPT-4 et GPT3.5. Ce dataset est un merge de 5 datasets existants: AlpacaEval2, MT-Bench, LIMA, HH-RLHF-redteam et MaliciousInstruct
 - mistral urial 7b est bien meilleur que mistral 7b instruct sur ce dataset et urial llama 2 70b est on par avec le llama 2 70b instruct sur ces 6 dimensions","1. **Introduction** : Ce document explore la question de l'alignement des modèles de langage de grande taille (LLMs) et propose une méthode innovante, URIAL, pour les aligner sans ajustement de paramètres. Il remet en question l'efficacité des méthodes traditionnelles d'alignement basées sur le fine-tuning.

2. **Points clés** :
   - L'alignement traditionnel utilise l'apprentissage supervisé et le renforcement à partir des retours humains (RLHF).
   - Une étude récente suggère que l'alignement peut être superficiel, n'affectant que des tokens stylistiques.
   - URIAL utilise l'apprentissage en contexte (ICL) avec seulement trois exemples pour obtenir un alignement efficace.
   - Les résultats montrent qu'URIAL surpasse les modèles alignés par SFT et RLHF en termes de performance sur divers critères.

3. **Concision** : URIAL propose une méthode d'alignement sans tuning efficace pour les LLMs, remettant en question l'importance des ajustements traditionnels. Les résultats démontrent son efficacité en surpassant les méthodes classiques sur plusieurs critères d'évaluation.",0.3099,0.0529,0.152
15/02/2024,https://www.shacknews.com/article/138730/openai-search-engine-google-bing,1 - High,Business,News,Processed,,,,"OpenAI travaillerait avec Bing search sur un nouveau moteur de recherche, basÃ© sur ses propres technologies d'IA pour concurrencer Google","**Introduction :**  
Le document traite d'un rapport selon lequel OpenAI, le créateur de ChatGPT, développe un moteur de recherche pour rivaliser avec Google. Cette initiative pourrait bouleverser le secteur de la recherche en ligne.

**Points clés :**  
- OpenAI travaille sur un service de recherche web, potentiellement alimenté par la technologie de Bing, soutenue par Microsoft.  
- La nouvelle a entraîné une baisse des actions de Google, passant de 145,94 $ à 142,76 $ en après-bourse.  
- Si OpenAI collabore réellement avec Bing, cela pourrait avoir des conséquences significatives pour le marché de la recherche en ligne.  
- Donovan Erskine, le journaliste, partage son parcours et son intérêt pour les jeux vidéo et le cinéma.

**Concision :**  
OpenAI développe un moteur de recherche pour concurrencer Google, alimenté par Bing. Cette annonce a fait chuter les actions de Google. Le projet pourrait transformer le paysage de la recherche en ligne.",0.0774,0.039,0.0645
28/02/2024,https://arxiv.org/abs/2402.17733,1 - High,Model,Model,Processed,28/02/2024,Translation,,"- Ils proposent un modÃ¨le qui adresse des tÃ¢ches liÃ©es au domaine de la traduction, comme la post-Ã©dition, la correction grammaticale ou le NER, dans plusieurs langues. Release du dataset dâ€™entraÃ®nement TowerBlocks : https://huggingface.co/datasets/Unbabel/TowerBlocks-v0.1
 - Partant dâ€™un Llama-2, ils construisent TowerBase pretraining sur des donnÃ©es monolingues et parallÃ¨les, puis SFT sur des instructions liÃ©es Ã la traduction pour obtenir TowerInstruct.
 - RÃ©sultats : TowerInstruct 13B est meilleur que les grands modÃ¨les open-source (Llama 70B - Mixtral 8x7B) et les modÃ¨les dÃ©diÃ©s sur de la traduction de et vers lâ€™anglais. Sur de la post-Ã©dition et du NER multilingue, meilleur que GPT-3.5","### Résumé

1. **Introduction** : Ce document présente TOWER, un modèle de langage multilingue open-source conçu pour des tâches liées à la traduction. Il vise à améliorer les performances des modèles de langage généralistes en les adaptant à des flux de travail de traduction spécifiques.

2. **Points clés** :
   - **Modèle utilisé** : TOWER est basé sur LLaMA-2 et comprend deux versions : TOWERBASE et TOWERINSTRUCT.
   - **Méthode** : Formation continue avec des données multilingues et ajustement supervisé sur des tâches spécifiques de traduction.
   - **Résultats** : TOWERINSTRUCT surpasse les alternatives open-source et rivalise avec des modèles fermés tels que GPT-4 sur plusieurs tâches de traduction, y compris l'estimation de la qualité et la correction d'erreurs grammaticales.
   - **Ressources** : Publication des modèles TOWER, du jeu de données de spécialisation TOWERBLOCKS et d'un cadre d'évaluation TOWEREVAL.

3. **Concision** : TOWER est un modèle de langage multilingue qui améliore les performances en traduction grâce à une formation continue et un ajustement spécifique, surpassant ainsi de nombreux modèles concurrents.",0.3529,0.1129,0.2139
28/02/2024,https://huggingface.co/protectai/deberta-v3-base-prompt-injection,1 - High,Model,Model,Processed,29/02/2024,"Injection de prompts, Safety",,"ModÃ¨le DeBERTaV3 fine tunÃ© sur de la dÃ©tection d'injection de prompt : 
 - PrÃ©sente des performances remarquables sur l'ensemble d'Ã©valuation avec un F1-score de 0,9998.
 - Le dataset contenait ~30% de prompts avec injection et ~70% de prompts normaux.
 - Une intÃ©gration avec LangChain est disponible via la classe HuggingFaceInjectionIdentifier (https://python.langchain.com/docs/guides/safety/hugging_face_prompt_injection)
 - License Apache 2.0","1. **Introduction** : Ce document présente la carte du modèle ""deberta-v3-base-prompt-injection"", une version fine-tunée du modèle DeBERTa de Microsoft, conçue pour détecter les injections de prompts dans les textes.

2. **Points clés** :
   - Modèle : deberta-v3-base-prompt-injection, fine-tuné par Laiyer.ai.
   - Objectif : Classifier les entrées en deux catégories : `0` (pas d'injection) et `1` (injection détectée).
   - Résultats d'évaluation : 
     - Perte : 0.0010
     - Précision : 99.99%
     - Rappel : 99.97%
     - Précision : 99.98%
     - F1 : 99.98%
   - Données d'entraînement : ~30% d'injections de prompts et ~70% de prompts valides.
   - Utilisation : Intégration possible avec des bibliothèques comme Transformers et Langchain.

3. **Concision** : Ce modèle vise à améliorer la sécurité des systèmes en identifiant les injections de prompts avec une performance élevée.",0.2721,0.1027,0.1701
28/02/2024,https://huggingface.co/blog/starcoder2,1 - High,Model,,Erreur: Erreur lors de la récupération des données HuggingFace: 401,29/02/2024,Code Generation,,"- BigCode publie StarCoder2, entraÃ®nÃ© sur The Stack v2, le plus grand dataset open source de code conÃ§u pour le pretraining de modÃ¨le avec ses 4 trillions de tokens et ses 600+ languages de programmation.
 - Par rapport Ã The Stack v1, cette deuxiÃ¨me version du dataset contient plus d'Ã©chantillons, suit une procÃ©dure amÃ©liorÃ©e de dÃ©tection du langage et de la licence, ainsi qu'une meilleure heuristique de filtrage. 
 - Trois tailles de modÃ¨les sont disponibles : 3B, 7B, et 15B paramÃ¨tres
 - Licence : BigCode OpenRAIL-M v1 license (mÃªme que StarCoder)",,,,
28/02/2024,https://x.com/emollick/status/1762885468528533573?s=46&amp;t=THiV4sETaorA7anic27uQA,1 - High,Business,,Erreur: Erreur lors de la récupération de l'article de blog: 400,29/02/2024,Service Client,,"Klarna AI est une entreprise amÃ©ricaine qui a intÃ©grÃ© OpenAI dans leur service client. D'aprÃ¨s leur communiquÃ© de presse:
 - l'assistant a eu 2.3 millions de conversations
 - il a pu rÃ©alisÃ© le travail de 700 agents
 - le score de satisfaction client est le mÃªme qu'avec des agents humains
 - rÃ©duction de 25% de requÃªtes rÃ©pÃ©tÃ©es
 - le temps de rÃ©solution des problÃ¨mes est passÃ© de 11min Ã 2 min
 Ces propos doivent Ãªtre nuancÃ©s car il s'agit d'un communiquÃ© de presse. Certains utilisateurs indiquent que le bot finit par donner la main Ã un humain dans de nombreux cas.",,,,
28/02/2024,https://www.linkedin.com/posts/mirco-ravanelli-489b692a_speechbrain-speechbrain-speech-activity-7168623278687768576-vMDp?utm_source=share&amp;utm_medium=member_android,1 - High,Library,Library,Processed,29/02/2024,Speech-to-Text,,"Deepmind a travaillÃ© avec plusieurs dÃ©veloppeurs de jeux video pour construire un agent capable de suivre des instructions en langage naturel dans diffÃ©rents environnements 3D:
 - A partir de ces jeux videos, un dataset d'actions a Ã©tÃ© compilÃ© avec des triplets (video de l'action, comandes Ã Ã©xecuter et instruction en langage naturel)
 -","**Introduction :** Ce document annonce la sortie de SpeechBrain 1.0, un outil open-source pour l'intelligence artificielle conversationnelle, développé par Mirco Ravanelli et son équipe. Il souligne les avancées significatives et les nouvelles fonctionnalités de cette version.

**Points clés :**
- SpeechBrain 1.0 inclut plus de 200 recettes et 100 modèles pré-entraînés sur Hugging Face.
- Améliorations en reconnaissance vocale, intégration de K2, et nouvelles méthodes de recherche par faisceau.
- Ajout de recettes et modèles pour le traitement de la parole, de l’audio, du texte et de l’EEG.
- Compatibilité renforcée avec des modèles Hugging Face comme GPT-2 et Llama-2.
- Support pour l'apprentissage continu et des processus d'augmentation de données améliorés.
- Remerciements aux sponsors et partenaires, et appel à soutenir le projet sur GitHub.

**Concision :** Le résumé présente les principales avancées de SpeechBrain 1.0, soulignant ses nouvelles fonctionnalités et l'importance de la collaboration au sein de la communauté.",0.0805,0.0116,0.0517
29/02/2024,https://huggingface.co/datasets/espnet/yodas,1 - High,Dataset,Dataset,Processed,29/02/2024,Audios,,"Des Ã©quipes de WavLab, un labo de recherche de Carnegie Mellon, ont regroupÃ© un dataset dâ€™audios de Youtube labellisÃ©s avec les sous-titres gÃ©nÃ©rÃ©s par youtube: 
 - le dataset couvre 140 langues diffÃ©rentes
 - il contient notamment ~21k heures de franÃ§ais, 167k h d'anglais, 37k h en espanhol, 20k h de portugais ou encore 14k d'allemand","1. **Introduction** : Ce document présente le jeu de données YODAS, qui contient 369 510 heures d'enregistrements audio et leurs transcriptions, extraites de YouTube. La version mise à jour, YODAS2, inclut des audios non segmentés et un taux d'échantillonnage plus élevé.

2. **Points clés** :
   - Deux modes de chargement du jeu de données : standard (téléchargement complet) et streaming (accès rapide sans téléchargement).
   - 149 langues sont disponibles, chaque langue étant divisée en sous-ensembles pour faciliter le traitement.
   - Les sous-ensembles sont classés selon la nature des légendes : manuelles ou automatiques.
   - Les statistiques détaillées de chaque sous-ensemble sont fournies.

3. **Concision** : Le jeu de données YODAS offre un large éventail d'enregistrements audio multilingues, facilitant ainsi la recherche en traitement du langage naturel et reconnaissance vocale.",0.1773,0.0143,0.0922
19/01/2024,https://www.theverge.com/2024/1/18/24042354/mark-zuckerberg-meta-agi-reorg-interview,1 - High,Business,Model,Processed,19/01/2024,AGI,,"Meta cherche Ã crÃ©er une AGI dans un futur proche. Avant la fin de l'annÃ©e, Meta aura accumulÃ© 340,000 Nvidia H100 GPUs.
 Meta entraÃ®ne actuellement Llama3, qui aura des capacitÃ©s de gÃ©nÃ©ration de code.
 La stratÃ©gie open / close de ces futurs assets n'est pas claire.","### Résumé

1. **Introduction** : L'article aborde les ambitions de Mark Zuckerberg, PDG de Meta, concernant la création d'une intelligence artificielle générale (AGI). Il discute des défis et des stratégies de Meta dans la course à l'AGI, tout en soulignant la dynamique de l'industrie technologique.

2. **Points clés** :
   - Zuckerberg souhaite construire une AGI sans avoir de définition précise ou de calendrier.
   - Meta prévoit d'acquérir plus de 340 000 GPU Nvidia H100 pour le développement de l'IA.
   - L'entreprise adopte une approche open source pour ses modèles d'IA, contrastant avec les tendances plus fermées d'autres entreprises comme OpenAI.
   - Meta continue de se concentrer sur le métavers tout en intégrant l'IA dans ses produits.
   - Zuckerberg envisage un futur où les interactions humaines incluront également des AIs.

3. **Concision** : Zuckerberg de Meta ambitionne de créer une AGI tout en intégrant l'IA dans ses applications. Avec un investissement massif en puissance de calcul et une approche open source, Meta se positionne pour influencer l'avenir de l'IA et du métavers, où les interactions humaines impliqueront également des intelligences artificielles.",0.1436,0.0448,0.0941
4/3/2024,https://www.anthropic.com/news/claude-3-family,1 - High,Model,Model,Processed,6/3/2024,Claude 3,,"Anthropic a annoncÃ© la sortie de la famille des modÃ¨les Claude 3: Haiku (le plus petit), Sonnet et Opus (le plus grand). Ce sont des modÃ¨les multimodaux derriÃ¨re l'API Anthropic qui rivalisent avec les modÃ¨les d'OpenAI.
 - Anthropic annonce que le modÃ¨le Opus est meilleur que GPT-4 sur tous les benchmarks. NÃ©anmoins, d'autres chercheurs n'ont pas les mÃªme rÃ©sultats que Anthropic pour l'Ã©valuation de GPT-4 prÃ©sentÃ©s dans ce github. Les rÃ©sultats sont tout de mÃªme proches entre les deux modÃ¨les
 - Sur la modalitÃ© de Vision, Opus est proche de GPT4-Vision mais lÃ©gÃ¨rement en dessous des capacitÃ©s de Gemini Ultra 1.0 sur 3 benchmarks sur 5.
 - Les coÃ»ts sont trÃ¨s compÃ©titifs. Bien qu'Opus soit assez cher, Sonnet n'est pas loin de GPT-4 et 2 Ã 3 fois moins cher. Pour des applications demandant moins de raisonnement, Haiku est trÃ¨s intÃ©ressant car il a des performances bien meilleures que GPT-3.5 et il est 2 fois moins cher sur les input tokens.
 - Claude 2 Opus atteint un recall Ã 99% sur le benchmark ""Needle in a Haystack"" sur tout le contexte de 200k tokens","1. **Introduction** : Le document annonce la famille de modèles Claude 3, qui établit de nouvelles normes dans le domaine de l'intelligence artificielle pour diverses tâches cognitives. Cette famille comprend trois modèles : Claude 3 Haiku, Claude 3 Sonnet et Claude 3 Opus, chacun offrant des performances croissantes.

2. **Points clés** :
   - Opus est le modèle le plus intelligent, surpassant ses concurrents sur plusieurs benchmarks.
   - Sonnet est deux fois plus rapide que Claude 2, adapté pour les tâches nécessitant des réponses rapides.
   - Haiku est le modèle le plus rapide et le plus économique pour des requêtes simples.
   - Les modèles affichent des capacités de vision avancées et une meilleure précision avec moins de refus.
   - Une fenêtre de contexte de 200K tokens est disponible, avec une capacité d'entrée potentielle de plus d'un million de tokens.

3. **Concision** : La famille Claude 3, avec Opus, Sonnet et Haiku, redéfinit l'intelligence artificielle en offrant des performances supérieures, une rapidité accrue et une meilleure compréhension contextuelle, tout en garantissant une utilisation responsable et sécurisée.",0.4895,0.1429,0.2053
5/3/2024,https://twitter.com/NielsRogge/status/1765043331896095021,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,6/5/2024,DocumentAI,,"Le modÃ¨le UDOP, dont le papier est sorti en Mars 2023, est enfin disponible sur le HuggingFace Hub. UDOP est un modÃ¨le de traitement documentaire combinant les architectures LayoutLM et Donut. Il s'agit donc d'un modÃ¨le encodeur-dÃ©codeur comme Donut mais dont l'encodeur est capable de prendre en entrÃ©es les 3 modalitÃ©s de LayoutLM : texte, layout et vision, alors que Donut ne prenait en entrÃ©e que l'image de la page. Le dÃ©codeur est capable de rÃ©soudre des tÃ¢ches gÃ©nÃ©ratives de texte, layout et vision via du prompting en langage naturel. Le modÃ¨le UDOP surpasse les modÃ¨les LayoutLM et Donut sur un ensemble de tÃ¢ches de visual QA, table QA, layout recognition, extraction d'information...
 Le modÃ¨le est disponible en license MIT sauf la partie dÃ©codeur visuel qui ne sera disponible que via API.",,,,
5/3/2024,https://predibase.com/blog/lorax-outlines-better-json-extraction-with-structured-generation-and-lora,1 - High,Method,Method,Processed,7/3/2024,#ConstrainedGeneration,,"Article de blog dÃ©taillant les effets du fine-tuning et de la gÃ©nÃ©ration contrainte dans une tÃ¢che de NER formulÃ©e avec de l'extraction structurÃ©e en JSON:
  - le fine-tuning est en LoRA et la gÃ©nÃ©ration structurÃ©e est faite avec Outlines
  - la gÃ©nÃ©ration structurÃ©e seule amÃ©liore largement les performances du modÃ¨le (Mistral 7B) mais pas autant qu'un fine-tuning
  - utiliser une brique de gÃ©nÃ©ration structurÃ©e garantit mieux la structure des prÃ©dictions qu'un fine-tuning
  - faire de la gÃ©nÃ©ration structurÃ©e avec un modÃ¨le fine-tunÃ© amÃ©liore encore les performances","1. **Introduction** : Ce document présente LoRAX, un serveur d'inférence open source pour les modèles de langage, qui permet d'extraire et de générer des données JSON structurées. Il met en avant l'utilisation de la génération structurée et du fine-tuning pour améliorer la précision des sorties JSON.

2. **Points clés** :
   - LoRAX prend en charge des adaptateurs fine-tunés sur un modèle de base partagé.
   - Deux méthodes principales sont abordées : la génération structurée et le fine-tuning.
   - Les résultats montrent que la combinaison des deux méthodes améliore la qualité des sorties JSON.
   - L'application de schémas JSON améliore la structure, mais le fine-tuning est nécessaire pour garantir la précision du contenu.
   - Des benchmarks indiquent que l'association des deux techniques offre les meilleures performances.

3. **Concision** : LoRAX permet une extraction et une génération de JSON précises grâce à la combinaison de génération structurée et de fine-tuning. Les résultats montrent que cette approche améliore significativement la qualité des sorties par rapport à l'utilisation d'un modèle de base seul.",0.372,0.1699,0.1884
6/3/2024,https://huggingface.co/blog/leaderboard-contextual,1 - High,Benchmark,,Erreur: Erreur lors de la récupération des données HuggingFace: 401,7/3/2024,Vision Language Models,,"Dataset crÃ©Ã© Ã l'UCLA pour les modÃ¨les multimodaux:
  - Le dataset vise Ã Ã©valuer les capacitÃ©s de raisonnement sur des Ã©lÃ©ments textuels prÃ©sents dans des images.
  - Il est conÃ§u pour Ãªtre difficile pour les modÃ¨les actuels: le meilleur modÃ¨le, GPT-4V, est Ã 20 points de moins que la performance humaine.
  - Les Ã©chantillons sont divisÃ©s en plusieurs sous-tÃ¢ches (Navigation, Utilisation du Web,...)
  - GPT-4V a un score parfait, au-delÃ de la performance humaine, pour la sous-tÃ¢che ""Raisonnement abstrait"" du benchmark 
  - Pas de licence: a priori, les donnÃ©es sont ouvertes",,,,
6/3/2024,https://www.mixedbread.ai/blog/mxbai-rerank-v1,1 - High,Model,Model,Processed,6/3/2024,"Reranking, Retrieving",,"Mixedbread.ai opensource une famille de modÃ¨les de reranking, dÃ©clinÃ©e en 3 tailles (xsmall, base et large) basÃ©e sur DeBERTa-v2. Un dÃ©monstrateur en ligne permet de tester les capacitÃ©s de reranking du modÃ¨le xsmall. Sur un subset de 11 datasets issus du benchmark BEIR, ces modÃ¨les se comparent trÃ¨s favorablement en reranking et offrent un trÃ¨s bon compromis entre performance et temps d'infÃ©rence. Le modÃ¨le mxbai-rerank-base-v1 permet de dÃ©passer le modÃ¨le propriÃ©taire d'embedding cohere-embed-v3, (72.3 vs 72.3 d'accuracy sur 11 datasets du benchmark BEIR)., mÃªme si cette comparaison est assez trompeuse : cohere-embed-v3 est un bi-encodeur alors qu'il s'agit ici d'un modÃ¨le de reranking. Le dÃ©tail des rÃ©sultats obtenus sont disponibles sur ce Google Sheet.
 Pas de rÃ©sultats sur la multilingualitÃ© du modÃ¨le...","1. **Introduction** : Ce document présente le lancement d'une nouvelle famille de modèles de reranking open-source, nommée Mixedbread, qui vise à améliorer la qualité des recherches en ligne en utilisant des techniques de recherche sémantique.

2. **Points clés** :
   - Trois modèles disponibles : mxbai-rerank-xsmall-v1, mxbai-rerank-base-v1, et mxbai-rerank-large-v1, chacun offrant un équilibre différent entre performance et taille.
   - Les modèles s'intègrent facilement dans les systèmes de recherche existants, comme Elasticsearch, en ajoutant une étape de reranking après la récupération initiale des résultats.
   - Tests effectués montrent que ces modèles surpassent les systèmes de recherche traditionnels en termes de pertinence et d'exactitude des résultats.
   - L'API sera bientôt disponible pour faciliter l'intégration des modèles sans gestion d'infrastructure.

3. **Concision** : Le lancement des modèles Mixedbread promet d'améliorer les recherches en ligne en offrant des solutions open-source efficaces, intégrables aux systèmes existants, avec des performances supérieures aux méthodes traditionnelles.",0.337,0.0929,0.1739
6/3/2024,https://twitter.com/lmsysorg/status/1765098152775045612,1 - High,Benchmark,,Erreur: Erreur lors de la récupération de l'article de blog: 400,8/3/2024,,,"Avec lâ€™arrivÃ©e rÃ©cente de nombreux modÃ¨les propriÃ©taires, le classement de la ChatBot Arena Ã©volue:
 
 * Lâ€™Arena propose aux utilisateurs de comparer des gÃ©nÃ©rations provenant de modÃ¨les diffÃ©rents et de les classer. Cela permet de dÃ©river un leaderboard basÃ© sur des prÃ©fÃ©rences humaines.
 * Les derniÃ¨res itÃ©rations de GPT-4 restent au sommet, battant la meilleure version de Claude 3 dâ€™une courte tÃªte. Gemini Pro complÃ¨te le podium.
 * La suite du leaderboard comprend les versions plus anciennes de GPT-4, une autre version de Claude 3 et les derniers modÃ¨les de Mistral
 * Le premier modÃ¨le aux poids ouverts est le Qwen 72B dâ€™Alibaba et le premier modÃ¨le en licence libre est le Mixtral 8x7B d'e Mistral",,,,
8/3/2024,https://huggingface.co/urchade/gliner_multi,1 - High,Model,Model,Processed,14/03/2024,NER,,"-> ModÃ¨le Bi-directionnel de NER
 -> En 0-shot, outperform chatGPT et des LLMs finetunÃ©s sur la tÃ¢che
 -> Architecure de transformer encoder qui encode joitement les entitÃ©s ainsi que l'input sentence
 -> Cependant derniÃ¨re phase du modÃ¨le sÃ©parÃ©e pour les entitÃ©s et pour les tokens de l'input sentence
 -> Sur 20 datasets de benchmark de NER, en 0-shot, GLINER surpasse ChatGPT et UniNER-7B sur 13 d'entre eux, et reste assez proche de la concurrence sur les autres
 -> Pour sa taille de modÃ¨le, est trÃ¨s intÃ©ressant pour son approche bi-encoder
 -> License: cc-by-nc-4.0","1. **Introduction** : Ce document présente GLiNER, un modèle de reconnaissance d'entités nommées (NER) utilisant un encodeur transformateur bidirectionnel similaire à BERT. GLiNER se distingue des modèles NER traditionnels en étant capable d'identifier tout type d'entité sans se limiter à des catégories prédéfinies.

2. **Points clés** :
   - GLiNER a été formé sur le jeu de données **Pile-NER** et est disponible en versions commerciales.
   - Plusieurs modèles sont proposés, avec des tailles de paramètres variant de 166M à 459M.
   - Le modèle peut être utilisé pour prédire des entités dans différents textes multilingues.
   - Exemples d'utilisation incluent la reconnaissance d'entités comme des personnes, des dates et des récompenses.

3. **Concision** : GLiNER est un modèle NER innovant, flexible et accessible, capable de traiter divers types d'entités dans plusieurs langues, offrant une alternative aux modèles NER classiques et aux LLM coûteux.",0.2437,0.044,0.1313
8/3/2024,https://huggingface.co/spaces/allenai/WildBench,1 - High,Benchmark,,Erreur: Erreur lors de la récupération des données HuggingFace: 401,12/3/2024,Benchmark,,"Compilation de 1024 tÃ¢ches ""real-world"" difficiles demandÃ©es par des vrais utilisateurs Ã ChatGPT:
 - Les auteurs ont annotÃ© chaque instruction avec la rÃ©fÃ©rence de GPT-4, le type de tÃ¢che parmi 12 catÃ©gories de tÃ¢ches et une checklist de critÃ¨res Ã respecter pour passer l'Ã©valuation
 - Ils ont crÃ©Ã© un leaderboard auquel peut Ãªtre ajoutÃ© n'importe quel LLM
 - Le benchmark est censÃ© Ãªtre dynamique et sera updatÃ© rÃ©guliÃ¨rement
 - Contrairement aux Ã©valuations d'Anthropic, Claude Opus se rÃ©vÃ¨le bien moins bon que GPT-4 sur ce benchmark. NÃ©anmoins, le benchmark est biaisÃ© car GPT-4 est utilisÃ© comme juge. Un des auteurs a prÃ©sentÃ© sur X quelques amÃ©liorations qu'ils souhaitent conduire pour amÃ©liorer le benchmark.",,,,
8/3/2024,https://huggingface.co/blog/lyon-nlp-group/french-mteb-datasets,1 - High,Benchmark,,Erreur: Erreur lors de la récupération des données HuggingFace: 401,12/3/2024,Embedding Benchmark,,"Des nouvelles tÃ¢ches en franÃ§ais pour les modÃ¨les d'embeddings ont Ã©tÃ© ajoutÃ©es au Massive Text Embedding Benchmark d'Amazon:
 - Une association de NLP lyonnaise a compilÃ© 14 datasets existants et crÃ©Ã© 3 nouveaux datasets en franÃ§ais pour couvrir les tÃ¢ches du benchmark existant dans les autres langues
 - Mistral-Embed arrive premier sur ce benchmark",,,,
11/3/2024,https://twitter.com/elonmusk/status/1767108624038449405,1 - High,Business,,Erreur: Erreur lors de la récupération de l'article de blog: 400,21/03/2024,,,OpenAI va opensourcer Grok,,,,
11/3/2024,https://arxiv.org/pdf/2403.04132.pdf,1 - High,Method,Method,Processed,7/5/2024,Eval,,"L'Ã©quipe LMSys de chez Berkeley publie un article sur le fonctionnment de a cÃ©lÃ¨bre arÃ¨ne Chatbot Arena.
 L'arÃ¨ne fonctionne sous la forme de ""battle"" : l'utilisateur donne un prompt puis l'arÃ¨ne lui prÃ©sente les rÃ©ponses obtenues par 2 LLMs diffÃ©rents Ã l'aveugle, l'utilisateur peut alors dire lequel des deux modÃ¨les a donnÃ© la meilleure rÃ©ponse, ou alors ""Ã©galitÃ©"" ou bien ""les 2 sont mauvais"". Les auteurs confirment que ces votes crowdsourcÃ©s sont bien alignÃ©s, Ã la fois avec le jugement de GPT-4 mais aussi avec des annotations expertes.
 La collecte d'annotations a commencÃ© en avril 2023, au moment de la publication de l'article, 240 000 votes ont Ã©tÃ© collectÃ©s. 
 En terme de couverture linguistique, 77% des battle sont anglais, 5% en chinois, les autres langues reprÃ©sentent 2% du total. En moyenne chaque modÃ¨le supportÃ© par l'arÃ¨ne reÃ§oit 8 000 votes.
 Le papier prÃ©sente beaucoup de dÃ©tails sur les mÃ©thodes utilisÃ©es pour passer des comparaisons Ã un score scalaire, pour filtrer les prompts de mauvaises qualitÃ©, ou encore sur le topic modeling effectuÃ© sur les prompts utilisateurs.
 Selon ce benchmark, gpt-4 turbo domine l'arÃ¨ne, mais notons qu'entre temps, les modÃ¨les d'Anthropic challengent le haut du classement.","### 1. Introduction
Le document présente Chatbot Arena, une plateforme ouverte pour évaluer les modèles de langage de grande taille (LLMs) en fonction des préférences humaines. Cette initiative répond aux défis posés par l'évaluation de l'alignement des LLMs avec les attentes des utilisateurs.

### 2. Points clés
- **Méthodologie** : Utilisation d'une approche de comparaison pair-à-pair et de crowdsourcing pour recueillir des votes d'une base d'utilisateurs diversifiée.
- **Données collectées** : Plus de 240 000 votes en quelques mois, impliquant environ 90 000 utilisateurs dans plus de 100 langues.
- **Analyse de la qualité** : Les questions générées par les utilisateurs sont variées et discriminantes, avec une bonne concordance entre les votes des utilisateurs et ceux des experts.
- **Impact** : Chatbot Arena est devenu un tableau de classement de référence dans le domaine des LLMs, largement cité par les développeurs et entreprises.

### 3. Concision
Chatbot Arena est une plateforme innovante pour évaluer les LLMs selon les préférences humaines, utilisant une méthode de comparaison pair-à-pair. Avec plus de 240 000 votes collectés, elle offre une analyse robuste de la qualité des réponses des modèles, devenant ainsi une référence dans l'évaluation des LLMs.",0.3779,0.0694,0.1613
11/3/2024,https://twitter.com/kundan_official/status/1760336497536823420,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,14/03/2024,Hallucination,,"* Sur les tÃ¢ches oÃ¹ les LLMs doivent exploiter un document de rÃ©fÃ©rence comme le RAG ou le rÃ©sumÃ© automatique, les LLMs peuvent parfois manquer de factualitÃ© et ""inventer"" des faits non supportÃ©s par les sources. GenAudit est un modÃ¨le dâ€™assistance au fact-checking qui identifie les erreurs de facualitÃ© et propose des modifications post-hoc en citant le span de texte associÃ©.
 * GenAudit permet de dÃ©tecter et corriger des erreurs de factualitÃ© commises par de nombreux modÃ¨les, y compris de trÃ¨s bonnes baselines telles que GPT-4 ou Mistral 7B.
 * Les auteurs finetunent une sÃ©rie de plusieurs modÃ¨les spÃ©cialisÃ©s dans cette tÃ¢che dâ€™identification et de correction, et dÃ©montrent que les modÃ¨les encoder-decoder obtiennent un meilleur trade-off prÃ©cision-recall que les modÃ¨les decoder-only, Ã la fois sur la tÃ¢che dâ€™identification et de correction. Flan-T5-XXL et Flan-UL2 sont les meilleurs modÃ¨les pour ce type de tÃ¢ches.
 * Les modÃ¨les commerciaux comme GPT-3.5 Turbo ou GPT-4, promptÃ©s en few shot obtiennent de bien moins bonnes performances que les modÃ¨les finetunÃ©s.
 * Les modÃ¨les finetunÃ©s seront released publiquement.",,,,
12/3/2024,https://txt.cohere.com/command-r/,1 - High,Model,Model,Processed,12/3/2024,RAG/ToolUse,,"Cohere a release les poids de leur modÃ¨le Command-R Ã 35 milliards de paramÃ¨tres, spÃ©cialiste du RAG avec citations de sources et de l'utilisation d'outils:
 - Ils comparent les prÃ©fÃ©rences humaines du modÃ¨le versus Mixtral sur diffÃ©rents domaines mÃ©tier. Les outputs sont prÃ©fÃ©rÃ©s 70% des cas en moyenne. 
 - Sur un triple benchmark open-source mÃªlant Natural Questions (Kwiatkowski et al. 2019), TriviaQA (Joshi et al. 2017) et HotpotQA (Yang et al. 2018) en single retrieval, Command-R est plus performant que Mixtral, GPT-3.5 et Llama 70B d'au moins 5 points d'accuracy.
 - En multi-step reasoning avec des outils de recherche, il atteint de trÃ¨s bonnes performances dÃ©passant les 3 modÃ¨les au-dessus d'au moins 5 points sur HotpotQA.
 - Le modÃ¨le dÃ©passe Llama 2 70B, Mixtral et GPT-3.5 sur des benchmarks de multiple choice et traduction sur 10 langues diffÃ©rentes,
 - Les poids du modÃ¨le sont disponibles sur HuggingFace, mais sont Ã usage non commercial
 - Il est disponible sur l'API Cohere Ã $0.50/1M de tokens d'entrÃ©e et $1.50/1M de tokens de sortie","1. **Introduction** : Le document présente Command A, une plateforme d'IA qui propose une gamme de modèles linguistiques performants et évolutifs, ainsi que des outils avancés de recherche et de récupération d'informations.

2. **Points clés** :
   - **Command** : Modèles linguistiques à haute performance et évolutivité.
   - **Aya Expanse** : Modèles multilingues performants dans 23 langues.
   - **Embed** : Outil multimodal de recherche et récupération d'informations.
   - **Rerank** : Modèle améliorant la qualité de recherche par une approche sémantique.
   - **North** : Plateforme d'IA intégrée pour optimiser le travail des employés modernes.
   - **Compass** : Système intelligent de recherche pour découvrir des insights commerciaux.

3. **Concision** : Command A est une plateforme d'IA offrant des modèles linguistiques avancés et des outils de recherche, visant à améliorer l'efficacité des travailleurs dans divers secteurs.",0.3873,0.0426,0.1831
12/3/2024,https://arxiv.org/pdf/2403.06634.pdf,1 - High,Method,Method,Processed,12/3/2024,Hacking,,"Les auteurs dÃ©crivent une attaque permettant de rÃ©cupÃ©rer une partie des poids d'un modÃ¨le Ã partir d'une API.
  - L'attaque s'appuie uniquement sur les rÃ©sultats renvoyÃ©s par l'API et repose sur des considÃ©rations basiques autour de la structure des rÃ©seaux de neurones.
  - L'attaque permet de dÃ©duire la dimension de l'espace latent utilisÃ© par le modÃ¨le et la matrice de projection utilisÃ©e pour obtenir les logits finaux
  - En pratique, l'attaque est rendue possible par certaines fonctionnalitÃ©s courantes comme la possibilitÃ© de biaiser les logits et de rÃ©cupÃ©rer les logits pour au moins une partie des tokens
  - En l'Ã©tat, les coÃ»ts nÃ©cessaires pour cette attaque sont d'autant plus Ã©levÃ©s que le vocabulaire du modÃ¨le est grand et que la partie observable des logits est petite. En particulier, ces coÃ»ts deviennent prohibitifs (mais pas inaccessibles) pour des modÃ¨les comme GPT 3.5/GPT 4.","### Résumé

1. **Introduction** : Ce document présente une attaque de vol de modèle, permettant d'extraire des informations précises de modèles de langage de production comme ChatGPT d'OpenAI ou PaLM-2 de Google, en accédant à leur API. L'attaque vise à récupérer la couche de projection d'embedding d'un modèle transformer.

2. **Points clés** :
   - L'attaque réussit à extraire la matrice de projection complète pour les modèles ada et babbage d'OpenAI pour moins de 20 USD.
   - La dimension cachée des modèles a été confirmée comme étant 1024 et 2048, respectivement.
   - La méthode repose sur des requêtes ciblées pour extraire des informations sur la dernière couche du modèle, révélant des dimensions cachées et des matrices de poids.
   - Des mesures de défense ont été mises en place par OpenAI et Google suite à la divulgation responsable de cette recherche.

3. **Concision** : Cette recherche démontre qu'il est possible de voler des informations critiques de modèles de langage de production via des attaques sur leurs API, soulevant des préoccupations de sécurité significatives.",0.4112,0.1071,0.1878
12/3/2024,https://www.cognition-labs.com/blog,1 - High,Model,Library,Processed,14/03/2024,"Code, SWE",,"Nouveau SOTA sur SWE-Bench: 
 Devin est un nouvel agent basÃ© sur un modÃ¨le de langue capable de rÃ©soudre des taches de software engineering jamais rÃ©solues auparavant automatiquement:
 - il est dotÃ© de capacitÃ©s de planification. Pour chaque nouvelle tÃ¢che complexe, il peut formuler un plan des Ã©tapes Ã suivre,
 - le modÃ¨le a accÃ¨s Ã un interprÃ©teur de code, une CLI et un moteur de recherche,
 - il peut ainsi rÃ©agir aux erreurs rencontrÃ©es sur son code et dÃ©bugger itÃ©rativement,
 - sur SWE-Bench, un benchmark trÃ¨s difficile de code, il rÃ©ussi Ã rÃ©soudre 13.86% des tÃ¢ches tandis que le prÃ©cÃ©dent SOTA n'en rÃ©solvait que 4.8% 
 - Devin est accessible sur une waiting list","1. **Introduction** : Le document présente une série de mises à jour et d'initiatives concernant Devin, un agent IA spécialisé dans le développement logiciel, qui a été amélioré pour mieux répondre aux besoins des utilisateurs et des entreprises.

2. **Points clés** :
   - Devin a amélioré ses capacités de test d'interface utilisateur et peut effectuer des modifications en lot.
   - Support ajouté pour GitLab et intégration de Sonnet 3.7.
   - Introduction de messages vocaux et gestion des comptes d'entreprise.
   - Initiatives open source pour aider à l'échelle du développement.
   - Améliorations de la vitesse, de la fiabilité et de la personnalisation de Devin.

3. **Concision** : Devin, l'agent IA, a été optimisé pour le développement logiciel avec de nouvelles fonctionnalités et un soutien accru pour les plateformes sociales et open source, tout en améliorant la collaboration et la productivité.",0.2467,0.0268,0.1267
14/03/2024,https://twitter.com/MetaGPT_/status/1767965444579692832,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 400,14/03/2024,"Code, SWE",,"* Data Interpreter est un agent autonome capable dâ€™orchestrer des notebooks Python, un browser, un terminal et nâ€™importe quel autre outil pour rÃ©soudre des tÃ¢ches complexes de Data Science.
 * Data Interpreter utilise une capacitÃ© de planification dynamique Ã diffÃ©rentes rÃ©solutions de dÃ©cisions, ainsi que du retrieving sur des expÃ©riences passÃ©es stockÃ©es dans une pool.
 * Sur ce nouveau type de problÃ¨me, les auteurs proposent deux nouveaux benchmarks, ML-Benchmark et Open-ended task benchmark. Le premier est constituÃ© de problÃ¨mes de datascience classique de difficultÃ© variable : time series forecasting, classification, etc... tandis que le deuxiÃ¨me est plus ouvert et nÃ©cessite de manipuler des outils variÃ©s (browser, modÃ¨le dâ€™OCR, etc...).
 * Sur les deux benchmarks, Data Interpreter obtient des performances SOTA et complÃ¨te en moyenne 97% des tÃ¢ches du Open-ended task benchmark, une amÃ©lioration de 112% par rapport au projet OpenInterpreter.",,,,
14/03/2024,https://huggingface.co/spaces/opencompass/open_vlm_leaderboard,1 - High,Benchmark,,Erreur: Erreur lors de la récupération des données HuggingFace: 401,21/03/2024,#multimodal #leaderboard,,"* 41 LLM multimodaux y sont comparÃ©s sur datasets. Les taches couvertes sont variÃ©es : captioning, OCR, lecture de diagramme, comprÃ©hension de tableauâ€¦
 * Le leaderboard est disponible sur HuggingFace.
 * Un repository VLMEvalKit est aussi disponible pour Ã©valuer ses propres LLM multimodaux.",,,,
15/03/2024,https://www.anthropic.com/news/claude-3-haiku,1 - High,Model,Model,Processed,21/03/2024,#LLM #multimodal #anthropic #API,,"* Plus de 25 fois moins cher que GPT4-turbo en prenant le coÃ»t moyen par token.
 * OptimisÃ© pour avoir une latence extrÃªmement faible, au prix de performances moins Ã©levÃ©es que pour les autres LLM dâ€™Anthropic (Sonnet et Opus).
 * Dâ€™aprÃ¨s le benchmark multimodal vision dâ€™Anthropic, Haiku est aussi performant que GPT4-Vision.","1. **Introduction** : Le document annonce le lancement de Claude 3 Haiku, le modèle le plus rapide et le plus abordable de sa catégorie, conçu pour des applications d'entreprise variées.

2. **Points clés** :
   - Claude 3 Haiku est trois fois plus rapide que ses concurrents, traitant 21 000 tokens par seconde pour des requêtes inférieures à 32 000 tokens.
   - Le modèle est conçu pour analyser rapidement de grands volumes de documents, comme des cas juridiques ou des contrats, à moitié prix par rapport à d'autres modèles similaires.
   - Il met l'accent sur la sécurité et la robustesse, avec des mesures de protection des données et des audits réguliers pour identifier les vulnérabilités.
   - Disponible via l'API et pour les abonnés Claude Pro, Haiku sera également accessible sur Amazon Bedrock et bientôt sur Google Cloud Vertex AI.

3. **Concision** : Claude 3 Haiku, le modèle d'intelligence artificielle le plus rapide, est lancé pour des applications d'entreprise, offrant une analyse rapide et abordable des données tout en garantissant la sécurité des informations.",0.133,0.0107,0.0904
15/03/2024,https://arxiv.org/pdf/2401.15391.pdf,1 - High,Dataset,Dataset,Processed,20/03/2024,#RAG #Multihop,,"Dataset constituÃ© de questions multihop (oÃ¹ l'information nÃ©cessaire est contenue dans plusieurs segments) posÃ©es sur des articles de news en anglais et de ground truths sourcÃ©es. Les questions sont de 4 types : dâ€™infÃ©rence (identifier un nom ou une entitÃ© Ã partir de ses attributs ou dâ€™une description), comparaisons temporelles (comparaison entre deux Ã©vÃ¨nements : rÃ©ponses en avant/aprÃ¨s ou Ã©ventuellement boolÃ©ennes), comparaisons boolÃ©ennes et questions adversariales. 
 Ils font deux expÃ©riences sur une pipeline LlamaIndex :
  - Evaluation du retrieving : comparaison de modÃ¨les dâ€™embedding en top-K retrieving avec ou sans reranker, sur diffÃ©rentes mÃ©triques (MMR, MAP, HIT). Tous les scores sont assez bas : le meilleur embedding obtient 0.66 en Hits@4 avec reranking.
  - Evaluation de la partie gÃ©nÃ©ration de rÃ©ponse en supposant que lâ€™Ã©tape de retrieving a remontÃ© toute lâ€™information nÃ©cessaire : ils prÃ©sentent au LLM les sources utilisÃ©es pour Ã©crire la ground truth. GPT-4 est performant mais les modÃ¨les open-source Llama-70B, Mixtral-8x7B, ont respectivement 0.32 et 0.36 dâ€™accuracy.
 MÃªme sur des requÃªtes multihop relativement simples, faire du top-K retrieving mÃªme avec un reranker donne donc de mauvais rÃ©sultats et mÃªme en prÃ©sentant au modÃ¨le des segments avec toute lâ€™information nÃ©cessaire, les gros modÃ¨les open-sources ont de (trÃ¨s) mauvaises performances en gÃ©nÃ©ration.
 En ce qui concerne le dataset, les types de multihop considÃ©rÃ©s sont encore trop simples par rapport aux cas rÃ©els. Toutes les questions sont boolÃ©ennes ou ont pour rÃ©ponse, non pas une phrase, mais une entitÃ© bien dÃ©finie. Ils excluent aussi les questions adversariales pour l'Ã©valuation du retrieving, donc n'Ã©valuent pas la capacitÃ© Ã retriever des informations utiles et connexes.
 Licence : odc-by","### 1. Introduction
Le document présente MultiHop-RAG, un nouveau jeu de données conçu pour évaluer les systèmes de génération augmentée par récupération (RAG) face à des requêtes multi-hop, qui nécessitent la récupération et le raisonnement sur plusieurs pièces d'évidence. Les systèmes RAG actuels montrent des lacunes dans le traitement de ce type de requêtes.

### 2. Points clés
- **Dataset** : MultiHop-RAG comprend une base de connaissances, des requêtes multi-hop, des réponses exactes et des preuves associées.
- **Méthode** : Utilisation de GPT-4 pour générer des requêtes et des claims à partir d'articles d'actualités.
- **Évaluation** : Deux expériences ont été menées : l'évaluation des modèles d'embedding pour la récupération d'évidence et l'évaluation des capacités de raisonnement de modèles LLM tels que GPT-4, PaLM et Llama2-70B.
- **Résultats** : Les systèmes RAG existants ont montré des performances insatisfaisantes dans la récupération et la réponse aux requêtes multi-hop.

### 3. Concision
MultiHop-RAG est un jeu de données novateur pour évaluer les systèmes RAG sur des requêtes complexes nécessitant plusieurs preuves. Il a été construit en utilisant GPT-4 et des articles d'actualités, révélant que les méthodes RAG actuelles sont insuffisantes pour traiter efficacement ces requêtes.",0.4549,0.0823,0.1721
17/03/2024,https://arxiv.org/pdf/2403.09611.pdf,1 - High,Model,Model,Processed,18/03/2024,#mutltimodal,,"- Un des premiers papiers, publiÃ© par des chercheurs de chez Apple, qui offre un comparatif sÃ©rieux sur les choix d'architecture de modÃ¨le, d'entraÃ®nement et de data mixture pour les LLM multimodaux.
 - Un plus long pre-training et un support pour de grandes images au pre-training et au SFT participent a augmenter les performances du modÃ¨le. A contrario, le choix du connecteur vision-langage semble Ãªtre nÃ©gligeable (Ã prendre avec des pincettes car seuls 3 connecteurs ont Ã©tÃ© testÃ©s).
 - La data mixture a un impact significatif sur les capacitÃ©s du modÃ¨le. Les auteurs recommendent un ratio caption/interleaved/text only ratio de 5:5:1. Avoir de l'interleaved amÃ©liore drastiquement le few-shot. Exclure les exemples â€œtext onlyâ€ fait que le modÃ¨le oublie ses capacitÃ©s de raisonnement et rÃ©duit les performances en few-shot.
 - Leur modÃ¨le MM1 n'est pas open-sourcÃ©, mais tous les datasets d'images utilisÃ©s pour le pre-training et le SFT sont dÃ©taillÃ©s et issus de la communautÃ© open-source.","1. **Introduction :** Ce document présente les méthodes et les analyses liées à la pré-formation de modèles de langage multimodaux (MLLM) performants, en se concentrant sur l'architecture, les choix de données et leurs impacts sur les performances des modèles.

2. **Points clés :**
   - Étude de l'importance des composants architecturaux et des choix de données pour la pré-formation des MLLM.
   - Identification de tendances clés, notamment l'impact de la résolution des images et des types de données utilisés (images avec légendes, documents image-texte entrelacés, données uniquement textuelles).
   - Développement de MM1, une famille de modèles multimodaux, atteignant des performances de pointe (SOTA) après une pré-formation à grande échelle et un ajustement supervisé.
   - Résultats compétitifs sur plusieurs benchmarks multimodaux, avec des capacités d'apprentissage contextuel et de raisonnement multi-image.

3. **Concision :** Le document explore la construction de MLLM performants, mettant en évidence l'impact des choix architecturaux et de données. MM1, le modèle développé, atteint des résultats SOTA grâce à une pré-formation soignée et un ajustement supervisé, démontrant des capacités avancées en apprentissage contextuel et raisonnement.",0.3495,0.0829,0.1553
17/03/2024,https://x.ai/blog/grok-os,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 403,21/03/2024,#OpenLLM #xAI,,"xAI a open-sourcÃ© les poids et architecture de Grok-1.
 314B de paramÃ¨tres, Mixture of 8 Experts, 2 experts par token. Utilise rotary position embeddings (RoPE). Taille de contexte 8192 tokens.
 Licence Apache-2.0",,,,
19/03/2024,https://contextual.ai/introducing-rag2/,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 403,27/03/2024,"RAG, E2E",,"Contextual AI introduisent des nouveaux Contextual Language Models qui unifie la pipeline classique de RAG (retriever+gÃ©nÃ©ration de synthÃ¨se) en un seul modÃ¨le:
 - Sur plusieurs benchmarks acadÃ©miques (Natural Questions, TriviaQA, HotPotQA, TruthfulQA, HaluEvalQA et FreshQA), RAG 2.0 est bien meilleur que GPT-4 en few-shot (entre 3 et 9 points de plus). Le modÃ¨le est particuliÃ¨rement performant sur sur des donnÃ©es rÃ©elles, comme ils le montrent sur des donne2es financiÃ¨res.
 - RAG 2.0 fonctionne bien sur des longs contextes battant GPT-4 jusqu'a 32k de contexte et Mixtral sur jusqu'Ã de 2M de tokens de contexte
 - Il faut rejoindre une waitlist pour pouvoir tester le modÃ¨le",,,,
19/03/2024,https://twitter.com/tianjun_zhang/status/1768706092211826966,1 - High,Method,,Erreur: Erreur lors de la récupération de l'article de blog: 400,27/03/2024,RAFT,,"Papier qui prÃ©sente la mÃ©thode RAFT:
 - la mÃ©thode consiste Ã finetuner un modÃ¨le pour rÃ©pondre Ã une question en mode chain-of-thought Ã partir de k documents donnÃ©es en entrÃ©e dont certains sont des ""distractors"" qui ne rÃ©pondent pas Ã la requÃªte
 - Les auteurs comparent RAFT Ã diffÃ©rentes baselines qui montrent que le finetuning sur des rÃ©ponses Chain-of-thought augmente bien les performances
 Ce papier nous rassure sur la mÃ©thode qu'on utilise pour les modÃ¨les intÃ©grÃ©s dans Search car elle est trÃ¨s proche de ce que nous utilisons.",,,,
20/03/2024,https://twitter.com/yanndubs/status/1770284212937207840,1 - High,Benchmark,,Erreur: Erreur lors de la récupération de l'article de blog: 400,21/03/2024,#Eval #OutputLength,,"Le leaderboard de chatLLMs AlpacaEval est dÃ©sormais normalisÃ© par dÃ©faut sur la taille des outputs.
 
 Meilleure corrÃ©lation avec le LMSys Chat Arena que MT-Bench (0.98 contre 0.94 ; 0.93 sans contrÃ´le de longueur).
 
 Le contrÃ´le sur la longueur corrige le biais de longueur de GPT-4 mais les modÃ¨les finetunÃ©s sur de outputs de GPT-4 sont probablement encore avantagÃ©s.
 
 LCAE est plus robuste Ã la verbositÃ© lorsque lâ€™on fait varier le prompt (le ranking reste le mÃªme que lâ€™on demande on rÃ©ponse concise/standard/verbeuse).
 
 [Leaderboard](https://tatsu-lab.github.io/alpaca_eval/), [Github](https://github.com/tatsu-lab/alpaca_eval).",,,,
21/03/2024,https://huggingface.co/blog/cosmopedia,1 - High,Dataset,,Erreur: Erreur lors de la récupération des données HuggingFace: 401,21/03/2024,#SyntheticData,,"Release de Cosmopedia, dataset open-source dâ€™une taille de 25 milliards de tokens de textbooks, blog posts et articles WikiHow gÃ©nÃ©rÃ©s synthÃ©tiquement par Mixtral-8x7B, dans le but notamment de pretrain des LLMs sur donnÃ©es synthÃ©tiques.
 Toute la stack, les prompts et le code pour gÃ©nÃ©rer le dataset est open-sourcÃ© :
  - GÃ©nÃ©ration avec Mixtral de 30 millions de prompts dâ€™instructions gÃ©nÃ©rÃ©s majoritairement en demandant dâ€™Ã©crire un blog post/textbook/leÃ§on/etc. Ã partir de datasets existants scrapÃ©s du web (RefinedWeb, OpenHermes) et en promptant Mixtral de faÃ§on un peu fine pour faire varier lâ€™auditoire/le style.
  - Ils gÃ©nÃ¨rent 25 milliards de tokens de textbooks synthÃ©tiques par Mixtral avec llm-swarm. Le dataset obtenu couvre une grande variÃ©tÃ© de topics avec moins de 1% de doublons.
 Ils entraÃ®nent un modÃ¨le 1B sur Cosmopedia et obtiennent de meilleurs performances que TinyLlama 1.1B sur ARC, OpenBookQA et MMLU et comparables Ã Qwen-1.5-1B sur Arc-challenge et OpenBookQA.",,,,
21/03/2024,https://twitter.com/natolambert/status/1770488846360428782,1 - High,Benchmark,,Erreur: Erreur lors de la récupération de l'article de blog: 400,21/03/2024,#RewardModels,,"- Benchmark de reward models de AllenAI.
 - Collection de triplets (prompt-bonne rÃ©ponse-mauvaise rÃ©ponse) sur plusieurs domaines (chat, chat oÃ¹ il est difficile de trancher entre les deux rÃ©ponses, raisonnement et code, safety). Les paires prompts-rÃ©ponses sont essentiellement collectÃ©es Ã partir dâ€™AlpacaEval et MT Bench pour le chat, et XSTest et Do-Not-Answer pour la safety et HumanEvalPack pour le raisonnement et le code.
 - Lâ€™Ã©valuation se fait simplement en vÃ©rifiant que le reward model assigne un score plus Ã©levÃ© Ã la bonne rÃ©ponse quâ€™Ã la mauvaise rÃ©ponse.
 - Les modÃ¨les qui obtiennent les meilleurs scores en moyenne sont Starling-RM-7B, zephyr-7b, et Nous-Hermes-2-Mistral-7B-DPO.
 Licence : ODC-By",,,,
25/01/2024,https://x.com/aparnadhinak/status/1748368364395721128?t=iYDFnzb_VJZGPmJFjbDGRg&amp;s=31https://x.com/aparnadhinak/status/1748368364395721128?t=iYDFnzb_VJZGPmJFjbDGRg&amp;s=31,1 - High,Method,,Erreur: Erreur lors de la récupération de l'article de blog: 400,30/01/2024,Eval,,Utiliser des classes plutÃ´t qu'un score numÃ©rique permet de rÃ©duire les biais de modÃ¨les Ã©valuateurs (e.g. GPT 4),,,,
21/03/2024,https://github.com/pytorch/torchtune,1 - High,Library,Library,Processed,21/03/2024,#Finetuning #Pytorch,,"Librairie native Pytorch trÃ¨s complÃ¨te pour tester et finetuner des LLM (full finetune, LoRA), avec FSDP.
 IntÃ©gration Ã Hugging Face datasets pour l'entraÃ®nement et Ã EvalHarness pour l'Ã©valuation.
 Support des checkpoints Huggingface.","### Introduction
Le document présente torchtune, une bibliothèque PyTorch destinée à l'entraînement et à l'expérimentation avec des modèles de langage de grande taille (LLM). Il met en avant ses fonctionnalités, ses mises à jour récentes, et les méthodes de post-entraînement disponibles.

### Points clés
- **Mises à jour récentes** : Support pour des modèles comme Llama 3.3, ajout de nouvelles fonctionnalités et améliorations de performance.
- **Méthodes de post-entraînement** : Inclut la finetuning supervisée (SFT), la distillation de connaissances (KD), et l'apprentissage par renforcement (RLHF).
- **Modèles supportés** : Llama, Gemma, Mistral, Phi, Qwen, etc.
- **Efficacité mémoire et vitesse d'entraînement** : Optimisations pour réduire l'utilisation mémoire et augmenter la vitesse d'entraînement.
- **Installation et démarrage** : Instructions claires pour installer la bibliothèque et commencer à l'utiliser.

### Concision
Torchtune est une bibliothèque PyTorch pour l'entraînement de LLM, offrant des méthodes variées de post-entraînement et supportant plusieurs modèles. Les mises à jour récentes améliorent la performance et l'efficacité, avec des instructions claires pour l'installation et l'utilisation.",0.0863,0.0255,0.0711
22/03/2024,https://github.com/mshumer/gpt-prompt-engineer,1 - High,Library,Library,Processed,27/03/2024,#PromptEngineering,,"- Automatise le prompt engineering. Les prompts sont testÃ©s et classÃ©s avec un ELO score. Il y a un preset pour supporter des problÃ¨mes de classification.
 - Permet une distillation des capacitÃ©s de rÃ©flexion de Claude Opus (cher et lent) vers Claude Haiku (Ã©conomique et rapide) via du few-shot learning.","1. **Introduction** : Ce document présente `gpt-prompt-engineer`, un outil dédié à l'ingénierie des prompts, permettant d'expérimenter et d'optimiser des requêtes pour des modèles d'IA comme GPT-4 et Claude 3 Opus.

2. **Points clés** :
   - Génération de prompts variés en fonction d'une description de tâche et de cas de test.
   - Évaluation des performances des prompts avec un système de notation ELO.
   - Versions spécifiques pour la classification et l'utilisation du modèle Claude 3 Opus.
   - Auto-génération de cas de test et possibilité de définir des variables d'entrée.
   - Intégration optionnelle avec Weights & Biases pour le suivi des configurations.

3. **Concision** : `gpt-prompt-engineer` facilite la création et l'évaluation de prompts pour des tâches d'IA, offrant des fonctionnalités avancées pour optimiser les résultats tout en réduisant les coûts et la latence.",0.179,0.0124,0.0988
24/03/2024,https://github.com/mistralai-sf24/hackathon,1 - High,Model,Model,Processed,26/03/2024,#Mistralv0.2,,"Lors d'un hackathon Ã San Francisco, Mistral a release le base modÃ¨le Ã l'origine du modÃ¨le Instruct v0.2.
 - Ils ont apportÃ© plusieurs updates au modÃ¨le: une fenÃªtre de contexte de 32k tokens, un rope theta de 1e6 et pas de sliding window
 - Ils auraiaient oubliÃ© de le livrer au mÃªme temps que le Instruct.","1. **Introduction** : Ce document présente le dépôt du modèle Mistral 7B, qui contient le code minimal pour exécuter et affiner ce modèle d'intelligence artificielle. Il fournit également des instructions pour l'installation et l'utilisation du modèle.

2. **Points clés** :
   - Téléchargement du modèle via un lien direct.
   - Installation des dépendances nécessaires à l'aide d'un conteneur Docker.
   - Instructions pour déployer le modèle avec vLLM et utiliser la bibliothèque transformers.
   - Détails sur le format de données requis pour l'affinage, incluant des exemples de fichiers JSONL pour la préformation et l'instruction.
   - Recommandation d'utiliser LoRA pour un affinage efficace et économe en mémoire, avec des instructions pour exécuter l'affinage sur un GPU.

3. **Concision** : Ce dépôt fournit les outils nécessaires pour télécharger, installer et affiner le modèle Mistral 7B, en utilisant des techniques modernes comme LoRA pour l'efficacité.",0.1369,0.0299,0.1071
25/01/2024,https://huggingface.co/papers/2312.15685,1 - High,Method,,Erreur: Erreur lors de la récupération des données HuggingFace: 401,30/01/2024,SÃ©lection de donnÃ©es pour instruction finetuning,,"Les auteurs proposent des mÃ©thodes pour sÃ©lectionner des exemples de SFT parmi un dataset d'exemples de qualitÃ© diverse selon leur qualitÃ©, la diversitÃ© et la complexitÃ©. Ils finetunent des Llama pour scorer ces samples afin de mesurer la qualitÃ© et la complexitÃ©. La diversitÃ© est mesurÃ©e en regardant la distance entre les embeddings. Leur mÃ©thode globale de sÃ©lection finale est basÃ©e sur un score s = complexitÃ© * qualitÃ© pour filtrer les samples en contraignant une certaine diversitÃ©. La mÃ©thode est un peu complexe mais les rÃ©sultats sont plutÃ´t bons.",,,,
25/01/2024,https://twitter.com/maximelabonne/status/1746317053143773628,1 - High,Method,,Erreur: Erreur lors de la récupération de l'article de blog: 400,30/01/2024,Eval,,"Matrice de corrÃ©lation entre diffÃ©rents benchmark, y compris le Elo score de l'arÃ¨ne LMSys (annotation humaines crowdsourcÃ©es). MT-Bench, AGI Eval, ARC-C, et MMLU sont de bons prÃ©dicteurs du jugement humain
 TruthfulQA et HellaSwag sont mal corrÃ©lÃ©s au jugement humain. AGI Eval est un trÃ¨s bon compromis entre coÃ»t d'infÃ©rence et corrÃ©lation au jugement humain.",,,,
27/03/2024,https://x.com/lmsysorg/status/1772759835714728217?t=GKXMABIffPxpBgX7UzEuBQ&amp;s=31,1 - High,Benchmark,,Erreur: Erreur lors de la récupération de l'article de blog: 400,27/03/2024,#Claude #LMSysArena,,"- Claude 3 Opus est premier du leaderboard devant GPT-4 preview
 - Claude 3 Haiku (modele ""low-cost"") est devant GPT-4-0613 et Mistral Large
 - Ce dernier point est trÃ¨s intÃ©ressant notamment par rapport au coÃ»t pour nos usages internes.",,,,
27/03/2024,https://huggingface.co/blog/embedding-quantization,1 - High,Method,,Erreur: Erreur lors de la récupération des données HuggingFace: 401,27/03/2024,#RAG #embeddings #quantization,,"- Les auteurs proposent un retrieval en 2 Ã©tapes suivant la logique de reranking. Lâ€™apport se fait dans la 1Ã¨re Ã©tape oÃ¹ les embeddings ont Ã©tÃ© quantifiÃ©s. Ils ont observÃ© une accÃ©lÃ©ration de x32 et un gain de mÃ©moire de x32 Ã©galement, tout en prÃ©servant 96% des performances originales.
 - Les auteurs ont aussi expÃ©rimentÃ© avec une quantification en int8.
 - La fonctionnalitÃ© est disponible dans la [v2.6.0 de sentence-transformers](https://github.com/UKPLab/sentence-transformers/releases#:~:text=Compare-,v2.6.0%20%2D%20Embedding%20Quantization%2C%20GISTEmbedLoss,-This%20release%20brings).",,,,
25/01/2024,https://unbabel.com/announcing-tower-an-open-multilingual-llm-for-translation-related-tasks/?utm_campaign=Tower%20Announcement&utm_content=278163605&utm_2 - Medium=social&utm_source=twitter&hss_channel=tw-2181959904,1 - High,Model,,Erreur: Erreur lors de la récupération de l'article de blog: 403,30/01/2024,ModÃ¨les de traduction multilangues,,Unbabel a release un modÃ¨le Llama 7B finetunÃ© pour de la traduction qui permet d'ajouter des instructions pour orienter la traduction,,,,
27/03/2024,https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm,1 - High,Model,Model,Processed,27/03/2024,#Databricks #LLM #MoE,,"ModÃ¨le de Databricks meilleur que Mixtral mais bien plus gros (132B total parameters of which 36B parameters are active) et entraine sur 12T tokens: 
 - Le modÃ¨le a 132B de paramÃ¨tres sous la forme d'une mixture d'experts avec 16 experts dont 4 sont activÃ©s Ã l'infÃ©rence soit 36B de paramÃ¨tres actifs.
 - Le modÃ¨le a Ã©tÃ© entrainÃ© sur 12T de tokens de texte, en grande partie e anglais, et code. Le modÃ¨le n'a pas Ã©tÃ© testÃ© sur d'autres langues.
 - Plusieurs benchmarks ont Ã©tÃ© fait en comparaison avec des modÃ¨les open-source et en comparaison avec des API fermÃ©es. DBRX Instruct est meilleur que Mixtral Instruct et GPT-3.5 sur la plupart des benchmarks, de quelques points sur l'Open LLM Benchamrk, de 15 points sur HumanEval et 6 points sur le benchmark Gauntlet (compilation e taches de NLU faite par Databricks). Il est tout de mÃªme moins bon que Claude 3 Haiku.
 - License permettant l'usage commerciale non contraint en-dessous de 700 million d'utilisateurs actifs. Ils ne permettent pas d'utiliser DBRX pour amÃ©liorer d'autres LLMs.","**Introduction :**  
Le document présente DBRX, un modèle de langage de pointe développé par Databricks, qui se distingue par ses performances sur divers benchmarks par rapport à d'autres modèles ouverts et fermés.

**Points clés :**  
- DBRX utilise une architecture de mélange d'experts (MoE) avec 132 milliards de paramètres, dont 36 milliards actifs.  
- Il a été pré-entraîné sur 12 trillions de tokens et excelle dans des tâches de programmation et de raisonnement mathématique.  
- DBRX surpasse GPT-3.5 et rivalise avec Gemini 1.0 Pro sur plusieurs benchmarks.  
- L'efficacité d'entraînement est améliorée, nécessitant jusqu'à 4 fois moins de puissance de calcul par rapport aux modèles précédents.  
- Le modèle est accessible via des API et peut être déployé sur la plateforme Databricks.  

**Concision :**  
DBRX est un modèle de langage performant de Databricks, surpassant plusieurs modèles concurrents grâce à son architecture MoE et à son efficacité d'entraînement.",0.4734,0.119,0.2189
28/03/2024,https://blog.research.google/2024/03/chain-of-table-evolving-tables-in.html,1 - High,Method,Method,Processed,4/4/2024,"CoT, TableQA",,"MÃ©thode par Google qui gÃ©nÃ©ralise le Chain-of-Thought sur des tableaux. 
 - Les auteurs proposent une chaÃ®ne de raisonnement sur un tableau, Ã partir dâ€™une requÃªte utilisateur qui peut Ãªtre complexe.
 - A chaque itÃ©ration, un LLM va choisir dans un pool dâ€™opÃ©ration, la prochaine opÃ©ration a effectuer en fonction de la question, du tableau et de lâ€™historique du raisonnement. Cette opÃ©ration peut mener Ã une modification du tableau, jusquâ€™Ã ce que celui ci puisse rÃ©pondre directement Ã la question de l'utilisateur.
 - Avec cette mÃ©thode, les auteurs montrent une amÃ©lioration des capacitÃ©s de raisonnement sur diffÃ©rents dataset de TableQA.","1. **Introduction** : Ce document présente le projet ""Chain-of-Table"", qui vise à améliorer la compréhension des tables par les modèles de langage à grande échelle (LLM) en leur permettant de raisonner de manière itérative sur des données tabulaires.

2. **Points clés** :
   - **Méthode** : Les LLM sont entraînés à générer des opérations tabulaires qui transforment progressivement une table pour répondre à une question spécifique.
   - **Modèles utilisés** : PaLM 2-S et GPT 3.5.
   - **Résultats** : Amélioration significative des performances sur les benchmarks WikiTQ, TabFact et FeTaQA, surpassant les méthodes de raisonnement génériques et assistées par programme.
   - **Robustesse** : Chain-of-Table montre une meilleure robustesse face à des questions complexes et à des tables de grande taille.

3. **Concision** : En somme, Chain-of-Table optimise le raisonnement tabulaire des LLM en décomposant les questions en opérations simples, améliorant ainsi la précision et la compréhension des résultats.",0.2994,0.0723,0.1377
28/03/2024,https://www.ai21.com/blog/announcing-jamba,1 - High,Model,Model,Processed,4/4/2024,"LLM, Open Source",,"- Jamba est le premier modÃ¨le basÃ© sur Mamba Ã atteindre des performances proches de Mixtral 8x7B. La principale nouveautÃ© de ce modÃ¨le est l'utilisation dâ€™une architecture hybride mixant blocs transformers et Mamba.
 - Avec une fenÃªtre de contexte de 256k, Jamba offre un dÃ©bit 3x plus important que Mistral 8x7B sur les longs contextes
 - Le modÃ¨le est disponible sur HuggingFace sous licence Apache 2.0","1. **Introduction** : Le document présente Jamba, un modèle innovant d'IA développé par AI21, qui combine la technologie Mamba avec l'architecture Transformer pour résoudre des tâches complexes tout en améliorant l'efficacité et le débit.

2. **Points clés** :
   - Jamba est le premier modèle de production basé sur une architecture hybride SSM-Transformer.
   - Offre un contexte de 256K, avec un débit trois fois supérieur à celui des modèles comparables.
   - Utilise des couches de mélange d'experts (MoE) pour optimiser les performances et la mémoire.
   - Disponible avec des poids ouverts sous licence Apache 2.0 et accessible via l'API NVIDIA.
   - Destiné à être utilisé comme base pour le développement de solutions personnalisées.

3. **Concision** : Jamba représente une avancée majeure dans le développement des modèles de langage, en combinant les forces des architectures Mamba et Transformer pour offrir une efficacité et une capacité inégalées.",0.2883,0.1173,0.1534
