import streamlit as st
import pandas as pd
import processing.data_extractor as d_ex
import processing.data_classifier as d_c
from summarization.summarizerAgent import generate_summary, criticize_summary, classify_document
from summarization.evaluate_rouge import evaluate_summary_with_original
from datetime import datetime, date,timedelta
import time
import requests
from bs4 import BeautifulSoup
import re
from scraping.arxiv_automated_scrapping.arxiv_latest_scrapping import (
    get_latest_arxiv_articles,
    filter_articles_by_keywords_multi
)
from scraping.reddit_scraping import get_entries_from_reddit
from scraping.huggingface_automatic_scraping import get_entries_from_huggingface

# Configuration de la page
st.set_page_config(
    page_title="Newsletter Generator",
    page_icon="todo",
    layout="wide"
)

# Initialiser les variables de session si elles n'existent pas
if 'page' not in st.session_state:
    st.session_state.page = 'home'
if 'arxiv_articles' not in st.session_state:
    st.session_state.arxiv_articles = None
if 'selected_article' not in st.session_state:
    st.session_state.selected_article = None
if 'selected_url' not in st.session_state:
    st.session_state.selected_url = None

#-------------------- FONCTIONS UTILITAIRES --------------------#

def format_date(date_str):
    """Formate la date en format lisible"""
    if not date_str:
        return "Date inconnue"
    try:
        date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        return date.strftime("%d/%m/%Y")
    except:
        return date_str

#-------------------- NAVIGATION --------------------#

def navigate_to(page_name):
    """Fonction centralis√©e de navigation"""
    st.session_state.page = page_name
    st.rerun()

def reset_and_go_home():
    """R√©initialise toutes les donn√©es en m√©moire et retourne √† l'accueil"""
    # Variables √† conserver (aucune dans ce cas)
    page_only = {'page': 'home'}
    
    # R√©initialiser toute la session
    for key in list(st.session_state.keys()):
        if key not in page_only:
            del st.session_state[key]
    
    # Aller √† l'accueil
    st.session_state.page = 'home'
    st.rerun()

#-------------------- EXTRACTION DE DONN√âES --------------------#

def extract_data_from_url(url_or_id):
    """
    Extrait les donn√©es d'une URL ou d'un identifiant
    
    Args:
        url_or_id: URL ou identifiant √† traiter
        
    Returns:
        tuple: (donn√©es extraites, cat√©gorie d√©tect√©e, succ√®s de l'extraction)
    """
    try:
        with st.spinner("Extraction en cours..."):
            extractor = d_ex.DataExtractor()
            data = extractor.extract(url_or_id).to_dict()
            
            # Classifier le contenu
            category, (input_tok, output_tok) = classify_document(data)
            
            # Marquer la source et harmoniser la structure de donn√©es
            data["source"] = "url"
            
            return data, category, True
    except Exception as e:
        return None, None, f"Erreur d'extraction : {e}"

def extract_arxiv_paper_content(arxiv_id):
    """
    Extrait le contenu complet d'un article ArXiv par son identifiant
    
    Args:
        arxiv_id: ID ou URL de l'article ArXiv
        
    Returns:
        dict: Les donn√©es extraites de l'article ou None en cas d'erreur
    """
    try:
        # Extraire l'ID ArXiv de l'URL si n√©cessaire
        match = re.search(r'abs/([^/]+)$', arxiv_id)
        if match:
            arxiv_id = match.group(1)
        
        # Utiliser l'extracteur de donn√©es existant
        url = f"https://arxiv.org/abs/{arxiv_id}"
        return extract_data_from_url(url)[0]
    except Exception as e:
        st.error(f"Erreur lors de l'extraction du contenu ArXiv: {e}")
        return None

def prepare_arxiv_article_data(article, full_extraction=True):
    """
    Pr√©pare les donn√©es d'un article ArXiv pour le traitement
    
    Args:
        article: Dictionnaire des m√©tadonn√©es d'un article ArXiv
        full_extraction: Si True, tente d'extraire le contenu complet
        
    Returns:
        tuple: (donn√©es pr√©par√©es, cat√©gorie, succ√®s de l'op√©ration)
    """
    with st.spinner("Pr√©paration des donn√©es de l'article..."):
        try:
            data = None
            
            # Tenter l'extraction compl√®te si demand√©e
            if full_extraction:
                try:
                    st.info("Extraction du contenu complet de l'article...")
                    data = extract_arxiv_paper_content(article["link"])
                    
                    # Ajouter les m√©tadonn√©es manquantes √† partir des m√©tadonn√©es ArXiv
                    if data:
                        if "title" not in data or not data["title"]:
                            data["title"] = article["title"]
                        
                        if "authors" not in data or not data["authors"]:
                            data["authors"] = article["authors"]
                except Exception as e:
                    st.warning(f"Erreur lors de l'extraction compl√®te: {e}")
                    data = None
            
            # Si l'extraction compl√®te a √©chou√© ou n'a pas √©t√© demand√©e, utiliser les m√©tadonn√©es de base
            if not data:
                if full_extraction:
                    st.warning("Utilisation des m√©tadonn√©es de base car l'extraction compl√®te a √©chou√©.")
                
                data = {
                    "title": article["title"],
                    "authors": article["authors"],
                    "date": article["published_date"],
                    "url": article["link"],
                    "content": article["summary"],
                    "category": article["category"]
                }
            
            # Marquer la source et standardiser le format
            data["source"] = "arxiv"
            
            # Pour les articles ArXiv, la cat√©gorie est toujours "research paper"
            category = "research paper"
            
            return data, category, True
            
        except Exception as e:
            st.error(f"Erreur lors de la pr√©paration des donn√©es: {e}")
            return None, None, f"Erreur: {str(e)}"

#-------------------- AFFICHAGE DES ARTICLES --------------------#

def display_arxiv_articles(articles, title="Articles"):
    """
    Affiche les articles ArXiv sous forme de tableau interactif
    """
    if not articles:
        st.warning("Aucun article trouv√©.")
        return
    
    st.subheader(title)
    
    # Cr√©er un DataFrame pour l'affichage
    df = pd.DataFrame([{
        "Index": idx,
        "Titre": article["title"],
        "Auteurs": ", ".join(article["authors"]),
        "Date": format_date(article["published_date"]),
        "Cat√©gorie": article["category"],
        "Toutes cat√©gories": ", ".join(article.get("all_categories", [article["category"]])),
        "R√©sum√©": article["summary"][:150] + "..." if len(article["summary"]) > 150 else article["summary"],
        "Lien": article["link"]
    } for idx, article in enumerate(articles)])
    
    # Afficher le tableau
    st.dataframe(df, use_container_width=True)

def display_article_details(article):
    """
    Affiche les d√©tails d'un article sp√©cifique dans un expander
    """
    with st.expander(f"{article['title']}", expanded=True):
        st.markdown(f"**Auteurs:** {', '.join(article['authors'])}")
        st.markdown(f"**Date:** {format_date(article['published_date'])}")
        st.markdown(f"**Cat√©gorie principale:** {article['category']}")
        if "all_categories" in article:
            st.markdown(f"**Toutes les cat√©gories:** {', '.join(article['all_categories'])}")
        st.markdown(f"**Lien:** [{article['link']}]({article['link']})")
        st.markdown("**R√©sum√©:**")
        st.markdown(article['summary'])

#-------------------- TRAITEMENT DES DONN√âES --------------------#

def save_extracted_data(data, category):
    """
    Enregistre les donn√©es extraites dans la session
    et redirige vers la page de traitement
    """
    st.session_state.extracted_data = data
    st.session_state.extracted_category = category
    navigate_to('process_data')

def process_extraction(data, category, success, source_page="home"):
    """
    Traite le r√©sultat d'une extraction et d√©cide de la suite
    
    Args:
        data: Donn√©es extraites
        category: Cat√©gorie d√©tect√©e
        success: R√©sultat de l'extraction (True/False ou message d'erreur)
        source_page: Page d'origine pour retour en arri√®re
    """
    if success is True:
        # Extraction r√©ussie
        save_extracted_data(data, category)
    else:
        # √âchec de l'extraction
        st.error(f"Erreur lors de l'extraction: {success}")

def generate_content_summary():
    """
    G√©n√®re un r√©sum√© du contenu extrait
    """
    try:
        with st.spinner("R√©sum√© en cours..."):
            summary, (input_tok, output_tok) = generate_summary(st.session_state.extracted_data)
            st.session_state.summary = summary
            st.session_state.summary_price = float(input_tok)*0.150/1e6 + float(output_tok)*0.6/1e6
            return True
    except Exception as e:
        st.warning(f"Erreur lors du r√©sum√© : {e}")
        return False

def evaluate_summary_with_critique():
    """
    G√©n√®re une critique du r√©sum√©
    """
    try:
        with st.spinner("Critique en cours..."):
            review = criticize_summary(st.session_state.extracted_data, st.session_state.summary)
            st.session_state.review = review
            return True
    except Exception as e:
        st.error(f"Erreur lors de la critique: {e}")
        return False

def calculate_rouge_metrics():
    """
    Calcule les m√©triques ROUGE pour le r√©sum√©
    """
    try:
        with st.spinner("Calcul des m√©triques Rouge..."):
            original_content = st.session_state.extracted_data.get("content", "")
            rouge_scores = evaluate_summary_with_original(st.session_state.summary, original_content)
            st.session_state.rouge_scores = rouge_scores
            return True
    except Exception as e:
        st.error(f"Erreur lors de l'√©valuation Rouge : {e}")
        return False

#-------------------- PAGES DE L'INTERFACE --------------------#

def show_home_page():
    """Page d'accueil avec options de navigation"""
    st.title("G√©n√©rateur de Newsletter Automatique :wave:")
    st.write("""
    Bienvenue sur cette application de d√©monstration. Veuillez choisir une option:
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### Extraction par URL")
        st.write("Entrez l'URL ou l'identifiant d'une ressource pour l'extraire et la r√©sumer.")
        if st.button("Extraction par URL üîç", key="btn_url", use_container_width=True):
            navigate_to('regular_extraction')
            
    with col2:
        st.markdown("### Recherche sur ArXiv")
        st.write("Recherchez et filtrez des articles r√©cents sur ArXiv pour les r√©sumer.")
        if st.button("Recherche ArXiv üìö", key="btn_arxiv", use_container_width=True):
            navigate_to('arxiv_scraper')

    col3, col4 = st.columns(2)

    with col3:
        st.markdown("### Recherche Automatique sur HuggingFace")
        st.write("Extrait les derniers articles en vogue de HuggingFace pour les r√©sumer.")
        if st.button("Auto-Extraction HuggingFace ü§ñ", key="btn_huggingface", use_container_width=True):
            navigate_to('huggingface_scraper')

    with col4:
        st.markdown("### Recherche Automatique sur Reddit")
        st.write("Extrait les derniers articles en vogue de Reddit pour les r√©sumer.")
        if st.button("Auto-Extraction Reddit ü§ñ", key="btn_reddit", use_container_width=True):
            navigate_to('reddit_scraper')

def show_regular_extraction_page():
    """Page d'extraction par URL"""
    st.title("Extraction par URL :mag:")
    
    # Bouton de retour
    if st.button("‚Üê Retour √† l'accueil", key="home_from_regular"):
        navigate_to('home')

    # Entr√©e utilisateur
    if not st.session_state.selected_url:
        url_or_id = st.text_input("Entrez l'URL ou l'identifiant de la ressource :", placeholder="Exemple : https://arxiv.org/abs/1234.56789")

        # Bouton pour lancer l'extraction
        if st.button("Extraire les donn√©es üöÄ", key='extract_button'):
            if url_or_id.strip() == "":
                st.warning("Veuillez entrer un lien ou un identifiant valide.")
            else:
                data, category, success = extract_data_from_url(url_or_id)
                process_extraction(data, category, success, 'regular_extraction')
    
    else:
        data, category, success = extract_data_from_url(st.session_state.selected_url)
        st.session_state.selected_url = None
        process_extraction(data, category, success, 'regular_extraction')

def show_arxiv_scraper_page():
    """Page du scrapper ArXiv"""
    st.title("Recherche sur ArXiv üìö")
    
    # Bouton de retour
    if st.button("‚Üê Retour √† l'accueil", key="home_from_arxiv"):
        navigate_to('home')

    st.markdown("""
    Recherchez et filtrez les articles r√©cents d'ArXiv.
    """)
    
    # Sidebar pour les param√®tres
    with st.sidebar:
        st.header("Param√®tres de recherche")
        
        # Mode de filtrage par cat√©gorie 
        st.subheader("Mode de filtrage")
        strict_category = st.checkbox(
            "Mode strict (cat√©gorie principale uniquement)", 
            value=False,
            help="Si activ√©, ne r√©cup√®re que les articles dont la cat√©gorie principale correspond exactement. "
                 "Si d√©sactiv√©, inclut les articles qui ont une des cat√©gories cibles parmi leurs cat√©gories secondaires."
        )
        
        # S√©lection des cat√©gories
        st.subheader("Cat√©gorie" if strict_category else "Cat√©gories")
        categories = {
            "Intelligence Artificielle": "cs.AI",
            "Apprentissage Automatique": "cs.LG",
            "Vision par Ordinateur": "cs.CV",
            "Traitement du Langage Naturel": "cs.CL",
            "R√©seaux de Neurones": "cs.NE",
            "Robotique": "cs.RO",
            "Machine Learning (Statistique)": "stat.ML"
        }
        
        # Mode de s√©lection de cat√©gorie(s) selon le mode strict ou non
        if strict_category:
            category_options = list(categories.keys())
            selected_category = st.radio(
                "S√©lectionnez une cat√©gorie principale:", 
                category_options,
                index=0,
                key="primary_category"
            )
            selected_categories = [categories[selected_category]]
        else:
            selected_categories = []
            for name, cat_id in categories.items():
                if st.checkbox(name, value=False, key=f"cat_{cat_id}"):
                    selected_categories.append(cat_id)
               
        # Mots-cl√©s pour filtrer
        st.subheader("Filtrage par mots-cl√©s", 
                     help="Entrez des mots-cl√©s pour filtrer les articles. La recherche s'applique au titre, au r√©sum√© et aux cat√©gories arxiv.")

        placeholder_text = """Exemples :  cs.HC (terminologie arXiv)
                        q-bio.NC
                        agentic
                        education
                        deep learning"""
        
        keywords_input = st.text_area(
            "Un mot-cl√© par ligne:",
            value="",
            placeholder=placeholder_text,
            height=150
        )
        
        keywords = [k.strip() for k in keywords_input.split("\n") if k.strip()]
        
        if keywords:
            st.info(f"üîç {len(keywords)} mots-cl√©s actifs: {', '.join(keywords)}")
        
        # Nombre d'articles r√©cents
        st.subheader("Nombre d'articles")
        max_articles = st.slider("Nombre d'articles √† r√©cup√©rer", 100, 1000, 100, 100)

        # Bouton pour lancer la recherche
        search_button = st.button("üîç Lancer la recherche", type="primary")
    
    # Zone principale pour l'affichage des r√©sultats
    if search_button:
        if not selected_categories:
            st.error("Veuillez s√©lectionner au moins une cat√©gorie.")
        else:
            # Afficher un message de chargement
            with st.spinner("R√©cup√©ration des articles en cours..."):
                try:
                    # Configuration de la UI pour le feedback
                    progress_bar = st.progress(0)
                    status = st.empty()
                    
                    details_container = st.container()
                    col1, col2 = details_container.columns([2, 1])
                    detail_log = col1.empty()
                    stats_display = col2.empty()
                    
                    # Initialiser les logs d√©taill√©s
                    search_details = []
                    stats = {"total": 0, "filtered": 0, "batches": 0, "duplicates": 0}
                    
                    # Fonctions de mise √† jour de l'interface
                    def add_log(message, level="info"):
                        timestamp = datetime.now().strftime("%H:%M:%S")
                        icon = "üîç" if level == "info" else "‚úÖ" if level == "success" else "‚ö†Ô∏è" if level == "warning" else "‚ùå"
                        search_details.append(f"{icon} `{timestamp}` {message}")
                        log_text = "\n\n".join(search_details[-10:])
                        detail_log.markdown(f"### D√©tails de la recherche\n{log_text}")
                    
                    def update_stats():
                        stats_text = f"""
                        ### Statistiques
                        - **Articles trouv√©s:** {stats['total']}
                        - **Lots trait√©s:** {stats['batches']}
                        - **Doublons ignor√©s:** {stats['duplicates']}
                        - **Articles filtr√©s:** {stats['filtered']}
                        """
                        stats_display.markdown(stats_text)
                    
                    # Callback pour les mises √† jour de progression
                    def progress_callback(status_message, batch_info=None, level="info"):
                        add_log(status_message, level=level)
                        if batch_info:
                            stats["batches"] += 1
                            stats["total"] = batch_info.get("total", stats["total"])
                            stats["duplicates"] = batch_info.get("duplicates", stats["duplicates"])
                            update_stats()
                            progress_value = min(batch_info.get("total", 0) / max_articles, 0.8)
                            progress_bar.progress(progress_value)
                    
                    # R√©cup√©ration des articles
                    add_log("Initialisation de la recherche...")
                    progress_bar.progress(0.1)
                    
                    start_time = time.time()
                    add_log(f"R√©cup√©ration de maximum {max_articles} articles r√©cents...")
                    
                    articles = get_latest_arxiv_articles(
                        categories=selected_categories,
                        max_articles=max_articles,
                        strict_category=strict_category,
                        callback=progress_callback
                    )
                    
                    elapsed = time.time() - start_time
                    add_log(f"{len(articles)} articles r√©cup√©r√©s en {elapsed:.1f} secondes", level="success")
                    progress_bar.progress(0.8)
                    stats["total"] = len(articles)
                    update_stats()
                    
                    # Filtrer par mots-cl√©s
                    if keywords and articles:
                        add_log(f"Application du filtrage par {len(keywords)} mots-cl√©s...")
                        filtered_articles = filter_articles_by_keywords_multi(articles, keywords)
                        stats["filtered"] = len(filtered_articles)
                        update_stats()
                        add_log(f"{len(filtered_articles)} articles correspondent aux mots-cl√©s", level="success")
                    else:
                        filtered_articles = articles
                        stats["filtered"] = len(articles)
                        update_stats()
                    
                    progress_bar.progress(1.0)
                    status.text(f"‚úÖ Traitement termin√©: {len(filtered_articles)}/{len(articles)} articles apr√®s filtrage")
                    
                    # Stocker les articles dans session_state
                    st.session_state.arxiv_articles = filtered_articles
                    
                except Exception as e:
                    st.error(f"Erreur : {e}")
    
    # Afficher les articles s'ils sont disponibles dans session_state
    if st.session_state.arxiv_articles:
        filtered_articles = st.session_state.arxiv_articles
        
        st.success(f"‚úÖ {len(filtered_articles)} articles trouv√©s")
        
        # Cr√©er deux onglets pour afficher les r√©sultats
        tab1, tab2 = st.tabs(["Articles filtr√©s", "Tous les articles"])
        
        with tab1:
            display_arxiv_articles(filtered_articles, "Articles filtr√©s par mots-cl√©s")
            
            # Zone de s√©lection d'article
            st.subheader("S√©lectionner un article √† traiter")
                      
            selected_index = st.number_input("Index de l'article √† traiter", 
                                           min_value=0, 
                                           max_value=len(filtered_articles)-1 if filtered_articles else 0,
                                           label_visibility="visible")
            
            process_button = st.button("Traiter cet article")

            
            if len(filtered_articles) > 0:
                # Afficher les d√©tails de l'article s√©lectionn√© UNIQUEMENT
                st.subheader("D√©tails de l'article s√©lectionn√©")
                
                if selected_index < len(filtered_articles):
                    article = filtered_articles[selected_index]
                    display_article_details(article)
            
            # Traiter l'article si le bouton principal est cliqu√©
            if process_button and len(filtered_articles) > 0:
                article = filtered_articles[selected_index]
                data, category, success = prepare_arxiv_article_data(article)
                process_extraction(data, category, success, 'arxiv_scraper')
        
        with tab2:
            # Dans l'onglet "Tous les articles", il n'y a pas de filtrage mais les m√™mes donn√©es
            # que celles r√©cup√©r√©es initialement
            display_arxiv_articles(st.session_state.arxiv_articles, "Tous les articles r√©cup√©r√©s")

def show_reddit_scraper_page():
    """Page du scrapper Reddit"""
    st.title("Recherche sur Reddit ")

    # Bouton de retour
    if st.button("‚Üê Retour √† l'accueil", key="home_from_reddit"):
        navigate_to('home')

    st.markdown("""
    Recherchez et filtrez les articles r√©cents de Reddit.
    """)

    with st.spinner("Recup√©ration des posts en cours..."):
        try:
            posts = get_entries_from_reddit()
            st.session_state.reddit_posts = posts
        except Exception as e:
            st.error(f"Erreur : {e}")

        if st.session_state.reddit_posts:
            reddit_posts = st.session_state.reddit_posts

            st.success(f"‚úÖ {len(reddit_posts)} posts trouv√©s")

            st.subheader("Reddit Posts")

            df = pd.DataFrame([{
                "Index": idx,
                "Liens": post['links'][0],
                "Titre": post['title'],
                "Text": post['text']
            } for idx, post in enumerate(reddit_posts)])
 
            #st.dataframe(df, use_container_width=True)

            for index, row in df.iterrows():
                col1, col2 = st.columns([4,1])
                with col1:
                    st.markdown(f"**{row['Titre']}**")
                    st.text(row['Text'][:100]+"...")
                with col2:
                    if st.button("üîç Analyser", key=row['Liens']):
                        st.session_state.selected_url = row['Liens']
                        navigate_to('regular_extraction')

def show_huggingface_scraper_page():
    """Page de scraping automatique des articles HuggingFace"""
    st.title("Recherche sur HuggingFace üß†")

    if st.button("‚Üê Retour √† l'accueil", key="home_from_huggingface"):
        navigate_to('home')

    st.markdown("""
    Retrouvez ici les derni√®res publications d'articles partag√©s sur HuggingFace Papers.
    S√©lectionnez la p√©riode puis cliquez sur "Rechercher maintenant" pour r√©cup√©rer les articles.
    """)

    mode = st.radio("Choisissez la p√©riodicit√© :", ["Hebdomadaire", "Mensuel"], horizontal=True)

    selected_year = st.selectbox("Ann√©e :", list(range(2025, 2019, -1)), index=0)

    if mode == "Hebdomadaire":
        selected_week = st.number_input("Semaine de l'ann√©e :", min_value=1, max_value=52, value=12, step=1)
        main_page_id = f"week/{selected_year}-W{selected_week:02d}"
        start_of_week = date.fromisocalendar(selected_year, selected_week, 1)  # Monday
        end_of_week = start_of_week + timedelta(days=6)

        st.caption(f"üìÖ Du {start_of_week.strftime('%d %B %Y')} au {end_of_week.strftime('%d %B %Y')}")
    else:
        months = {
            "Janvier": "01", "F√©vrier": "02", "Mars": "03", "Avril": "04",
            "Mai": "05", "Juin": "06", "Juillet": "07", "Ao√ªt": "08",
            "Septembre": "09", "Octobre": "10", "Novembre": "11", "D√©cembre": "12"
        }
        month_names = list(months.keys())
        selected_month_name = st.selectbox("Mois :", month_names, index=2)
        selected_month = months[selected_month_name]
        main_page_id = f"month/{selected_year}-{selected_month}"

    search_button = st.button("üîç Rechercher maintenant")

    if search_button:
        with st.spinner(f"R√©cup√©ration des articles pour {main_page_id}..."):
            try:
                posts = get_entries_from_huggingface(main_page_id=main_page_id)
                st.session_state.huggingface_posts = posts
            except Exception as e:
                st.error(f"Erreur : {e}")
                return

    if st.session_state.get("huggingface_posts"):
        hf_posts = st.session_state.huggingface_posts

        st.success(f"‚úÖ {len(hf_posts)} articles HuggingFace trouv√©s")
        st.subheader("Articles HuggingFace")

        table_data = []
        for idx, post in enumerate(hf_posts):
            title = post.get('title', f"Paper {idx}")
            text = post.get('content', '')
            link = post.get('links', [''])[0]

            table_data.append({
                "Index": idx,
                "Lien": link,
                "Titre": title,
                "Texte": text
            })

        df = pd.DataFrame(table_data)

        for index, row in df.iterrows():
            col1, col2 = st.columns([4, 1])
            with col1:
                st.markdown(f"**{row['Titre']}**")
                st.text(row['Texte'][:100] + "...")
            with col2:
                if row['Lien']:
                    if st.button("üîç Analyser", key="hf_" + row['Lien']):
                        st.session_state.selected_url = row['Lien']
                        navigate_to('regular_extraction')
                else:
                    st.markdown("üîó Lien manquant")
                                
def process_data_pipeline():
    """
    Affiche l'interface UNIFI√âE de traitement pour les donn√©es extraites,
    quelle que soit leur source (URL ou ArXiv)
    """
    st.title("Traitement des donn√©es")
    
    # Bouton de retour adaptatif selon la source
    source = st.session_state.extracted_data.get("source", "unknown")
    if source == "arxiv":
        if st.button("‚Üê Retour √† la recherche ArXiv", key="back_to_source"):
            if "extracted_data" in st.session_state:
                del st.session_state.extracted_data

            if "summary" in st.session_state:
                del st.session_state.summary
            
            if "review" in st.session_state:
                del st.session_state.review
            
            if "rouge_scores" in st.session_state:
                del st.session_state.rouge_scores

            navigate_to('arxiv_scraper')
    else:
        if st.button("‚Üê Retour √† l'extraction par URL", key="back_to_source"):
            if "extracted_data" in st.session_state:
                del st.session_state.extracted_data

            if "summary" in st.session_state:
                del st.session_state.summary
            
            if "review" in st.session_state:
                del st.session_state.review
            
            if "rouge_scores" in st.session_state:
                del st.session_state.rouge_scores
                
            navigate_to('regular_extraction')
    
    # Afficher les donn√©es extraites
    if "extracted_data" in st.session_state:
        data = st.session_state.extracted_data
        st.markdown(f"## {data['title']}")
        
        # Afficher toutes les m√©tadonn√©es disponibles
        metadata_fields = [
            ("authors", "Auteurs", lambda x: ', '.join(x) if isinstance(x, list) else x),
            ("url", "Source", lambda x: f"[{'ArXiv' if source=='arxiv' else 'Source'}]({x})"),
            ("category", "Cat√©gorie", lambda x: x),
            ("date", "Date", lambda x: x)
        ]
        
        for field, label, formatter in metadata_fields:
            if field in data and data[field]:
                st.markdown(f"**{label}:** {formatter(data[field])}")
        
        # Afficher le contenu
        st.markdown("### Contenu")
        with st.expander("Voir le contenu complet", expanded=False):
            st.write(data.get('content', 'Aucun contenu disponible'))
        
        # Informations de cat√©gorie
        st.markdown("### Classification")
        with st.container():
            if "extracted_category" in st.session_state:
                st.success(f"**Cat√©gorie d√©tect√©e :** {st.session_state.extracted_category}")
            else:
                st.warning("Aucune cat√©gorie d√©tect√©e")
        
        # Section de r√©sum√© - Action commune quelle que soit la source
        st.markdown("## Interpr√©tation")
        if st.button("R√©sumer le contenu üìù"):
            generate_content_summary()
        
        # Afficher le r√©sum√© s'il est disponible
        if "summary" in st.session_state:
            st.markdown("### R√©sum√© G√©n√©r√©")
            st.write(st.session_state.summary)
            st.write(f"**Prix du r√©sum√©** üí∏: {st.session_state.summary_price}$")
    
            # Section d'√©valuation - Actions communes quelle que soit la source
            st.markdown("## Evaluation et Critique")
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("Critiquer le r√©sum√© üíØ"):
                    evaluate_summary_with_critique()
    
                if "review" in st.session_state:
                    st.markdown("### Critique du r√©sum√© :")
                    st.write(st.session_state.review)
                
            with col2:
                if st.button("√âvaluer le R√©sum√© avec Rouge üßÆ"):
                    calculate_rouge_metrics()
            
                if "rouge_scores" in st.session_state:
                    st.markdown("### Scores Rouge :")
                    st.write("**Rouge-1 Pr√©cision :** {:.2f}".format(st.session_state.rouge_scores["rouge1_precision"]))
                    st.write("**Rouge-2 Pr√©cision :** {:.2f}".format(st.session_state.rouge_scores["rouge2_precision"]))
                    st.write("**Rouge-L Pr√©cision :** {:.2f}".format(st.session_state.rouge_scores["rougeL_precision"]))
    else:
        st.error("Aucune donn√©e n'est disponible. Veuillez retourner √† l'extraction.")

def show_footer():
    """Affiche le pied de page commun"""
    st.markdown("---")
    
    # Ajouter le bouton de r√©initialisation avant le footer
    col1, col2, col3 = st.columns([1, 1, 1])
    with col2:
        if st.button("üè† Retour √† l'accueil (r√©initialiser tout)", use_container_width=True):
            reset_and_go_home()
    
    st.markdown(
        """
        ¬© 2024 Arij BEN RHOUMA, Antoine CASTEL, Arthur VOGELS - Projet de fin d'√©tude CentraleSup√©lec X ILLUIN Technology ü§ñ.
        """
    )

# Application principale - routage bas√© sur l'√©tat de session
def main():
    # Router: affiche diff√©rentes pages selon l'√©tat de session
    router = {
        'home': show_home_page,
        'regular_extraction': show_regular_extraction_page,
        'arxiv_scraper': show_arxiv_scraper_page,
        'process_data': process_data_pipeline,
        'reddit_scraper': show_reddit_scraper_page,
        'huggingface_scraper': show_huggingface_scraper_page,
    }
    
    # Afficher la page actuelle ou par d√©faut la page d'accueil
    current_page = st.session_state.page
    page_function = router.get(current_page, show_home_page)
    page_function()
    
    # Toujours afficher le footer
    show_footer()

# Point d'entr√©e
if __name__ == "__main__":
    main()
